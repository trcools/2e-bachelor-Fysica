{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Numerical Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal\n",
    "import math\n",
    "import struct\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximations in scientific computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absolute Error and Relative Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Absolute Error** = approximate value - true value\n",
    "\n",
    "**Relative Error** = $\\frac{\\rm{absolute\\,error}}{\\rm{true\\,value}}$\n",
    "\n",
    "Another interpretation of relative error is that if an approximate value has a relative error of about $10^{-p}$, then its decimal representation has about $p$ correct **significant digits** (the leading nonzero digit and the $p-1$ following  digits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision and Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**: the number of digits with which a number is expressed.\n",
    "\n",
    "**Accuracy**: the number of *correct* significant digits in an approximation of the desired quantity.\n",
    "\n",
    "> **Example**\n",
    ">\n",
    "> 3.25260376469 is a very precise number but is not very accurate as an approximation for $\\pi$. Computing a quantity using a given precision does not necessarily mean that the result will be accurate to that precision!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncation and Rounding error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Truncation error**: The difference between the true result and the result given by an algorithm using exact arithmetic. It is due to approximations such as truncating an infinite series or replacing derivatives by finite differences,...\n",
    "\n",
    "**Rounding error**: The difference between the result produced by a given algorithm using exact arithmetic and the same algorithm, using finite-precision, rounded arithmetic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating-point number systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a digital computer, numbers are represented by a *floating-point* number system, which resembles *scientific notation* in which a number is expressed as a number of moderate size times an appropriate power of ten, e.g. $0.0007396$ can be written as $7.396\\times10^{-4}$. In this format, the decimal point moves, or *floats* as the power of 10 changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A floating-point number system is characterized by four integers:\n",
    "\n",
    "| symbol &nbsp;      | name             |\n",
    "|--------------------|------------------|\n",
    "|$\\beta$             | Base or radix    |\n",
    "|$p$                 | Precision        |\n",
    "|$[L,U]$             | Exponent range   |\n",
    "\n",
    "Any floating-point number then has the form\n",
    "\n",
    "$\\pm \\left(d_0+\\frac{d_1}{\\beta}+\\frac{d_2}{\\beta^2}+\\cdots+\\frac{d_{p-1}}{\\beta^{p-1}}\\right)\\beta^E$\n",
    "where $d_i$ and $E$ are integers such that $0\\leq d_i \\leq \\beta-1$ and $L \\leq E\\leq U$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A floating-point system is **normalized** if the leading digit $d_0$ always equals  1 (unless the number represented is zero).\n",
    "\n",
    "This is advantageous because\n",
    "- Each number has a unique representation.\n",
    "- No digits are wasted on leading zeros, thereby maximizing precision.\n",
    "- In a binary system ($\\beta=2$), the leading bit is always 1, and thus need not be stored, thereby gaining an additional bit of precision.\n",
    "\n",
    "The two most important systems in use are the IEEE single precision (SP) and double precision (DP) standards with:\n",
    "\n",
    "| System &nbsp;   | $\\beta$ | $p$ | $L$ | $U$|\n",
    "|-----------------|---------|-----|-----|----|\n",
    "|IEEE SP          |2        |24   | -126| 127|\n",
    "|IEEE DP          |2        |53   |-1022|1023|\n",
    "\n",
    "\n",
    "The single-precision binary floating-point exponent is encoded using an **offset-binary representation**, with the zero offset being 127"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Example** \n",
    ">\n",
    "> Single precision numbers are stored in 4 bytes (or 32 bits), used as follows:\n",
    "> - 1 sign bit\n",
    "> - 8 bits for the exponent (ranging from -126 to 127)(the exponent sets the order of magnitude of the float)\n",
    "> - 23 bits for the mantissa (the mantissa sets the actual precise value of the float)\n",
    ">\n",
    "> The exponent is calculated by taking -127 and then adding $2^n$ for every 1 in the 8 bits starting from the right.\n",
    "> A simple example 01011001 is the same as: $-127 + 1 + 8 + 16 + 64 = -38$ \n",
    ">\n",
    "> For instance the number 0.75 is stored as\n",
    ">\n",
    "> 0 01111110 10000000000000000000000\n",
    ">\n",
    ">The float is calculated by multiplying a **number** (called the **mantissa** with a certain **sign** by 2 raised to a certain **power**.  \n",
    ">\n",
    ">The first digit defines the **sign** of the number: 1 meaning negative and 0 meaning positive. In this case we have a 0, so the float will be positive.\n",
    ">\n",
    ">The following 8 digits define the **power**. The 8 digits refer to powers of 2. The first of them corresponds to $2^7$, the second to $2^6$ and so on till the eight digit referring to $2^0$. To find the exponent we sum over these powers of 2 where the corresponding digit is 1. The value of this sum is than added to -127, the result is the exponent. In this case the exponent E is given by:\n",
    ">\n",
    ">$E = -127 + (2^1 + 2^2 + 2^3 + 2^4 + 2^5 +2^6) = -127 + 126 = -1$\n",
    ">\n",
    ">***Note:** The smallest value for the exponent is L = -126 corresponding to the digits 00000001. The largest value for the exponent is U = 127 corresponding to the digits 11111110. The digits 00000000 and 11111111 are invalid combinations.*\n",
    ">\n",
    ">The last 23 digits define the **mantissa** which is calculated by:\n",
    ">\n",
    ">$\\left(1+\\frac{d_1}{2}+\\frac{d_2}{4}+.\\ldots \\frac{d_{23}}{2^{23}}\\right)$\n",
    ">\n",
    ">In the equation $d_1$ till $d_{23}$ referred to the last 23 digits, being a 1 or a 0. In this case the following is found:\n",
    ">\n",
    ">$\\left(1+\\frac{1}{2}+\\frac{0}{4}+.\\ldots \\frac{0}{2^{23}}\\right) = 1.5$\n",
    ">\n",
    ">So in total we get:\n",
    ">\n",
    ">$\\left(1+\\frac{1}{2}+\\frac{0}{4}+.\\ldots \\frac{0}{2^{23}}\\right) 2^{(-127+126)}=1.5\\cdot 2^{-1}=0.75$\n",
    ">\n",
    "\n",
    "> If we would explicitly store $d_0$ (and allow for **denormalized** numbers with $d_0=0$), the representation would read \n",
    ">\n",
    "> 0 &nbsp;01111110&nbsp; **1**1000000000000000000000 \n",
    ">\n",
    "> Note that by adding the $d_0$ bit (in bold), we lost a bit of precision ($d_{23}$) because we only have 23 bits in total in the mantissa. \n",
    ">\n",
    ">However, the same number could also be written as\n",
    ">\n",
    "> 0 &nbsp;  01111111 &nbsp;  **0**11000000000000000000000\n",
    "> , corresponding to $+\\left(0+\\frac{1}{2}+\\frac{1}{4}+.\\ldots \\frac{0}{2^{23}}\\right) 2^{(-127^{\\rm offset}+\\mathit{127})}=0.75\\cdot 2^{0}=0.75$\n",
    ">\n",
    "> This means that we've lost a bit of precision and gained nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"python in itself only supports double precison floats,\\n \"\n",
    "    \"but numpy allows to use several different data types,\\n \"\n",
    "    \"including single precision floats\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "def bit_rep(num):\n",
    "    \"\"\"Translate a number into its bit representation.\"\"\"\n",
    "    return \"\".join(\n",
    "        bin(c).replace(\"0b\", \"\").rjust(8, \"0\") for c in struct.pack(\"!f\", num)\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"np.single(0.75) has bit representation of:\", bit_rep(np.single(0.75)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**\n",
    ">\n",
    "> Convert this 32 bit single precision number to its decimal representation.\n",
    ">\n",
    "> 0 10000000 01101010000010011110011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A floating-point number is finite and discrete. The number of normalized floating-point numbers in a given system is\n",
    "$2 (\\beta-1)\\beta^{p-1}(U-L+1)+1$\n",
    "- $2$ choices of sign\n",
    "- $(\\beta-1)$ choices for the leading digit of the mantissa $(=d_0)$ \n",
    "- $\\beta^{p-1}$ because there are $\\beta$ choices for each of the remaining $p-1$ digits of the mantissa\n",
    "- $(U-L+1)$ possible values for the exponent ($+1$ because the boundaries of [L, U] are being counted)\n",
    "- $+1$ because the number could be zero\n",
    "\n",
    "The smallest positive normalized number (the **underflow level**): if all the bits in the mantissa part and all but the last bit in the exponent part are 0, the number equals\n",
    "\n",
    "$\\beta^L$\n",
    "\n",
    "The largest number (the **overflow level**) equals\n",
    "\n",
    "$\\beta^{U+1}(1-\\beta^{-p})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Short derivation of the overflow level:**\n",
    ">\n",
    ">The largest possible value is the value where the upper limit U is reached and all values in the mantissa are equal to the maximum value of $(\\beta-1)$ \n",
    "> \n",
    "> the overflow level thus is $\\beta^{U}(\\beta-1)\\cdot \\sum_{k=0}^{p-1}\\beta^{-k}$: for a IEEE single precision binary number this would look like\n",
    ">\n",
    ">0 11111110 11111111111111111111111\n",
    ">\n",
    "> > If this was in ternary, all the ones would be two's, explaining the factor $\\beta-1$.\n",
    ">\n",
    "> Note that not all values in the 8 bits of the exponent are 1. This form is reserved for `inf` and `NaN`.\n",
    ">\n",
    "> Hence we find:\n",
    ">\n",
    ">$$\n",
    "\\beta^{U}(\\beta-1)\\cdot \\sum_{k=0}^{p-1}\\beta^{-k}\n",
    "= \\beta^{U} (\\beta-1)\\cdot \\dfrac{\\sum_{k=0}^{p-1}\\beta^{k}}{\\beta^{p-1}}\n",
    "= \\beta^{U}(\\beta-1)\\cdot \\dfrac{1}{\\beta^{p-1}}\\cdot \\dfrac{\\beta^{p}-1}{\\beta-1}\n",
    "= \\beta^{U+1} \\cdot (1-\\beta^{-p})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Example**\n",
    ">\n",
    "> Let's look at two examples of how the overflow level is calculated.\n",
    ">\n",
    "> First lets look at the situation where p = 24 and $\\beta$ = 2.\n",
    ">\n",
    "> The overflow level is then: \n",
    ">\n",
    ">$2^{U}\\left(1+\\frac{\\mathbf{1}}{2}+\\frac{\\mathbf{1}}{4}+.\\ldots \\frac{\\mathbf{1}}{2^{23}}\\right)=2^{U+1}\\left(\\frac{1}{2}+\\frac{1}{4}+.\\ldots \\frac{1}{2^{24}}\\right)=2^{U+1}\\left(1 - \\frac{1}{2^{24}}\\right)$\n",
    ">\n",
    "> - **Note:** for $\\beta$ = 2 the $d_i$ are always either 0 or 1 since $0 \\leq d_i \\leq \\beta -1$. The maximum value for the $d_i$ thus is 1. \n",
    ">\n",
    ">For the second example lets look at the situation where p = 50 and $\\beta$ = 10.\n",
    ">\n",
    "> The overflow level is then:\n",
    ">\n",
    ">$10^{U}\\left(9+\\frac{\\mathbf{9}}{10}+\\frac{\\mathbf{9}}{100}+.\\ldots \\frac{\\mathbf{9}}{10^{49}}\\right)=10^{U+1}\\left(\\frac{9}{10}+\\frac{9}{100}+.\\ldots \\frac{9}{10^{50}}\\right)=10^{U+1}\\left(1 - \\frac{1}{10^{50}}\\right)$\n",
    ">\n",
    "> - **Note:** for $\\beta$ = 10 the maximum value for the $d_i$ is 9. \n",
    ">\n",
    "> As expected, for both cases the overflow level is given by: $\\beta^{U+1}(1-\\beta^{-p})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Example**\n",
    ">\n",
    "> Now, let's take a look at a *toy* floating point system with $\\beta$=2, p=3, L=-1 and U=1.\n",
    ">\n",
    "> This system supports 25 floating point-numbers:\n",
    ">\n",
    ">$2 (\\beta-1)\\beta^{p-1}(U-L+1)+1 = 2 (2-1)2^{3-1}(1-(-1)+1)+1 = 25$\n",
    ">\n",
    "> The largest number is $\\beta^{U}(\\beta-1)\\cdot \\sum_{k=0}^{p-1}\\beta^{-k}=2^2\\cdot 1(1-2^{-3})=3.5$\n",
    ">\n",
    "> The smallest number is $\\beta^L=2^{-1}=0.5$\n",
    "\n",
    "Floating-point numbers are not uniformly distributed throughout their range, but are equally spaced only between successive powers of $\\beta$.\n",
    "\n",
    "> A comparison of small systems is shown in the graph below. Note that, although these systems are extremely small, they are representative for all float-point systems in their property that they are unevenly spaced. Try larger values of p=5, p=8 to see the density grow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_custom_floats():\n",
    "    # Custom floating-point format parameters\n",
    "    sign_bits = [\"0\", \"1\"]  # 1 bit for sign\n",
    "    exponent_bits_list = [\n",
    "        \"01\",\n",
    "        \"10\",\n",
    "        \"11\",\n",
    "    ]  # 2 bits for exponent ('00' reserved for zero)\n",
    "    mantissa_bits_list = [\"00\", \"01\", \"10\", \"11\"]  # 2 bits for mantissa\n",
    "    bias = 2  # Exponent bias\n",
    "    m = 2  # Number of mantissa bits (excluding implicit leading 1)\n",
    "    float_values = []  # List to hold values\n",
    "    bit_strings = []  # List to hold bit strings\n",
    "\n",
    "    # Manually add zero (special case where exponent and mantissa are zero)\n",
    "    zero_positive_bit_string = \"0 00 00\"\n",
    "    zero_negative_bit_string = \"1 00 00\"\n",
    "    zero_positive_value = 0.0\n",
    "    zero_negative_value = -0.0\n",
    "    float_values.append(zero_negative_value)\n",
    "    bit_strings.append(zero_negative_bit_string)\n",
    "    float_values.append(zero_positive_value)\n",
    "    bit_strings.append(zero_positive_bit_string)\n",
    "\n",
    "    for sign_bit in sign_bits:\n",
    "        for exponent_bits in exponent_bits_list:\n",
    "            exponent_value = int(exponent_bits, 2)\n",
    "            E = exponent_value - bias\n",
    "            for mantissa_bits in mantissa_bits_list:\n",
    "                # Skip adding zero again\n",
    "                if exponent_bits == \"00\" and mantissa_bits == \"00\":\n",
    "                    continue\n",
    "                mantissa_value = int(mantissa_bits, 2)\n",
    "                mantissa = 1 + mantissa_value * (2**-m)\n",
    "                value = (-1) ** int(sign_bit) * mantissa * (2**E)\n",
    "                # Create bit string with spaces between sign, exponent, and mantissa\n",
    "                bit_string = f\"{sign_bit} {exponent_bits} {mantissa_bits}\"\n",
    "                float_values.append(value)\n",
    "                bit_strings.append(bit_string)\n",
    "\n",
    "    # Sort the lists based on the floating-point values\n",
    "    sorted_indices = sorted(range(len(float_values)), key=lambda i: float_values[i])\n",
    "    sorted_float_values = [float_values[i] for i in sorted_indices]\n",
    "    sorted_bit_strings = [bit_strings[i] for i in sorted_indices]\n",
    "\n",
    "    return sorted_float_values, sorted_bit_strings\n",
    "\n",
    "\n",
    "def plot_custom_float_distribution():\n",
    "    # Generate the floats and their corresponding bit strings\n",
    "    float_values, bit_strings = generate_custom_floats()\n",
    "\n",
    "    # Filter values and bit strings within the range -3.5 to 3.5 separately\n",
    "    filtered_values = [v for v in float_values if -3.5 <= v <= 3.5]\n",
    "    filtered_bits = [\n",
    "        bit_strings[i] for i, v in enumerate(float_values) if -3.5 <= v <= 3.5\n",
    "    ]\n",
    "\n",
    "    # Print the sorted list with binary representations\n",
    "    print(\"Sorted Floating-Point Numbers with Binary Representations:\")\n",
    "    for val, bits in zip(filtered_values, filtered_bits, strict=False):\n",
    "        print(f\"{val:6.3f}   {bits}\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.close(\"custom_float\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 3), num=\"custom_float\")\n",
    "    ax.set_title(\"Distribution of Custom Floating-Point Numbers\")\n",
    "    ax.set_yticks([])\n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    "\n",
    "    # Plot the floating-point numbers\n",
    "    ax.plot(\n",
    "        filtered_values,\n",
    "        [0] * len(filtered_values),\n",
    "        marker=\"|\",\n",
    "        linestyle=\"None\",\n",
    "        markersize=10,\n",
    "    )\n",
    "\n",
    "    # Add horizontal line (x-axis)\n",
    "    ax.axhline(y=0, color=\"black\", linewidth=0.5)\n",
    "\n",
    "    # Display all numbers on the x-axis\n",
    "    ax.set_xticks(\n",
    "        filtered_values, [f\"{v:.3f}\" for v in filtered_values], rotation=90\n",
    "    )\n",
    "\n",
    "\n",
    "plot_custom_float_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real numbers that are exactly representable in given floating-point system are called **machine numbers**. If a given number is not representable, it must be rounded to a \"nearby\" floating-point number. The error introduced by this approximation is called the **rounding error**. The most accurate and unbiased rule to round (and the de-facto standard today, also in the IEEE standards) is **round to nearest**, where a number is represented by its nearest floating-point number. In case of a tie, we use the number whose last stored digit is even. An alternative would be **chopped**: the expansion in $\\frac{d_i}{\\beta^i}$ is truncated after the ($p$-1)st digit, i.e.  $\\frac{d_{p-2}}{\\beta^{p-2}}$.\n",
    "\n",
    "The accuracy of a floating-point system is called the **machine precision**. Its value depends on the particular rounding rules that are being used. In case of rounding to nearest it equals to:\n",
    "$\\epsilon_{\\rm mach}=\\frac{1}{2}\\beta^{1-p}$\n",
    "\n",
    ">**Example**\n",
    ">\n",
    "> For the IEEE SP and DP systems, $\\epsilon_{\\rm mach}=2^{-24}\\approx 10^{-7}$ and $\\epsilon_{\\rm mach}=2^{-53}\\approx 10^{-16}$, respectively. These systems thus have about 7 and 16 decimal digits of precision.\n",
    "\n",
    "Although both values are small,  $\\epsilon_{\\rm mach}$ should not be confused with the underflow level.\n",
    "\n",
    "Finally, there are two additional special values to indicate exceptional situations:\n",
    "- `Inf`, which stand for **infinity**, which results e.g. from dividing a non-zero number by zero.\n",
    "- `NaN`, which stands for **not a number**, and results from an undefined operations such as $\\frac{0}{0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The largest double precision number is about 1.8e308, \"\n",
    "    \"larger numbers become infinity.\"\n",
    ")\n",
    "print(1.797e308, 1.798e308)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\n",
    "    \"The smallest double precision number is about 4.95e-324, \"\n",
    "    \"numbers that are smaller than half this value get rounded to zero.\"\n",
    ")\n",
    "print(4.95e-324, 2.4e-324)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Because 0.1 is stored as: \")\n",
    "print(\"\\t\", decimal.Decimal(0.1))\n",
    "print(\"and 0.3 is stored as \")\n",
    "print(\"\\t\", decimal.Decimal(0.3))\n",
    "print(\".1+.1+.1 does not equal .3\")\n",
    "print(\"As shown by testing .1+.1+.1 == .3, which gives:\", 0.1 + 0.1 + 0.1 == 0.3)\n",
    "print(\"Instead, it equals\", 0.1 + 0.1 + 0.1)\n",
    "print(\"which differs from .3 by\", 0.1 + 0.1 + 0.1 - 0.3, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Practices for computer arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cancellation**\n",
    "\n",
    "- Avoid subtracting two almost identical numbers\n",
    "\n",
    "**Addition**\n",
    "\n",
    "- Avoid adding small and large numbers\n",
    "- Perform a sequence of additions ordered from the smallest number to the largest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_cancellation():\n",
    "    print(\"Example of cancellation error\")\n",
    "    x = 0.1234567891234567890\n",
    "    y = 0.1234567891234567\n",
    "    print(\n",
    "        \"The real value of x - y is 8.9e-17, \"\n",
    "        \"however python returns a number which is about 7% smaller\"\n",
    "    )\n",
    "    print(x - y, \"\\n\")\n",
    "\n",
    "\n",
    "example_cancellation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following sum of an alternating harmonic series\n",
    "\n",
    "$$\n",
    "S = \\sum_{k=1}^{N} (-1)^{k+1} \\frac{1}{k} \\approx \\ln(2)\n",
    "$$\n",
    "\n",
    "for large $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sum in natural order with single precision\n",
    "def alternating_harmonic_natural(N):\n",
    "    total = np.float32(0.0)\n",
    "    print(\"Summing alternating harmonic series in natural order (from 1 to N):\")\n",
    "    for k in range(1, N + 1):\n",
    "        term = np.float32((-1) ** (k + 1) / k)\n",
    "        total += term\n",
    "    true_value = np.float32(math.log(2))  # True value is ln(2)\n",
    "    relative_error = (total - true_value) / true_value\n",
    "    print(f\"\\tTotal sum: {total:.16f}\")\n",
    "    print(f\"\\tRelative error: {relative_error:.16e}\\n\")\n",
    "    return total\n",
    "\n",
    "\n",
    "# Function to sum in reverse order with single precision\n",
    "def alternating_harmonic_reverse(N):\n",
    "    total = np.float32(0.0)\n",
    "    print(\"Summing alternating harmonic series in reverse order (from N to 1):\")\n",
    "    for k in range(N, 0, -1):\n",
    "        term = np.float32((-1) ** (k + 1) / k)\n",
    "        total += term\n",
    "    true_value = np.float32(math.log(2))  # True value is ln(2)\n",
    "    relative_error = (total - true_value) / true_value\n",
    "    print(f\"\\tTotal sum: {total:.16f}\")\n",
    "    print(f\"\\tRelative error: {relative_error:.16e}\\n\")\n",
    "    return total\n",
    "\n",
    "\n",
    "# Function to sum positive and negative terms separately in single precision\n",
    "def alternating_harmonic_grouped(N):\n",
    "    print(\"Summing positive and negative terms separately:\")\n",
    "    total_positive = np.float32(\n",
    "        sum(np.float32(1.0 / k) for k in range(1, N + 1, 2))\n",
    "    )  # Positive terms\n",
    "    total_negative = np.float32(\n",
    "        sum(np.float32(-1.0 / k) for k in range(2, N + 1, 2))\n",
    "    )  # Negative terms\n",
    "    total = total_positive + total_negative\n",
    "    true_value = np.float32(math.log(2))  # True value is ln(2)\n",
    "    relative_error = (total - true_value) / true_value\n",
    "    print(f\"\\tTotal sum: {total:.16f}\")\n",
    "    print(f\"\\tRelative error: {relative_error:.16e}\\n\")\n",
    "    return total\n",
    "\n",
    "\n",
    "# Main block to execute all summation methods in single precision\n",
    "N = 10000000\n",
    "print(\n",
    "    f\"Computing the alternating harmonic series up to N = {N} in single precision\\n\"\n",
    ")\n",
    "print(f\"The true value is ln(2) ≈ {np.float32(math.log(2)):.16f}\\n\")\n",
    "\n",
    "# Compute and compare the results\n",
    "sum_natural = alternating_harmonic_natural(N)\n",
    "sum_reverse = alternating_harmonic_reverse(N)\n",
    "sum_grouped = alternating_harmonic_grouped(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even relatively simple mathematical problems can exhibit numerical issues when computed using finite-precision arithmetic. A classic example is the **quadratic formula** used to find the roots of a quadratic equation.\n",
    "\n",
    "Consider the quadratic equation:\n",
    "\n",
    "$$\n",
    "ax^2 + bx + c = 0\n",
    "$$\n",
    "\n",
    "Its solutions are given by:\n",
    "\n",
    "$$\n",
    "x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n",
    "$$\n",
    "\n",
    "However, naïvely implementing this formula can lead to numerical problems such as **overflow**, **underflow**, and **catastrophic cancellation**, especially when the coefficients are very large or very small in magnitude.\n",
    "\n",
    "- When $b^2$ is much larger than $ 4ac $, the discriminant $ \\sqrt{b^2 - 4ac} $ is nearly equal to $ |b| $. Subtracting two nearly equal numbers, can cause significant loss of precision due to catastrophic cancellation.\n",
    "  \n",
    "- To compute the roots more accurately, use the following rearranged formula:\n",
    "\n",
    "    $$\n",
    "    x = \\frac{2c}{-b \\mp \\sqrt{b^2 - 4ac}}\n",
    "    $$\n",
    "\n",
    "    which has the flipped sign in the denominator and avoids the subtraction of two nearly equal numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function evaluations**: When implementing a function for evaluation, take care in how you write it to avoid numerical problems: \n",
    "\n",
    "E.g. when evaluating $ f(x) = \\sqrt{x + 1} - \\sqrt{x} $ for large $ x$ both terms are nearly equal, and their subtraction leads to loss of significant digits.\n",
    "\n",
    "This can be solved by re-writing it as\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{(\\sqrt{x + 1} - \\sqrt{x})(\\sqrt{x + 1} + \\sqrt{x})}{\\sqrt{x + 1} + \\sqrt{x}} = \\frac{1}{\\sqrt{x + 1} + \\sqrt{x}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1e16  # Large value of x\n",
    "\n",
    "# Original expression\n",
    "f_original = math.sqrt(x + 1) - math.sqrt(x)\n",
    "\n",
    "# alternative expression\n",
    "f_alt = 1 / (math.sqrt(x + 1) + math.sqrt(x))\n",
    "\n",
    "print(\"Original function result:\", f_original)\n",
    "print(\"Alternative function result:\", f_alt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "title": "Numerical Limitations",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

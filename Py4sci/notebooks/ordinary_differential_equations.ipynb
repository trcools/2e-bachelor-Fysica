{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4966fd59",
   "metadata": {},
   "source": [
    "# Ordinary Differential Equations (ODEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1edc999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fractions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import integrate, linalg, optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f22948-1216-4298-8c70-b8c02961665e",
   "metadata": {},
   "source": [
    "## Introduction and useful concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f089f05",
   "metadata": {},
   "source": [
    "Most physical systems change over time. \n",
    "From an orbiting satellite to a cooling cup of coffee, from a swinging pendulum to a decaying radioisotope, from reacting chemical species to competing biological species, a state of flux is the norm. \n",
    "\n",
    "One of the motivating problems for the invention of differential calculus was to characterize the motion of celestial bodies and earthly projectiles so that their future locations could be predicted.\n",
    "Differential equations provide a mathematical language for describing continuous change. \n",
    "\n",
    "Beginning with Newton’s laws of motion, most of the fundamental laws of science are expressed as differential equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd612405",
   "metadata": {},
   "source": [
    "Suppose that the state of a system at any given time $t$ is described by some vector function $\\mathbf{y}(t)$,\n",
    "\n",
    "For example, the components of $\\mathbf{y}(t)$ might represent the spatial coordinates of a projectile or concentrations of various chemical species. \n",
    "\n",
    "A **differential equation** prescribes a relationship between this unknown state function $\\mathbf{y}(t)$ and one or more of its derivatives with respect to $t$ that must hold at any given time. \n",
    "\n",
    "In solving a differential equation, the objective is to determine a differentiable function $\\mathbf{y}(t)$ that satisfies the prescribed relationship.\n",
    "\n",
    "Finding such a solution of the differential equation is important because it will enable us to predict the future evolution of the system over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01775ab",
   "metadata": {},
   "source": [
    "> **Example**\n",
    ">\n",
    "> *Newton’s Second Law of Motion* states that force equals mass times acceleration ($F = ma$)\n",
    ">\n",
    "> This differential equation relates the state of an object, in this case its position in space, to the second derivative of that state function. \n",
    "> In one dimension, the differential equation looks like this:\n",
    ">\n",
    "> $$F(t,y(t),dy(t)/dt)=m\\frac{d^2y}{dt^2}$$\n",
    "> \n",
    "> where the force $F$ in general depends on the time $t$, the position $y(t)$, and the velocity $dy(t)/dt$, and the acceleration is the second derivative of the position $\\frac{d^2y}{dt^2}$.\n",
    "> If $F$ is the gravitational force on an object (near Earth’s surface), then $F = −mg$, where $g$ is the standard gravity.\n",
    "> The solution to the differential equation is then given by\n",
    ">\n",
    "> $$y(t)=-\\frac{1}{2}gt^2+c_1t+c_2$$\n",
    ">\n",
    "> where $c_1$ and $c_2$ are constants that depend on the initial position and velocity of the object. \n",
    "> This solution function describes the trajectory of the object over time under the force of gravity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2444d769",
   "metadata": {},
   "source": [
    "When there is only one independent variable, such as time, then all derivatives of the dependent variables are with respect to that independent variable, and we have an **ordinary differential equation, or ODE**. \n",
    "\n",
    "In the PDE notebook we will consider systems with more than one independent variable, so that partial derivatives are required and we have a **partial differential equation, or PDE**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b64811",
   "metadata": {},
   "source": [
    "**Notation**: To make ODEs less cumbersome to express, we will use the notation $\\mathbf{y}'(t) = d\\mathbf{y}(t)/dt$ to indicate the\n",
    "first derivative with respect to the (only) independent variable $t$, and we will often suppress the explicit dependence on $t$, for example writing $\\mathbf{y}' = d\\mathbf{y}/dt$, with the dependence\n",
    "on t understood. \n",
    "\n",
    "> **Example**\n",
    ">\n",
    "> With these conventions Newton’s Second Law can be written $F = m y''$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d65346",
   "metadata": {},
   "source": [
    "The highest-order derivative appearing in an ODE determines the **order** of the ODE. \n",
    "\n",
    "> **Example**\n",
    ">\n",
    "> Newton’s Second Law is a second-order ODE.\n",
    "\n",
    "\n",
    "The most general $k$th order ODE has the **implicit** form\n",
    "\n",
    "$$\\mathbf{f}(t,\\mathbf{y},\\mathbf{y}',\\ldots,\\mathbf{y}^{(k)})=\\mathbf{0}$$\n",
    "\n",
    "where $\\mathbf{f}$ is a known function and y(t) is to be determined. \n",
    "\n",
    "A $k$th order ODE is said to be **explicit** if it can be written in the form\n",
    "\n",
    "$$\\mathbf{y}^{(k)}(t)= \\mathbf{f}(t,\\mathbf{y},\\mathbf{y}',\\ldots,\\mathbf{y}^{(k-1)}) $$\n",
    "\n",
    "Many ODEs arise naturally in this form, and many others can be transformed into it.\n",
    "\n",
    "> **Example**\n",
    "> \n",
    "> Newton’s Second Law is technically implicit, but it can be made explicit by dividing both sides by the mass $m$, so that it becomes \n",
    ">\n",
    "> $$y''=F/m$$ \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff248b6",
   "metadata": {},
   "source": [
    "We will only consider first-order ODEs. \n",
    "This is not a real restriction because a higher-order ODE can always be transformed into an equivalent first-order system as follows. \n",
    "\n",
    "For an explicit $k$th order ODE of the form just given, define the $k$ new unknowns\n",
    "\n",
    "- $u_1(t) = y(t)$, \n",
    "- $u_2(t) = y'(t)$\n",
    "- $\\cdots$\n",
    "- $u_k(t) = y^{(k−1)}(t)$, \n",
    "\n",
    "so that the original $k$th order equation becomes a system of $k$ first-order equations:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{u}'_1 \\\\\n",
    "\\mathbf{u}'_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{u}'_{k-1} \\\\\n",
    "\\mathbf{u}'_k\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\mathbf{u}_2 \\\\\n",
    "\\mathbf{u}_3 \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{u}_k \\\\\n",
    "\\mathbf{f}(t,\\mathbf{u}_1,\\mathbf{u}_2,\\cdots,\\mathbf{u}_k)\n",
    "\\end{bmatrix} = \\mathbf{g}(t,\\mathbf{u})\n",
    "$$\n",
    "\n",
    "> **Example**\n",
    ">\n",
    "> Again, Newton’s Second Law, which is of second order, is a good example. \n",
    "> \n",
    "> If we define the new unknowns:\n",
    "> \n",
    "> - $u_1(t) = y(t)$\n",
    "> - $u_2(t) = y'(t)$\n",
    ">\n",
    "> ,then Newton’s Second Law becomes a system of two first-order equations\n",
    ">\n",
    "> $$\n",
    "\\begin{bmatrix}u'_1 \\\\\n",
    "u'_2\\end{bmatrix}=\\begin{bmatrix}\n",
    "u_2 \\\\ F/m\n",
    "\\end{bmatrix}\n",
    "$$\n",
    ">\n",
    "> Which looks a lot more familiar if we introduce the velocity $v$ instead of $u_2$ and the position $x$ instead of $u_1$\n",
    ">\n",
    "> $$\n",
    "\\begin{bmatrix}x' \\\\\n",
    "v'\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "v \\\\\n",
    "F/m\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e65b3b9",
   "metadata": {},
   "source": [
    "An ODE $\\mathbf{y}'= \\mathbf{f}(t, \\mathbf{y})$ does not by itself determine a unique solution function because only the slopes $\\mathbf{y}'(t)$ of the solution components are prescribed by the ODE for any value of $t$, not the solution value $\\mathbf{y}(t)$ itself, so there is usually an infinite family of functions that satisfy the ODE. \n",
    "\n",
    "To single out a particular solution, we must specify the value of the solution function, denoted by $\\mathbf{y}_0$, for some value of $t$, denoted by $t_0$.\n",
    "Thus, part of the given problem data is the requirement that $\\mathbf{y}(t_0) = \\mathbf{y}_0$.\n",
    "\n",
    "Under reasonable assumptions, this additional requirement determines a unique solution to the given ODE. \n",
    "Because the independent variable $t$ often represents time, we think of $t_0$ as the initial time and $\\mathbf{y}_0$ as the initial value of the state vector. \n",
    "\n",
    "Accordingly, the requirement that $\\mathbf{y}(t_0) = \\mathbf{y}_0$ is called an **initial condition**, and an ODE together with an initial condition is called an **initial value problem**, or **IVP**.\n",
    "\n",
    "\n",
    "Starting from its initial state $\\mathbf{y}_0$ at time $t_0$ , the ODE governs the dynamic evolution of the system for $t \\geq t_0$ , and we seek a function $\\mathbf{y}(t)$ that satisfies the initial condition and describes the state of the system as a function of time.\n",
    "\n",
    "---\n",
    "\n",
    "If we integrate the ODE $\\mathbf{y}' = \\mathbf{f}(t, \\mathbf{y})$ and use the initial condition $\\mathbf{y}(t_0) = \\mathbf{y}_0$,\n",
    "we obtain the integral equation\n",
    "\n",
    "$$\\mathbf{y}(t)=\\mathbf{y}_0+\\int_{t_0}^t\\mathbf{f}(s)ds$$\n",
    "\n",
    "which explains, why solving an ODE, by whatever means, is often referred to as **integrating the ODE**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d616c-102b-45cd-b9ad-94713fe53e44",
   "metadata": {},
   "source": [
    "## Numerically solving ODE's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f950b06c",
   "metadata": {},
   "source": [
    "Our approach to solving differential equations numerically will be based on discretization:\n",
    "\n",
    "We will replace differential equations by algebraic equations whose solutions approximate those of the given differential equations. \n",
    "For an initial value problem, approximate solution values are generated step by step in discrete increments across the interval in which the solution is sought. \n",
    "For this reason, numerical methods for solving ODEs are sometimes called discrete variable methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ed237",
   "metadata": {},
   "source": [
    "A numerical solution of an IVP is obtained by starting at time $t_0$ with the given initial value $\\mathbf{y}_0$ and attempting to track the solution trajectory dictated by the ODE. \n",
    "\n",
    "We can determine the initial slope $\\mathbf{y}'_0$ of each component of the solution by evaluating $\\mathbf{f}$ at the given initial data, i.e., $\\mathbf{y}'_0 = \\mathbf{f}(t_0,\\mathbf{y}_0)$.\n",
    "We use this information to predict the value $\\mathbf{y}_1$ of the solution at some future time $t_1 = t_0 + h_0$ for some suitably chosen increment $h_0$.\n",
    "We can then evaluate $\\mathbf{y}'_1 = \\mathbf{f}(t_1 , \\mathbf{y}_1)$ and repeat the process to take another step forward, and so on until we reach the final desired time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e588a1-f0d9-4d04-8958-81519ef4f9b1",
   "metadata": {},
   "source": [
    "### Euler forward method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33ce0b9",
   "metadata": {},
   "source": [
    "The simplest example of this approach is **Euler’s method**, for which the approximate solution at time $t_{k+1} = t_k + h_k$ is given by \n",
    "\n",
    "$$\\mathbf{y}_{k+1} = \\mathbf{y}_k + h_k \\mathbf{f}(t_k, \\mathbf{y}_k)$$\n",
    "\n",
    "Euler’s method is an example of a **single-step method** in which the next approximate solution value depends only on the current values of $t_k$ , $\\mathbf{y}_k$, and $h_k$.\n",
    "\n",
    "For reasons we will soon see, Euler’s method is generally inefficient, so it is seldom used in practice, but it is of fundamental importance in understanding the basic concepts and principles in solving differential equations numerically\n",
    "\n",
    "Euler's method can be derived in several ways:\n",
    "\n",
    "- **Finite diffference appoximation**\n",
    "\n",
    "If we replace the derivative $\\mathbf{y}'(t)$ in the ODE $\\mathbf{y}' = \\mathbf{f}(t, \\mathbf{y})$ by a first-order forward difference approximation (see notebook on integration and differentiation), we obtain an algebraic equation\n",
    "\n",
    "$$\n",
    "\\frac{\\mathbf{y}_{k+1}-\\mathbf{y}_k}{h_k}=\\mathbf{f}(t_k,\\mathbf{y}_k)\n",
    "$$\n",
    "\n",
    "which gives Euler’s method when solved for $y_{k+1}$.\n",
    "\n",
    "- **Taylor Series** \n",
    "\n",
    "Consider the Taylor series\n",
    "\n",
    "$$\n",
    "\\mathbf{y}(t + h) = \\mathbf{y}(t) + h \\mathbf{y}'(t) + 1/2 h^2 \\mathbf{y}''(t) + \\cdots\n",
    "$$\n",
    "\n",
    "Euler’s method results from taking $t = t_k$ , $h = h_k$ , $\\mathbf{y}'(t_k ) = \\mathbf{f}(t_k , \\mathbf{y}_k )$, and dropping terms of second and higher order.\n",
    "\n",
    "This is actually a very convenient and powerful method to construct solvers and we'll see a more advanced example further below when we construct our very own Runge-Kutta solver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7717874e",
   "metadata": {},
   "source": [
    "> **Example**\n",
    ">\n",
    "> Consider the ODE $y'=y$ with initial value $y_0$ at $t_0=0$\n",
    ">\n",
    "> This simple problem is easily solved analytically, but for illustration let us apply Euler’s method to solve it numerically.\n",
    "> For simplicity, we will use a fixed step size $h$. We first advance the solution from time $t_0 = 0$ to time $t_1 = t_0 + h$\n",
    ">\n",
    "> $$y_1=y_0+hy'_0=y_0+hy_0= (1+h)y_0$$\n",
    ">\n",
    "> Note that the approximate solution value $y_1$ we obtain at $t_1$ is not exact (i.e., $y_1 \\neq y(t_1 ))$.\n",
    "> \n",
    "> For example, if $t_0 = 0$, $y_0 = 1$, and $h = 0.5$, then $y_1 = 1.5$, whereas the exact solution for this initial value is $y(0.5) = \\exp(0.5) \\approx 1.649$.\n",
    ">\n",
    "> Thus, the value $y_1$ lies on a different solution of the ODE from the one on which we started, as illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff484fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_unstable_solution():\n",
    "    \"\"\"Illustrate the (in)stability of the solution of the ODE y'=y with y(0)=1\n",
    "    when integrated with a sufficiently small step size h=0.5.\n",
    "    \"\"\"\n",
    "\n",
    "    def exact(x, c):\n",
    "        return np.exp(x - c)\n",
    "\n",
    "    x = np.linspace(0, 5, 100)\n",
    "    steps: int = 12\n",
    "    t = np.linspace(0, 5, steps)\n",
    "    h = (t[0] + t[-1]) / steps\n",
    "    y = np.zeros(steps)\n",
    "    y[0] = 1\n",
    "    for i in np.arange(1, steps):\n",
    "        y[i] = y[i - 1] + h * y[i - 1]\n",
    "\n",
    "    plt.close(\"unstable\")\n",
    "    fig, ax = plt.subplots(num=\"unstable\")\n",
    "    for i in np.linspace(0, 1.6, 5):\n",
    "        ax.plot(\n",
    "            x, exact(x, i), \"-\", c=\"#1f77b4\" if i != 0 else \"r\", label=\"y(O) = 1\"\n",
    "        )\n",
    "\n",
    "    ax.plot(t, y, \"o-\", c=\"k\")\n",
    "    ax.axis([0, 5, 0, 100])\n",
    "\n",
    "\n",
    "demo_unstable_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff9f81c",
   "metadata": {},
   "source": [
    "We can continue to take additional steps, generating a table of discrete values of the approximate solution over whatever interval we desire. \n",
    "As we do so, we will hop from one solution to another at each step. \n",
    "The solutions of this ODE are unstable, so the errors we make at each step are amplified with time as a result of the divergence of the solutions, as could be seen in the previous figure\n",
    "\n",
    "For an equation with stable solutions, on the other hand, the errors in the numerical solution do not grow, and for an equation with asymptotically stable solutions, such as $y' = -y$, the errors diminish with time, as shown in the next figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23357f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_stable_solution():\n",
    "    \"\"\"Illustrate the stability of the solution of the ODE y'=-y with y(0)=1\n",
    "    when integrated with a sufficiently small step size h=0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def exact(x, c):\n",
    "        return np.exp(-x - c)\n",
    "\n",
    "    x = np.linspace(0, 5, 100)\n",
    "    steps: int = 12\n",
    "    t = np.linspace(0, 5, steps)\n",
    "    h = (t[0] + t[-1]) / steps\n",
    "    y = np.zeros(steps)\n",
    "    y[0] = 1\n",
    "    for i in np.arange(1, steps):\n",
    "        y[i] = y[i - 1] - h * y[i - 1]\n",
    "\n",
    "    plt.close(\"stable\")\n",
    "    fig, ax = plt.subplots(num=\"stable\")\n",
    "    for i in np.linspace(0, 1.6, 5):\n",
    "        ax.plot(\n",
    "            x, exact(x, i), \"-\", c=\"#1f77b4\" if i != 0 else \"r\", label=\"y(O) = 1\"\n",
    "        )\n",
    "\n",
    "    ax.plot(t, y, \"o-\", c=\"k\")\n",
    "    ax.axis([0, 5, 0, 1])\n",
    "\n",
    "\n",
    "demo_stable_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac214021-742a-44fd-8377-a517bf550b19",
   "metadata": {},
   "source": [
    "### Accuracy and Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b7929d",
   "metadata": {},
   "source": [
    "**Rounding error and truncation error**\n",
    "\n",
    "Like other methods that approximate derivatives by finite differences, a numerical procedure for solving an ODE suffers from two distinct sources of error:\n",
    "\n",
    "- **Rounding error** , which is due to the finite precision of floating-point arithmetic\n",
    "- **Truncation error** (or **discretization error**), which is due to the method used, and which would remain, even if all arithmetic were performed exactly\n",
    "\n",
    "Although they arise from different sources, these two types of errors are not independent of each other.\n",
    "For example, the truncation error can usually be reduced by using a smaller step size $h$, but doing so may incur greater rounding error. \n",
    "\n",
    "In most practical situations, truncation error is the dominant factor in determining the accuracy of numerical solutions of ODEs, so we will ignore rounding error in this context.\n",
    "\n",
    "**Local and global error**\n",
    "\n",
    "The truncation error at the $k$th step comes in two distinct but related flavors:\n",
    "\n",
    "- **Global error** is the cumulative overall error\n",
    "\n",
    "$$\\mathbf{e}_k=\\mathbf{y}_k-\\mathbf{y}(t_k)$$\n",
    "\n",
    "where $\\mathbf{y}_k$ is the computed solution at $t_k$ and $\\mathbf{y}(t)$ is the true solution of the ODE passing through the initial point ($t_0$ ,$\\mathbf{y}_0$).\n",
    "\n",
    "- **Local error** is the error made in one step of the numerical method,\n",
    "\n",
    "$$\\boldsymbol{\\ell}_k=\\mathbf{y}_k-\\mathbf{u}_{k-1}(t_k)$$\n",
    "\n",
    "where $\\mathbf{u}_{k-1}$ is the solution of the ODE passing through the previous point ($t_{k−1}$ , $\\mathbf{y}_{k−1}$ ).\n",
    "\n",
    "The global error is obviously of primary interest, but only the local error can be readily estimated and controlled, so we need to understand the relationship between the two.\n",
    "\n",
    "In a bank savings account earning compound interest, early deposits have more time to grow than later ones, and this growth means that the total value of the account is not simply the sum of the individual deposits. \n",
    "Similarly, the global error of an approximate solution to an ODE at a given step reflects not only the local error at that step, but also the compounded effects of the local errors at all previous steps.\n",
    "Thus, the global error is not simply the sum of the local errors. \n",
    "If the solutions of the ODE are diverging, then the local errors at each step are magnified over time, so that the global error is greater than the sum of the local errors.\n",
    "If the solutions of the ODE are converging, on the other hand, then the global error may\n",
    "be less than the sum of the local errors.\n",
    "In order to assess the effectiveness of a numerical method, we need to characterize both its local error (accuracy) and the compounding effects over multiple steps (stability).\n",
    "\n",
    "**Accuracy**\n",
    "\n",
    "The accuracy of a numerical method is said to be of order $p$ if\n",
    "\n",
    "$$\\boldsymbol{\\ell}_k=\\mathcal{O}(h_k^{p+1})$$\n",
    "\n",
    "The motivation for this definition, with the order of accuracy one less than the exponent of the step size in the local error, is that if the local error is $\\mathcal{O}(h_k^{p+1})$, then the local error per unit step, $\\boldsymbol{\\ell}_k /h_k$, is $\\mathcal{O}(h_k^p)$, and it can be shown that under reasonable conditions the global error $\\mathbf{e}_k$ is $\\mathcal{O}(h^p)$, where $h$ is the average step size.\n",
    "\n",
    "\n",
    "**Stability** \n",
    "\n",
    "The concept of stability of a numerical method for an ODE is analogous to the stability of solutions to an ODE:\n",
    "\n",
    "Recall that a solution to an ODE is stable if perturbations of the solution do not diverge away from it over time. Similarly, a numerical method is said to be stable if small perturbations do not cause the resulting numerical solution to diverge away without bound. \n",
    "\n",
    "Such divergence of numerical solutions could be caused by instability of the solution to the ODE, but as we will see, it can also be caused by the numerical method itself, even when the solutions to the ODE are stable. \n",
    "\n",
    "To focus specifically on instability due to the numerical method, an alternate definition of stability requires that the numerical solution at any arbitrary but fixed time $t$ remains bounded as $h \\rightarrow 0$. \n",
    "The two definitions are effectively equivalent, however, as either definition prohibits excessive growth as the number of steps becomes arbitrarily large\n",
    "\n",
    "> **example**\n",
    "> \n",
    "> Let us first examine stability and accuracy in the simple context of Euler’s method applied to the scalar ODE \n",
    ">\n",
    "> $$y'= \\lambda y$$\n",
    ">\n",
    "> where $\\lambda$ is a (possibly complex) constant. \n",
    ">\n",
    "> With initial condition $y(0) = y_0$ , the exact solution to the IVP is given by \n",
    ">\n",
    "> $$y(t) = y_0 e^{\\lambda t}$$\n",
    ">\n",
    "> Applying Euler’s method to this ODE using a fixed step size $h$, we have the recurrence\n",
    ">\n",
    "> $$y_{k+1} = y_k + h \\lambda y_k = (1 + h \\lambda) y_k$$\n",
    ">\n",
    "> which implies that\n",
    ">\n",
    "> $$y_k = (1 + h\\lambda)^k y_0$$\n",
    ">\n",
    "> The quantity $1 + h \\lambda$ is called the **growth factor**.\n",
    ">\n",
    "> - If Re$(\\lambda) < 0$, then the exact solution of the ODE decays to zero as t increases, as will the successive computed solution values if $\\|1+h \\lambda\\| < 1$. \n",
    ">\n",
    "> - If $\\|1+h\\lambda\\| > 1$, on the other hand, then the computed solution values grow without bound regardless of the sign of Re$(\\lambda)$, which means that Euler’s method can be unstable even when the exact solution is stable.\n",
    ">\n",
    "> In order for Euler’s method to be stable, the step size h must satisfy the inequality\n",
    ">\n",
    "> $$\\|1 + h \\lambda\\| \\leq 1$$\n",
    ">\n",
    "> which says that $h \\lambda$ must lie inside a circle in the complex plane of radius 1 centered at -1. \n",
    ">\n",
    "> If $\\lambda$ is real, then $h\\lambda$ must lie in the interval $(-2, 0)$, which means that for $\\lambda < 0$, we must have $h \\leq −2/\\lambda$ for Euler’s method to be stable. \n",
    ">\n",
    "> We also note that the growth factor 1 + hλ agrees with the series expansion\n",
    ">\n",
    "> $$e^{h\\lambda}=1+h\\lambda+\\frac{(h\\lambda)^2}{2}+\\frac{(h\\lambda)^3}{6}+\\cdots$$\n",
    ">\n",
    "> through terms of first order in h, so the accuracy of Euler’s method is of first order.\n",
    ">\n",
    "> This stability criterion is illustrated in the demo below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89942b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_euler_stability():\n",
    "    \"\"\"This demo illustrates the stability of the Euler forward method when\n",
    "    integrating y'=-y with  y(0)=10 from 0 to 10000 as function of the step\n",
    "    size taken in this interval.\n",
    "\n",
    "    We expect the method to be stable if 0<h<2,\n",
    "    and print the error for h=1.99,2.00 and 2.01.\n",
    "    \"\"\"\n",
    "\n",
    "    def int(h):\n",
    "        y = 10.0\n",
    "        tmax = 10000.0\n",
    "        for _t in np.arange(0.0, tmax + h, h):\n",
    "            y = y - h * y\n",
    "        return np.abs(y - 10 * np.exp(-tmax))\n",
    "\n",
    "    print(\"h\\t error\")\n",
    "    for h in np.arange(1.99, 2.011, 0.01):\n",
    "        print(h, \"\\t\", int(h))\n",
    "\n",
    "\n",
    "demo_euler_stability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856fc8e4-ea2d-4f1b-9215-83cd29eee401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_euler_stability():\n",
    "    \"\"\"Plot the results of the Euler method for different step sizes\n",
    "    and compare with the exact solution.\n",
    "\n",
    "    This function creates a 1x3 grid of subplots to display the numerical\n",
    "    approximation and exact solution for each step size(h) using the Euler\n",
    "    forward method.\n",
    "    \"\"\"\n",
    "    plt.close(\"euler_stab\")\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(8, 3), sharey=True, num=\"euler_stab\")\n",
    "\n",
    "    h_values = np.arange(1.99, 2.011, 0.01)\n",
    "\n",
    "    for idx, h in enumerate(h_values):\n",
    "        y = 10\n",
    "        y_list = [y]\n",
    "        t_values = np.arange(0, 200, h)\n",
    "        for _t in t_values:\n",
    "            y -= h * y\n",
    "            y_list.append(y)\n",
    "\n",
    "        axs[idx].plot(t_values, y_list[:-1], label=\"Euler\")\n",
    "        axs[idx].plot(\n",
    "            t_values,\n",
    "            10 * np.exp(-t_values),\n",
    "            label=r\"Exact: $10\\, e^{-x}$\",\n",
    "            linestyle=\"-\",\n",
    "            color=\"orange\",\n",
    "            linewidth=3,\n",
    "        )\n",
    "        axs[idx].set_title(f\"Step size $h = {h:.2f}$\")\n",
    "        axs[idx].set_xlabel(\"Time\")\n",
    "        if idx == 0:\n",
    "            axs[idx].legend()\n",
    "            axs[idx].set_ylabel(\"Value\")\n",
    "\n",
    "\n",
    "plot_euler_stability()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732da16c",
   "metadata": {},
   "source": [
    "**General case**\n",
    "\n",
    "A more general analysis produces the same stability and accuracy results as we obtained using the simple scalar test equation.\n",
    "Especially for more complicated numerical methods, this simple scalar test ODE is far easier to work with than a general ODE, and it produces essentially the same stability results if we equate the complex coefficient $\\lambda$ with the eigenvalues of the Jacobian matrix $\\mathbf{J}_f$ of $\\mathbf{f}$ with respect to $\\mathbf{y}$ at a given point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35007954-d2f4-452d-b343-092c6e17b8e1",
   "metadata": {},
   "source": [
    "### Implicit methods (Euler backward method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfd6ad",
   "metadata": {},
   "source": [
    "Euler’s method is an **explicit method** in that it uses only information at time $t_k$ to\n",
    "advance the solution to time $t_{k+1}$ . \n",
    "\n",
    "Methods which also use information at time $t_{k+1}$ are called **implicit methods**.\n",
    "\n",
    "The simplest example is the **Euler backward** method\n",
    "\n",
    "$$\\mathbf{y}_{k+1}=\\mathbf{y}_{k}+h_k\\mathbf{f}(t_{k+1},\\mathbf{y}_{k+1})$$\n",
    "\n",
    "> **Derivation**\n",
    ">\n",
    "> This method can again be derived using a Taylor expansion:\n",
    ">\n",
    "> Consider the Taylor series\n",
    ">\n",
    "> $$\\mathbf{y}(t - h) = \\mathbf{y}(t) - h \\mathbf{y}'(t) + \\frac{1}{2}h^2 \\mathbf{y}''(t) + \\cdots$$\n",
    ">\n",
    "> By taking $t = t_{k+1}$ , $h = h_k$ , $\\mathbf{y}'(t_{k+1} ) = \\mathbf{f}(t_{k+1} , \\mathbf{y}_{k+1} )$, and dropping terms of second and higher order, we find:\n",
    ">\n",
    "> $$\\mathbf{y}(t_{k+1}-h) = \\mathbf{y}(t_{k+1}) - h_k \\mathbf{f}(t_{k+1},\\mathbf{y}_{k+1})$$\n",
    ">\n",
    "> Because $t_{k+1}-h=t_k$, and by isolating $\\mathbf{y}(t_{k+1})$, this is Euler's backward method.\n",
    "\n",
    "The backward Euler method is implicit because we must evaluate $f$ with the argument $y_{k+1}$ *before we know its value*.\n",
    "This is not a problem because this statement simply means that a value for $y_{k+1}$ that satisfies the preceding equation must be determined, and if $f$ is a nonlinear function of $y$, as is often the case, then an iterative solution method, as seen in the nonlinear equations notebook, must be used. \n",
    "\n",
    "A good starting guess for the iteration can be obtained from an explicit method, such as Euler’s method, or from the solution at the previous time step.\n",
    "\n",
    "> **Example**\n",
    ">\n",
    "> Consider the nonlinear scalar ODE $y' = −y^3$ with initial condition $y(0) = 1$.\n",
    ">\n",
    "> Using the backward Euler method with a step size of $h = 0.5$, we obtain the equation\n",
    ">\n",
    "> $$y_1=y_0+hf(t_1,y_1)=1-0.5y_1^3$$\n",
    "> \n",
    "> for the solution value at the next step. \n",
    ">\n",
    "> You can solve this nonlinear equation using fixed-point-iteration with an initial guess given by the current value of $y_0=1$, or using an initial guess given by the solution of the Euler forward method, as is done below.\n",
    ">\n",
    "> $$y_1=y_0-0.5y_0^3=0.5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3834e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize.fixed_point((lambda y: 1.0 - 0.5 * y**3), 0.5, method=\"iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e69af7",
   "metadata": {},
   "source": [
    "> The analytical solution to this problem is\n",
    ">\n",
    "> $$y=\\frac{1}{\\sqrt{2x+1}}$$\n",
    ">\n",
    "> resulting in a value of $y(0.5)=0.7071067811865475$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf7380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical(x):\n",
    "    return 1.0 / np.sqrt(2.0 * x + 1.0)\n",
    "\n",
    "\n",
    "analytical(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec193a",
   "metadata": {},
   "source": [
    "> Note that both the Euler forward (0.5) and Euler backward (0.77) methods result in an estimate that is quite far from the real value, given the large step size of 0.5.\n",
    ">\n",
    "> Repeating this procedure with a smaller step size results in better estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c35034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_stepsize(N):\n",
    "    \"\"\"Function that show the accuracy of the Euler forward\n",
    "    and Euler backward functions to numerically integrate\n",
    "    y'=-y^3 with y(0)=1 from 0 to 0.5\n",
    "    as function of the step size taken in this interval\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        N\n",
    "            A measure for the number of steps taken between 0 and 1.\n",
    "            N=1 means that h=0.5,\n",
    "            N=2 that h=0.25\n",
    "            and so on.\n",
    "    \"\"\"\n",
    "\n",
    "    # analytical solution\n",
    "    def func(x):\n",
    "        return 1.0 / np.sqrt(2.0 * x + 1.0)\n",
    "\n",
    "    # check with euler forward with smaller steps sizes of 0.5/N\n",
    "    y = 1\n",
    "    for _ in np.arange(N):\n",
    "        y = y - 0.5 / N * y**3.0\n",
    "\n",
    "    print(\"Euler forward result\\n----------------------------\")\n",
    "    print(\"Step size, result, error\")\n",
    "    print(0.5 / N, y, np.abs(y - func(0.5)))\n",
    "\n",
    "    # same for euler backward\n",
    "\n",
    "    y = 1\n",
    "    for _ in np.arange(N):\n",
    "        yold = y\n",
    "        y = optimize.fixed_point(\n",
    "            (lambda ynew, yold=yold: yold - 0.5 / N * ynew**3),\n",
    "            yold,\n",
    "            method=\"iteration\",\n",
    "        )\n",
    "\n",
    "    print(\"\\nEuler backward result\\n----------------------------\")\n",
    "    print(\"Step size, result, error\")\n",
    "    print(0.5 / N, y, np.abs(y - func(0.5)))\n",
    "\n",
    "\n",
    "demo_stepsize(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1be68c",
   "metadata": {},
   "source": [
    "**Stability of the Euler backward method**\n",
    "\n",
    "Given the extra trouble and computation in using an implicit method, one might wonder why we would bother. \n",
    "The answer is that implicit methods generally have a significantly larger stability region than comparable  explicit methods. \n",
    "To determine the stability and accuracy of the backward Euler method, we apply it to the scalar test ODE $y'=\\lambda y$, obtaining\n",
    "\n",
    "$$y_{k+1}=y_k+h\\lambda y_{k+1}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$(1-h\\lambda)y_{k+1}=y_k$$\n",
    "\n",
    "so that\n",
    "\n",
    "$$y_k=\\left(\\frac{1}{1-h\\lambda}\\right)^k y_0$$\n",
    "\n",
    "Thus, for the backward Euler method to be stable we must have\n",
    "\n",
    "$$\\left|\\frac{1}{1-h\\lambda}\\right|\\leq 1$$\n",
    "\n",
    "which holds for any $h > 0$ when Re$(\\lambda) < 0$.\n",
    "Thus, the stability region for the backward Euler method includes the entire left half of the complex plane, or the interval $(-\\infty, 0)$ if $\\lambda$ is real, and there is no stability restriction on the step size\n",
    "when computing a stable solution. The growth factor\n",
    "\n",
    "$$\\frac{1}{1-h\\lambda}=1+h\\lambda+(h\\lambda)^2+\\cdots$$\n",
    "\n",
    "agrees with the expansion for $e^{h\\lambda}$ through terms of order $h$, so the backward Euler method is first-order accurate.\n",
    "\n",
    "For any ODE, the stability region for the backward Euler method includes the entire left half of the complex plane, and hence for computing a stable solution, the method is stable for any positive step size. \n",
    "Such a method is said to be **unconditionally stable**.\n",
    "The great virtue of an unconditionally stable method is that the desired local accuracy places the only constraint on our choice of step size.\n",
    "Thus, we may be able to take much larger steps than for an explicit method of comparable order and attain much higher overall efficiency despite requiring more computation per step because of having to solve an equation at each step of the implicit method.\n",
    "\n",
    "However, not all implicit methods have this property. \n",
    "Implicit methods generally have larger stability regions than explicit methods, but the allowable step size is\n",
    "not always unlimited. \n",
    "Implicitness alone is not sufficient to guarantee stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e5f2f3-30ed-42f1-8067-812739ff8dc4",
   "metadata": {},
   "source": [
    "###  Stiffness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e88fa1",
   "metadata": {},
   "source": [
    "**Stiffness** is a concept that can be defined a number of ways. For us, the most meaningful way is its correspondence to the physics behind the problems we are investigating.\n",
    "\n",
    "If a system contains dynamics on very different timescales, like a slow relaxation towards a certain equilibrium, but with rapid oscillations around it, or with very strongly damped transients, then it is considered stiff.\n",
    "\n",
    "Mathematically, a stable ODE $\\mathbf{y}_0 = \\mathbf{f}(t, \\mathbf{y})$ is stiff if its Jacobian\n",
    "matrix $\\mathbf{J}_f$ has eigenvalues that differ greatly in magnitude. \n",
    "There may be eigenvalues with relatively large negative real parts (corresponding to strongly damped components of the solution) or relatively large imaginary parts (corresponding to rapidly oscillating components of the solution).\n",
    "\n",
    "Some numerical methods are very inefficient for stiff equations because the rapidly varying component of the solution forces very small step sizes to be used to maintain stability.\n",
    "Since the stability restriction depends on the rapidly varying component of the solution, whereas the accuracy restriction depends on the slowly varying component, the step size may be much more severely restricted by stability than by the required accuracy. \n",
    "\n",
    "Euler’s forward method, for example, is extremely inefficient for solving a stiff equation because of its small stability region. The unconditional stability of the implicit backward Euler method, on the other hand, makes it suitable for stiff problems. \n",
    "\n",
    "Stiff ODEs need not be difficult to solve numerically, provided a suitable method, generally implicit, is chosen.\n",
    "This is illustrated with an example below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e91755",
   "metadata": {},
   "source": [
    "> **Example**\n",
    ">\n",
    "> Consider the stiff ODE\n",
    ">\n",
    "> $$y'=-100y+100t+101$$\n",
    "> \n",
    "> with $y(0)=1$.\n",
    "> \n",
    "> The analytical solution to this equation (in general) is $1+t+ce^{-100t}$ (i.e. with a very strongly damped component). For our specific initial value, $c=0$, so the solution becomes the linear equation $y(t)=1+t$, which in principle would be very well suited to solve with Euler's forward method.\n",
    ">\n",
    "> However, to illustrate the effect of truncation or rounding errors,\n",
    "> let us perturb the initial value slightly. \n",
    ">\n",
    "> With a step size h = 0.1, the first few steps for the given initial values are given for the analytical solution in the following table\n",
    ">\n",
    "> | t    | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 |\n",
    "> |------|-----|-----|-----|-----|-----|-----|\n",
    "> | y(t) | 1   | 1.1 | 1.2 | 1.3 | 1.4 | 1.5 |\n",
    ">\n",
    ">\n",
    "> Let's compare this with the numerical solutions obtained with the Euler forward and backward methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdbc9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_stiff():\n",
    "    \"\"\"Function that show the accuracy of the Euler forward\n",
    "    and Euler backward functions to numerically integrate\n",
    "    a very stiff problem\n",
    "\n",
    "    The output loooks as follows:\n",
    "    - the first column will contain the time points\n",
    "    - the second column will contain the analytical results\n",
    "    - the third column the approximation using euler forward\n",
    "    - the fourth column the approximation using euler backward\n",
    "    \"\"\"\n",
    "\n",
    "    # define empty array with correct dimension to store the output\n",
    "    results = np.zeros((6, 4))\n",
    "\n",
    "    # define 6 timepoints, 0,0.1,...0.5 and the analytical solution\n",
    "    results[:, 0] = np.arange(0.0, 0.6, 0.1)\n",
    "    results[:, 1] = results[:, 0] + 1\n",
    "\n",
    "    # define function y'=-100y+100t+101\n",
    "    def func(y, t):\n",
    "        return -100 * y + 100 * t + 101\n",
    "\n",
    "    # define helper function to print header\n",
    "    def print_header():\n",
    "        print(\"    \", \"t,   analytical, forward, backward\")\n",
    "\n",
    "    # define helper function to print results\n",
    "    def print_results():\n",
    "        for i in np.arange(6):\n",
    "            print(results[i, :])\n",
    "\n",
    "    # euler forward method, starting from slightly perturbed initial value y=0.99\n",
    "    results[0, 2] = 0.99\n",
    "    for i in np.arange(1, 6):\n",
    "        results[i, 2] = results[i - 1, 2] + 0.1 * func(\n",
    "            results[i - 1, 2], results[i - 1, 0]\n",
    "        )\n",
    "\n",
    "    # euler backwards method, starting from very perturbed initial value y=0.\n",
    "    results[0, 3] = 0.0\n",
    "    for i in np.arange(1, 6):\n",
    "        # Because f is linear in y, it's very easy to explicitly write\n",
    "        # the analytical solution to  y1 = y0 + h*yf(t1,y1),\n",
    "        # instead of using an iterative method y1 = [y0+h(100t1+101)] / (101*h)\n",
    "        results[i, 3] = (\n",
    "            1.0 / 11.0 * (results[i - 1, 3] + 0.1 * (100.0 * results[i, 0] + 101.0))\n",
    "        )\n",
    "\n",
    "    print_header()\n",
    "    with np.printoptions(precision=3, suppress=True):\n",
    "        print(results)\n",
    "\n",
    "    plt.close(\"stiff\")\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), num=\"stiff\")\n",
    "    ax1.plot(results[:, 0], results[:, 2], label=\"euler forward method\")\n",
    "    ax2.plot(results[:, 0], results[:, 3], label=\"euler backwards method\")\n",
    "    for ax in (ax1, ax2):\n",
    "        ax.set(xlabel=\"t\", ylabel=\"y(t)\")\n",
    "        ax.plot(results[:, 0], results[:, 1], label=\"analytical solution\")\n",
    "        ax.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "demo_stiff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7c87d",
   "metadata": {},
   "source": [
    "> As you can see, even a tiny perturbation ruins the solution obtained using the Euler forward solver, whereas even a large perturbation eventually damps out as the result tends toward the correct solution using Euler backward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a2623-6d42-429e-a26c-bb5f31d28ee7",
   "metadata": {},
   "source": [
    "### Runge-Kutta methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d40ef",
   "metadata": {},
   "source": [
    "We can construct better methods, by not only relying on the first derivative of $y'$, but also taking into account higher order derivatives. Such methods are called **Taylor series methods**. However such methods are only practical if the higher order derivatives of $y$ are known analytically and are practical to calculate. \n",
    "\n",
    "**Runge-Kutta methods** are single-step methods that are similar in motivation to Taylor series methods but do not involve explicit computation of higher derivatives. \n",
    "Instead, Runge-Kutta methods replace higher derivatives by finite difference approximations based on values of $f$ at points between $t_k$ and $t_{k+1}$.\n",
    "\n",
    "This requires some bootstrapping to obtain the necessary values of $f$, since we do not know the second argument of $f$ , namely the solution $y(t)$, for $t$ between $t_k$ and $t_{k+1}$.\n",
    "\n",
    "\n",
    "The best-known Runge-Kutta method is the classical fourth-order scheme\n",
    "\n",
    "$$\\mathbf{y}_{k+1}=\\mathbf{y}_k+\\frac{h_k}{6}(\\mathbf{k}_1+2\\mathbf{k}_2+2\\mathbf{k}_3+\\mathbf{k}_4)$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\\begin{split}\n",
    "\\mathbf{k}_1 &= \\mathbf{f}(t_k,\\mathbf{y}_k) \\\\\n",
    "\\mathbf{k}_2 &= \\mathbf{f}(t_k+h_k/2,\\mathbf{y}_k+(h_k/2)\\mathbf{k}_1) \\\\\n",
    "\\mathbf{k}_3 &= \\mathbf{f}(t_k+h_k/2,\\mathbf{y}_k+(h_k/2)\\mathbf{k}_2) \\\\\n",
    "\\mathbf{k}_4 &= \\mathbf{f}(t_k+h_k,\\mathbf{y}_k+h_k\\mathbf{k}_3) \\\\\n",
    "\\end{split}$$\n",
    "\n",
    "Runge-Kutta methods have a number of virtues. \n",
    "- To proceed to time $t_{k+1}$, they require no history of the solution prior to time $t_k$\n",
    "     - which makes them self-starting at the beginning of the integration\n",
    "     - and also makes it easy to change the step size during the integration\n",
    "- These features also make Runge-Kutta methods relatively easy to program, which accounts in part for their popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d15b319-70f8-4c4f-abd5-20494b7a5a6c",
   "metadata": {},
   "source": [
    "#### Butcher tableaus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26db9c0",
   "metadata": {},
   "source": [
    "The notation to describe these solvers quickly becomes quite dense and difficult to read. Butcher invented a method to write down all necessary information to understand and implement a solver in a very elegant way: **Butcher tableaus**.\n",
    "\n",
    "In general, the approximate solution of the ODE\n",
    "\n",
    "$$\\frac{dy}{dt}=f(t,y)$$\n",
    "\n",
    "at time $t+h$, given its value $y$ on time $t$, is obtained by taking $s$ intermediate evaluations of $f$ at times $c_i$,\n",
    "\n",
    "$$k_i=f\\left(t+c_ih,y_n+h\\sum^s_{j=1}a_{ij}k_j\\right)$$\n",
    "\n",
    "and then adding them to $y_n$ with the correct weights $b_i$:\n",
    "\n",
    "$$y_{n+1}=y_n+h\\sum^s_{i=1}b_ik_i$$\n",
    "\n",
    "These formulas can compactly be represented by a Butcher tableau as follows:\n",
    "\n",
    "$$\\begin{array}{c|cccc}\n",
    "c_1 & a_{11} & a_{12} & \\cdots & a_{1s} \\\\\n",
    "c_2 & a_{21} & a_{22} & \\cdots & a_{2s} \\\\\n",
    "\\vdots & \\vdots & \\vdots &\\ddots & \\vdots \\\\\n",
    "c_s & a_{s1}& a_{s2}& \\cdots &a_{ss} \\\\ \\hline\n",
    "& b_1 & b_2 & \\cdots & b_s \n",
    "\\end{array}$$\n",
    "\n",
    "If the tableau only contains elements below the diagonal, then it corresponds to an **explicit** solver. Otherwise it is an **implicit** solver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde8b919",
   "metadata": {},
   "source": [
    "Below, the Butcher tableaus of all solvers mentioned above are given. For the methods which have a lower order solution embedded, the lower order solution is shown as an extra row below the higher order solution. The difference between both solutions approximates the error.\n",
    "\n",
    "- **Euler Forward**\n",
    "\n",
    "    $$\\begin{array}{c|c}\n",
    "      0 & \\\\ \\hline\n",
    "      & 1\n",
    "    \\end{array}$$\n",
    " \n",
    "- **Euler Backward**\n",
    " \n",
    "    $$\\begin{array}{c|c}\n",
    "      1 & 1 \\\\ \\hline\n",
    "      & 1\n",
    "    \\end{array}$$\n",
    " \n",
    "- **4th order Runke-Kutta solver**\n",
    "\n",
    "    $$\\begin{array}{c|cccc}\n",
    "    0 &  &  &  &  \\\\\n",
    "    \\frac{1}{2} & \\frac{1}{2} &  &  &  \\\\\n",
    "    \\frac{1}{2} & 0 & \\frac{1}{2} &  &  \\\\\n",
    "    1 & 0 & 0 & 1 &  \\\\ \\hline\n",
    "    & \\frac{1}{6} & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{6}\n",
    "    \\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee36632",
   "metadata": {},
   "source": [
    "#### Example derivation (2nd order Runge-Kutta method)\n",
    "\n",
    "We are trying to solve the differential equation\n",
    "\n",
    "$$\\frac{dy}{dt} = f(t, y)$$\n",
    "\n",
    "with second-order accuracy using what will turn out to be Heun's method.\n",
    "\n",
    "The Butcher tableau for this method looks like:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc}\n",
    "0 &  &  \\\\\n",
    "c_1 & a_{21} &  \\\\ \\hline\n",
    "& b_1 & b_2\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We'll introduce shortcuts for notational clarity:\n",
    "\n",
    "- $f(t, y) = f$\n",
    "- Partial derivatives are written as a subscript:\n",
    "    - $\\frac{\\partial f}{\\partial t}=f_t$\n",
    "    - $\\frac{\\partial f}{\\partial y}=f_y$\n",
    "\n",
    "\n",
    "\n",
    "We use the general form for a second-order Runge-Kutta method:\n",
    "\n",
    "$$\n",
    "k_1 = f(t, y_n)\n",
    "$$\n",
    "\n",
    "$$\n",
    "k_2 = f(t + c_1 h, y_n + a_{21} h k_1)\n",
    "$$\n",
    "\n",
    "By taking the second-order Taylor expansion for $k_1$ and $k_2$:\n",
    "\n",
    "$$\n",
    "k_1 = f(t, y_n) = f\n",
    "$$\n",
    "\n",
    "$$\n",
    "k_2 = f(t + c_1 h, y_n + a_{21} h k_1) = f + c_1 h f_t + a_{21} h f f_y + \\frac{c_1^2 h^2}{2} f_{tt} + a_{21}^2 \\frac{h^2}{2} f^2 f_{yy} + a_{21} c_1 h^2 f f_{ty}\n",
    "$$\n",
    "\n",
    "We now substitute into the general formula for updating \\(y\\):\n",
    "\n",
    "$$\n",
    "y_{n+1} = y_n + h(b_1 k_1 + b_2 k_2)\n",
    "$$\n",
    "\n",
    "This becomes:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    y_{n+1} &= y_n + b_1 h f + b_2 h f + b_2 c_1 h^2 f_t + b_2 a_{21} h^2 f f_y \\\\\n",
    "    &\\qquad + b_2 \\frac{c_1^2 h^3}{2} f_{tt} + b_2 a_{21} c_1 h^3 f f_{ty} + b_2 \\frac{a_{21}^2 h^3}{2} f^2 f_{yy}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "We can now compare this to the Taylor expansion of $y(t)$ up to second order:\n",
    "\n",
    "$$\n",
    "y_{n+1} = y_n + h f + \\frac{h^2}{2} (f_t + f_y f)\n",
    "$$\n",
    "\n",
    "From this comparison, we obtain the following system of equations to determine the unknowns $b_1$, $b_2$, $c_1$, and $a_{21}$:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    b_1 + b_2 &= 1 \\\\\n",
    "    b_2 c_1 &= \\frac{1}{2} \\\\\n",
    "    b_2 a_{21} &= \\frac{1}{2}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Rewriting these equations:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    b_1 &= 1 - b_2 \\\\\n",
    "    b_2 &= \\frac{1}{2 c_1} \\\\\n",
    "    a_{21} &= c_1\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Finally, by choosing \\(c_1 = 1\\), we get the **Heun method** with the following Butcher tableau:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc}\n",
    "0 &  &  \\\\\n",
    "1 & 1 &  \\\\ \\hline\n",
    "& \\frac{1}{2} & \\frac{1}{2}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "This formulation shows the Heun method, where $ c_1 = 1$, $a_{21} = 1$, and $b_1 = b_2 = \\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbb2371",
   "metadata": {},
   "source": [
    "#### Example derivation (3rd order Runge-Kutta method)\n",
    "\n",
    "As another example we will derive our very own explicit third order solver.\n",
    "\n",
    "We are trying to solve a first order differential equation\n",
    "\n",
    "$$\\frac{dy}{dt}=f(t,y)$$\n",
    "\n",
    "with third order accuracy.\n",
    "\n",
    "The Butcher tableau we are trying to fill in looks like this:\n",
    "\n",
    "$$\\begin{array}{c|ccc}\n",
    "0 &  &  &   \\\\\n",
    "a & c &  &   \\\\\n",
    "b & d & e &   \\\\ \\hline\n",
    "& F & G & H\n",
    "\\end{array}$$\n",
    "\n",
    "As already mentioned previously, the notation quickly becomes very dense. \n",
    "Therefore we will introduce some notational shortcuts:\n",
    "\n",
    "- Whenever we evaluate $f$ at $(t,y)$, we no longer write its argument: $f(t,y)=f$\n",
    "- Partial derivatives are written as a subscript:\n",
    "    - $\\frac{\\partial f}{\\partial t}=f_t$\n",
    "    - $\\frac{\\partial f}{\\partial y}=f_y$\n",
    "\n",
    "\n",
    "Using\n",
    "\n",
    "$$k_i=f\\left(t+c_ih,y_n+h\\sum^s_{j=1}a_{ij}k_j\\right)$$\n",
    "\n",
    "and each time taking the second order Taylor expansion we can find $k_1$,$k_2$ and $k_3$ up to order $h^2$ (after substituting $k_1$ and $k_2$ in $k_2$ and $k_3$ where necessary):\n",
    "\n",
    "$$\\begin{split}\n",
    "    k_1 &= f(t,y_n)=f \\\\\n",
    "    k_2 &= f(t+ah,y_n+hck_1) \\\\\n",
    "    &= f+ahf_t+chff_y+a^2\\frac{h^2}{2}f_{tt}+ach^2ff_{ty}+c^2\\frac{h^2}{2}f^2f_{yy} \\\\\n",
    "    k_3 &= f(t+bh,y_n+hdk_1+hek_2) \\\\\n",
    "    &= f+bhf_t+dhff_y+ehff_y+aeh^2f_yf_t+ceh^2ff_y^2 \\\\\n",
    "    &\\qquad +b^2\\frac{h^2}{2}f_{tt}+bdh^2ff_{ty}+beh^2ff_{ty}+d^2\\frac{h^2}{2}f^2f_{yy} \\\\\n",
    "    &\\qquad +e^2\\frac{h^2}{2}f^2f_{yy}+deh^2f^2f_{yy}\n",
    "\\end{split}$$\n",
    " \n",
    "inserting this in \n",
    " \n",
    "$$y_{n+1}=y_n+h\\sum^s_{i=1}b_ik_i$$\n",
    " \n",
    "gives\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "     y_{n+1}&=y_n+Fhf+Ghf+Gah^2f_t+Gch^2ff_y+Ga^2\\frac{h^3}{2}f_{tt} \\\\\n",
    "     &\\qquad +Gach^3f_{ty}+Gc^2\\frac{h^3}{2}f^2f_{yy}+Hhf+Hbh^2f_t+Hdh^2ff_y \\\\\n",
    "     &\\qquad +Heh^2ff_y+Haeh^3f_yf_t+Hceh^3ff_y^2+Hb^2\\frac{h^3}{2}f_{tt} \\\\\n",
    "     &\\qquad +Hbdh^3ff_{ty}+Hbeh^3ff_{ty}+Hd^2\\frac{h^3}{2}f^2f_{yy} \\\\\n",
    "     &\\qquad +He^2\\frac{h^3}{2}f^2f_{yy}+Hdeh^3f^2f_{yy},\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "which we can compare with the Taylor expansion up to order 3:\n",
    "\n",
    "$$\n",
    "y_{n+1}=y_n+hf+\\frac{h^2}{2}(f_t+f_yf)+\\frac{h^3}{6}(f_{tt}+2f_{ty}f+f_tf_y+f^2f_{yy}+f_y^2f)\n",
    "$$\n",
    "\n",
    "to find the following set of equations to determine the unknowns $a$ up to $G$:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    F+G+H &= 1 \\\\\n",
    "    aG+bH &= 1/2 \\\\\n",
    "    cG+H(d+e) &= 1/2 \\\\\n",
    "    Ga^2+Hb^2 &= 1/3 \\\\\n",
    "    Gac+Hb(d+e) &= 1/3 \\\\\n",
    "    Hea &= 1/6 \\\\\n",
    "    Ga^2+H(d+e)^2 &= 1/3 \\\\\n",
    "    Hec &= 1/6 \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "When defining $a$ and $b$ ourselves, this can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    a &= a \\\\\n",
    "    b &= b \\\\\n",
    "    c &= a \\\\\n",
    "    d &= \\frac{b(3a^2-3a+b)}{a(3a-2)} \\\\\n",
    "    e &= b-d \\\\\n",
    "    F &= \\frac{6ab-3a-3b+2}{6ab} \\\\\n",
    "    G &= \\frac{3b-2}{6a(b-a)} \\\\\n",
    "    H &= \\frac{3a-2}{6b(a-b)} \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "When choosing $a=1/2$ and $b=1$, we obtain the **third order Runge-Kutta method** \n",
    "\n",
    "$$\n",
    "\\begin{array}{c|ccc}\n",
    "    0 &  &  &   \\\\\n",
    "    \\frac{1}{2} & \\frac{1}{2} &  &   \\\\\n",
    "    1 & -1 & 2 &  \\\\ \\hline\n",
    "    & \\frac{1}{6} & \\frac{2}{3} & \\frac{1}{6}   \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "However, if we would like to find our own solver, we choose any value we like for $a$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374220c5-c78c-4bdb-b148-5ed8901cb5bb",
   "metadata": {},
   "source": [
    "#### Adaptive step size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedbb7fb",
   "metadata": {},
   "source": [
    "Classical Runge-Kutta methods provide no error estimate on which to base the choice of step size and therefore require a **fixed step**.\n",
    "\n",
    "When a solver contains not only a solution of order $\\mathcal{O}(N)$ but also a solution of $\\mathcal{O}(N-1)$, we can use the difference between both solutions as an approximation of the error size $\\epsilon$ on the solution. \n",
    "This error depends on the size of the time step $h$, and given a certain error tolerance $\\tau$, it is possible to suggest a $h$ for the next time step which is a large as possible, while still maintaining the level of accuracy required as follows:\n",
    "\n",
    "$$h_{\\mathrm{optimal}}=h_{\\mathrm{current}}\\left(\\frac{\\tau}{\\epsilon}\\right)^{(1/N)}$$\n",
    "\n",
    "These solvers can therefore use an **adaptive step size**.\n",
    "\n",
    "In systems governed by dynamics whose speed changes in time, this can boost the performance of a solver tremendously. Even if the *adaptive step* does not need to adapt itself a lot and remains more or less constant, you have the advantage that your simulation runs at the best possible efficiency, given the required accuracy.\n",
    "\n",
    "A simple example of such a solver is **Heun's method**.\n",
    "In this method, the first order accurate solution is found using Euler's forward method\n",
    "\n",
    "$$\\tilde{y}_{n+1}=y_n+hf(t,y_{n})$$\n",
    "\n",
    "Afterwards, the second order solution is found:\n",
    "\n",
    "$$y_{n+1}=y_n+\\frac{h}{2}\\left(\\vphantom{\\frac{h}{2}}f(t,y_n)+f(t+h,\\tilde{y}_{n+1})\\right)$$\n",
    "\n",
    "The difference between both solutions is an estimate of the error.\n",
    "\n",
    "The butcher tableau looks like this:\n",
    "\n",
    " \n",
    "$$\n",
    "\\begin{array}{c|cc}\n",
    "    0 &  &  \\\\\n",
    "    1 & 1 &  \\\\ \\hline\n",
    "     & \\frac{1}{2} & \\frac{1}{2} \\\\\n",
    "     & 1 & 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Probably the most used embedded pair method was developed by Dormand and Prince. This is a 5th order accurate solver with a 4th order embedded error estimate.\n",
    "This solver is the default solver in `scipy` and in matlab.\n",
    "\n",
    "Its butcher tableau is given here\n",
    "\n",
    "- **Dormand-Prince method**\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|ccccccc}\n",
    "0 &  &  &  &  &  &  &  \\\\\n",
    "\\frac{1}{5} & \\frac{1}{5} &  &  &  &  &  &  \\\\\n",
    "\\frac{3}{10} & \\frac{3}{40} & \\frac{9}{40} &  &  &  &  &  \\\\\n",
    "\\frac{4}{5} & \\frac{44}{45} & -\\frac{56}{15} & \\frac{32}{9} &  &  &  &  \\\\\n",
    "\\frac{8}{9} & \\frac{19372}{6561} & -\\frac{25360}{2187} & \\frac{64448}{6561} & -\\frac{212}{729} &  &  &  \\\\\n",
    "1 & \\frac{9017}{3168} & -\\frac{355}{33} & \\frac{46732}{5247} & \\frac{49}{176} & -\\frac{5103}{18656} &  &  \\\\\n",
    "1 & \\frac{35}{384} & 0 & \\frac{500}{1113} & \\frac{125}{192} & -\\frac{2187}{6784} & \\frac{11}{84} &  \\\\ \\hline\n",
    "& \\frac{35}{384} & 0 & \\frac{500}{1113} & \\frac{125}{192} & -\\frac{2187}{6784} & \\frac{11}{84} & 0 \\\\\n",
    "& \\frac{5179}{57600} & 0 & \\frac{7571}{16695} & \\frac{393}{640} & -\\frac{9209}{339200} & \\frac{187}{2100} & \\frac{1}{40}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d35bfb-9589-424c-86b4-345e1ac71ac9",
   "metadata": {},
   "source": [
    "#### First-same-as-last (FSAL) property"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07aadbf",
   "metadata": {},
   "source": [
    "When we have a closer look at Heun's method, we see that we need two evaluations per time step. \n",
    "\n",
    "Now consider the **Bogacki-Shampine** method, which is a third order method with embedded second order solution.\n",
    "This method seems to require 4 function evaluations per step.\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cccc}\n",
    "    0 &  &  &  &  \\\\\n",
    "    \\frac{1}{2} & \\frac{1}{2} &  &  &  \\\\\n",
    "    \\frac{3}{4} & 0 & \\frac{3}{4} &  &  \\\\\n",
    "    1 & \\mathbf{\\frac{2}{9}} & \\mathbf{\\frac{1}{3}} & \\mathbf{\\frac{4}{9}} & \\\\ \\hline\n",
    "    & \\mathbf{\\frac{2}{9}} & \\mathbf{\\frac{1}{3}} & \\mathbf{\\frac{4}{9}} & 0 \\\\\n",
    "    & \\frac{7}{24} & \\frac{1}{4} & \\frac{1}{3} & \\frac{1}{3}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "However, contrary to Heun's method, in the Bogacki-Shampine method the last evaluation of step $n$ corresponds with the first evaluation of step $n+1$ (which is shown as the identical lines in bold), thus effectively reducing the number of evaluations per step to 3.\n",
    "\n",
    "This property is called is the **first-same-as-last (FSAL)** property and makes this solver 4/3 times more efficient compared to the case when it didn't have the FSAL property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f86855-9878-449f-9238-59befda3c815",
   "metadata": {},
   "source": [
    "#### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b9ec6",
   "metadata": {},
   "source": [
    "In this section we will validate the implementation of the different solvers by investigating the error on the solution as a function of the step size. \n",
    "If the solvers are implemented correctly, this error should go down to the level of numerical noise at low time steps at a rate proportional to $h^N$ for a solver of order $\\mathcal{O}(N)$.\n",
    "\n",
    "> **Example**\n",
    ">\n",
    "> As an example we'll integrate the ODE $y'(t)=y(t)$ with $y(0)=1$ from $t=0$ to $t=5$ with a variable step size $h$ between $10^{-4}$ and $10^{-1}$, and investigate the error on the result at $t=5$ as function of $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3deeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_demo():\n",
    "    \"\"\"Demonstrate the obtained error in the solution of an example ODE y'=y\n",
    "    as function of step size for a 3rd and 5th order solver.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define ODE\n",
    "    def ODE(t, y):\n",
    "        return y\n",
    "\n",
    "    # empty table to store perfomance for 10 values of h\n",
    "    # the first column contains h\n",
    "    # the second column error on the result obtained with RK23\n",
    "    # the third column error on the result obtained with R45\n",
    "    perf = np.zeros([10, 3])\n",
    "\n",
    "    steps = np.logspace(-4, -1, num=10)\n",
    "    # solve ODE\n",
    "    for i, h in enumerate(steps):\n",
    "        sol = integrate.solve_ivp(ODE, [0, 5], [1], max_step=h, method=\"RK23\")\n",
    "        sol2 = integrate.solve_ivp(ODE, [0, 5], [1], max_step=h, method=\"RK45\")\n",
    "        perf[i] = [\n",
    "            h,\n",
    "            np.abs(sol.y[0, -1] - np.exp(5)),\n",
    "            np.abs(sol2.y[0, -1] - np.exp(5)),\n",
    "        ]\n",
    "\n",
    "    # plot results\n",
    "    plt.close(\"performance\")\n",
    "    fig, ax = plt.subplots(num=\"performance\")\n",
    "    ax.plot(perf[:, 0], perf[:, 1], \"o\", label=\"RK23\")\n",
    "    ax.plot(perf[:, 0], 20 * perf[:, 0] ** 3.0, label=\"~h^3\")\n",
    "    ax.plot(perf[:, 0], perf[:, 2], \"o\", label=\"RK45\")\n",
    "    ax.plot(perf[:, 0], 0.1 * perf[:, 0] ** 5.0, label=\"~h^5\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlabel('step size \"h\"')\n",
    "    ax.set_ylabel(\"error\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "performance_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb16434",
   "metadata": {},
   "source": [
    "> As seen in the figure, the error indeed scales as the step size to the third and fifth power for the third and fifth order solver, respectively.\n",
    "> Note that, as the solver reaches the machine precision of about $10^{-16}$, it is of no use to further decrease the step size.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7550dfff",
   "metadata": {},
   "source": [
    "> **Example**\n",
    ">\n",
    "> Secondly, we'll also have a look at the obtained error and number of function evaluations taken as function of a predefined relative error tolerance for 2 adaptive step size solvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9573c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_demo2():\n",
    "    \"\"\"Demonstrate the obtained error and number of function evaluations taken\n",
    "    as function of the used relative error when solving of an example ODE y'=y\n",
    "    with an adaptive step 3rd and 5th order solver.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define ODE\n",
    "    def ODE(t, y):\n",
    "        return y\n",
    "\n",
    "    # Empty table to store performance for 10 values of the relative error tolerance\n",
    "    # First column: rtol\n",
    "    # Second column: error on result obtained with RK23\n",
    "    # Third column: number of function evaluations taken by RK23\n",
    "    # Fourth column: error on result obtained with RK45\n",
    "    # Fifth column: number of function evaluations taken by RK45\n",
    "    perf = np.zeros([10, 5])\n",
    "\n",
    "    # Solve ODE\n",
    "    steps = np.logspace(-12, -1, num=10)\n",
    "    for i, tol in enumerate(steps):\n",
    "        sol = integrate.solve_ivp(ODE, [0, 5], [1], rtol=tol, method=\"RK23\")\n",
    "        sol2 = integrate.solve_ivp(ODE, [0, 5], [1], rtol=tol, method=\"RK45\")\n",
    "        perf[i] = [\n",
    "            tol,\n",
    "            np.abs(sol.y[0, -1] - np.exp(5)),\n",
    "            sol.nfev,\n",
    "            np.abs(sol2.y[0, -1] - np.exp(5)),\n",
    "            sol2.nfev,\n",
    "        ]\n",
    "\n",
    "    # Plot results: Error as a function of error tolerance\n",
    "    plt.close(\"performance_demo2\")\n",
    "    fig, (ax0, ax1) = plt.subplots(\n",
    "        1, 2, squeeze=True, figsize=(12, 6), num=\"performance_demo2\"\n",
    "    )\n",
    "    ax0.set_xscale(\"log\")\n",
    "    ax0.set_yscale(\"log\")\n",
    "    ax0.plot(perf[:, 0], perf[:, 1], \"o\", label=\"RK23\")\n",
    "    ax0.plot(perf[:, 0], perf[:, 3], \"o\", label=\"RK45\")\n",
    "    ax0.plot(perf[:, 0], 200 * perf[:, 0], label=\"~rtol\")\n",
    "    ax0.set_xlabel(\"Error tolerance\")\n",
    "    ax0.set_ylabel(\"Error\")\n",
    "    ax0.legend(loc=\"upper right\", fancybox=True, shadow=True)\n",
    "    ax0.set_title(\"Error as a Function of Error Tolerance\")\n",
    "    ax0.grid(True)\n",
    "\n",
    "    # Plot results: Number of function evaluations as a function of error tolerance\n",
    "    ax1.set_xscale(\"log\")\n",
    "    ax1.set_yscale(\"log\")\n",
    "    ax1.plot(perf[:, 0], perf[:, 2], \"o\", c=\"c\", label=\"RK23\")\n",
    "    ax1.plot(perf[:, 0], 5 * perf[:, 0] ** (-1.0 / 3), label=\"~h^-1/3\")\n",
    "    ax1.plot(perf[:, 0], perf[:, 4], \"o\", c=\"m\", label=\"RK45\")\n",
    "    ax1.plot(perf[:, 0], 9 * perf[:, 0] ** (-1.0 / 5), label=\"~h^-1/5\")\n",
    "    ax1.set_xlabel(\"Error tolerance\")\n",
    "    ax1.set_ylabel(\"Function evaluations\")\n",
    "    ax1.legend(loc=\"upper right\", fancybox=True, shadow=True)\n",
    "    ax1.set_title(\"Function Evaluations as a Function of Error Tolerance\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "performance_demo2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36083e",
   "metadata": {},
   "source": [
    "At first sight, it seems to pay off to implement increasingly complex and higher-order solvers. \n",
    "However, for each additional order, a number of extra evaluations per step are necessary, as shown in the table below.\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|ccccccccc}\n",
    "    \\mathrm{solver}&\\mathrm{Euler}& \\mathrm{Heun12} & \\mathrm{RK3}& \\mathrm{Bogacki-Shampine23}& \\mathrm{RK4} & \\mathrm{Dormand-Prince45}& \\mathrm{Fehlberg6}& \\mathrm{Fehlberg7} \\\\ \\hline\n",
    "    \\frac{\\#\\mathrm{evaluations}}{\\mathrm{step}}&1&2&3&4&4&6&8&13\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "There also exists a theoretical limit which order can be achieved by a certain number of evaluations per step.\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|ccccccccc}\n",
    "    \\mathcal{O}(\\mathrm{solver})&1&2&3&4&5&6&7&8 \\\\ \\hline\n",
    "    \\frac{\\#\\mathrm{evaluations}}{\\mathrm{step}}&1&2&3&4&6&7&9&11\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Note that the Bogacki-Shampine (when not considering FSAL) and the Fehlberg methods appear to be suboptimal, but this stems from the fact that they also have a lower order solution embedded, which further increases the number of conditions their numbers in the Butcher tableau have to fulfill, and consequently require more variables and thus more evaluations per step.\n",
    "\n",
    "A second point to take into account is the memory usage of these solvers. The Seventh order Fehlberg method is only slightly faster than the Sixth order Fehlberg  method, but uses 13 evaluations per step, as compared to 8. \n",
    "It thus requires almost twice the amount of memory.\n",
    "Especially in GPU-software, where the memory bandwidth often is a limiting factor, such considerations need to be taken into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dcc5ec-c624-4e32-b460-64b1b9813611",
   "metadata": {},
   "source": [
    "### Extrapolation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ddc73",
   "metadata": {},
   "source": [
    "**Extrapolation methods** are based on the use of a single-step method to integrate the ODE over a given interval, $t_k \\leq t \\leq t_{k+1}$ , using several different step sizes $h_i$ and yielding results denoted by $\\mathbf{Y}(h_i)$.\n",
    "This gives a discrete approximation to a function $Y(h)$, where $\\mathbf{Y}(0) = \\mathbf{y}(t_{k+1})$. \n",
    "\n",
    "An interpolating polynomial or rational function $\\hat{\\mathbf{Y}}(h)$ is fit to these data, and $\\hat{\\mathbf{Y}}(0)$ is then taken as the approximation to $\\mathbf{Y}(0)$. \n",
    "\n",
    "We saw an example of this approach in **Richardson extrapolation** for numerical differentiation and integration.\n",
    "\n",
    "Extrapolation methods are capable of achieving very high accuracy, but they tend to be much less efficient and less flexible than other methods for ODEs, so they are used mainly when extremely high accuracy is required and cost is not a significant factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d3486",
   "metadata": {},
   "source": [
    "### Multistep methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ceb777",
   "metadata": {},
   "source": [
    "Whereas Runge-Kutta methods only use information of one previous point (i.e. a single step method), **Multistep methods** use information at more than one previous point to estimate the solution at the next point.\n",
    "\n",
    "One of the most popular *explicit* multistep methods is the fourth-order **Adams-Bashforth** method, which uses information of 3 previous time steps, next to the current one:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_{k+1}= \\mathbf{y}_{k}+\\frac{h}{24}\\left(55\\mathbf{y}'_{k}-59\\mathbf{y}'_{k-1}+37\\mathbf{y}'_{k-2}-9\\mathbf{y}'_{k-3} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9495c7",
   "metadata": {},
   "source": [
    "> **Derivation**\n",
    ">\n",
    "> This derivation considers a scalar function $y$, but the results can be applied componentwise to nonscalar functions as well.\n",
    ">\n",
    "> We derive a multistep method of the form\n",
    ">\n",
    "> $$\\mathbf{y}_{k+1}= \\alpha \\mathbf{y}_{k}+h(\\beta_0\\mathbf{y}'_{k}+\\beta_1\\mathbf{y}'_{k-1}+\\beta_2\\mathbf{y}'_{k-2}+\\beta_3\\mathbf{y}'_{k-3})$$\n",
    ">\n",
    "> To determine the 5 coefficients $\\alpha, \\beta_0, \\beta_1, \\beta_2$ and $\\beta_3,$ we require that this formula exactly integrates the first 5 monomials $1,t,t^2,t^3$ and $t^4$.\n",
    ">\n",
    "> $$\n",
    "\\begin{split}\n",
    "1&=\\alpha+h(\\beta_0\\cdot 0+\\beta_1\\cdot 0+\\beta_2\\cdot 0+\\beta_3\\cdot 0) \\\\\n",
    "t_{k+1}&=\\alpha t_k+h(\\beta_0\\cdot 1+\\beta_1\\cdot 1+\\beta_2\\cdot 1+\\beta_3\\cdot 1) \\\\\n",
    "t_{k+1}^2&=\\alpha t_k^2+h(\\beta_0 2t_{k}+\\beta_1 2t_{k-1}+\\beta_2 2t_{k-2}+\\beta_3 2t_{k-3}) \\\\\n",
    "t_{k+1}^3&=\\alpha t_k^3+h(\\beta_0 3t^2_{k}+\\beta_1 3t^2_{k-1}+\\beta_2 3t^2_{k-2}+\\beta_3 3t^2_{k-3}) \\\\\n",
    "t_{k+1}^4&=\\alpha t_k^4+h(\\beta_0 4t^3_{k}+\\beta_1 4t^3_{k-1}+\\beta_2 4t^3_{k-2}+\\beta_3 4t^3_{k-3})\n",
    "\\end{split}\n",
    "$$\n",
    ">\n",
    "> Because this method needs to work for *any* value of $t_k$ and $h$, we can conventiently choosing $t_k=0$ and $h=1$. \n",
    ">\n",
    "> It then follows that $t_{k+1}=1$, $t_{k-1}=-1$, $t_{k-2}=-2$ and , $t_{k-3}=-3$\n",
    ">\n",
    "> The first equation in the system thus becomes \n",
    ">\n",
    "> $$1=\\alpha\\cdot1+h(0)$$\n",
    ">\n",
    "> From which it follows that $\\alpha=1$. The remaining system of equations thus reduces to\n",
    ">\n",
    "> $$\n",
    "\\begin{split}\n",
    "1&=\\beta_0 +\\beta_1+\\beta_2+\\beta_3 \\\\\n",
    "1&=-2\\beta_1 -4\\beta_2-6\\beta_3 \\\\\n",
    "1&=3\\beta_1 +12\\beta_2+27\\beta_3 \\\\\n",
    "1&=1-4\\beta_1 -32\\beta_2-108\\beta_3 \\\\\n",
    "\\end{split}\n",
    "$$\n",
    ">\n",
    "> Which we can solve using `linalg.solve` (as seen in the linear systems notebook) to find the coefficients of the Adams-Bashforth method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3ad468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_adams_bashforth():\n",
    "    \"\"\"Solve the linear system encountered in the derivation of\n",
    "    the Adams-Bashforth method.\n",
    "\n",
    "    It's solution returns the coefficients of said method.\n",
    "    \"\"\"\n",
    "    A = np.array([[1, 1, 1, 1], [0, -2, -4, -6], [0, 3, 12, 27], [0, -4, -32, -108]])\n",
    "    b = np.array([1, 1, 1, 1])\n",
    "    x = linalg.solve(A, b)\n",
    "\n",
    "    with np.printoptions(\n",
    "        formatter={\"all\": lambda x: str(fractions.Fraction(x).limit_denominator())}\n",
    "    ):\n",
    "        print(x)\n",
    "\n",
    "\n",
    "solve_adams_bashforth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a97c6a",
   "metadata": {},
   "source": [
    "One of the most popular *implicit* multistep methods is the fourth-order **Adams-Moulton** method, which uses information of 2 previous time steps, next to the current one *and the next one*:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_{k+1}= \\mathbf{y}_{k}+\\frac{h}{24}\\left(9\\mathbf{y}'_{k+1}+19\\mathbf{y}'_{k}-5\\mathbf{y}'_{k-1}+\\mathbf{y}'_{k-2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c2e2e5",
   "metadata": {},
   "source": [
    "> **Exercise**\n",
    ">\n",
    "> A very similar derivation can be written down to find the coefficients of the Adams-Moulton method, which is left as an exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5cadb",
   "metadata": {},
   "source": [
    "Just like for single-step methods, implicit multistep methods are usually more accurate and stable than explicit multistep methods, but they require an initial guess to solve the resulting (usually nonlinear) equation for $\\mathbf{y}_{k+1}$.\n",
    "A good initial guess is conveniently supplied by an explicit method, so the explicit and implicit methods can be used as a **predictor-corrector pair**. \n",
    "One of the most used pairs is the Adams-Bashforth predictor and Adams-Moulton corrector shown above.\n",
    "\n",
    "One could use the corrector repeatedly (i.e., fixed-point iteration) until some convergence tolerance is met, but doing so may not be worth the expense. \n",
    "Instead, typically, a fixed number of corrector steps, often only one, is, giving a **PECE (predict, evaluate, correct, evaluate)** scheme.\n",
    "Although it has no effect on the value of $\\mathbf{y}_{k+1}$ , the second evaluation of $\\mathbf{f}$ in a PECE scheme yields an improved value of $\\mathbf{y}'_{k+1}$ for use in later steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111935a1",
   "metadata": {},
   "source": [
    "**A few properties of multistep methods worth knowing**\n",
    "- Since multistep methods require several previous solution values and derivative values, how do we get started initially, before we have any past history to use? \n",
    "One strategy is to use a single-step method, which requires no past history, to generate solution values at enough points to begin using a multistep method.\n",
    "- Changing step size is complicated, since the interpolation formulas are most conveniently based on equally spaced intervals for several consecutive points, so multistep methods are not idealy suited for adaptive step sizes.\n",
    "- A good local error estimate can be determined from the difference between the predictor and the corrector.\n",
    "- Implicit methods have a much greater region of stability than explicit methods but must be iterated to convergence to realize this benefit fully (e.g., a PECE scheme is actually explicit, albeit in a somewhat complicated way).\n",
    "-  A properly designed implicit multistep method can be very effective for solving\n",
    "stiff equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ac623d-5ad0-4f04-9277-ec3194d2e64a",
   "metadata": {},
   "source": [
    "### Multivalue methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea88ad",
   "metadata": {},
   "source": [
    "Changing step size is difficult with multistep methods because the past history of the solution is most easily maintained at equally spaced intervals. \n",
    "Similar to multistep methods, **multivalue** methods are based on polynomial interpolation, but they avoid many of the implementation difficulties associated with multistep methods.\n",
    "\n",
    "The main idea motivating multivalue methods is the observation that the interpolating polynomial itself can be evaluated at any point, not just at equally spaced intervals. \n",
    "The equal spacing associated with multistep methods is simply an artifact of the way the methods are represented as a linear combination of successive solution and derivative values with fixed weights.\n",
    "\n",
    "Multivalue methods are therefore a direct extension of multistep methods which allow adaptive step sizes (at the cost of more function evaluations per step and a more complicated implementation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ad229-f392-48ef-934c-440a5c9bb895",
   "metadata": {},
   "source": [
    "### Methods to solve ODE's with `scipy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a4cce5",
   "metadata": {},
   "source": [
    "All documentation on solving initial value problems for ODE's in `scipy` can be found here:\n",
    "\n",
    "<https://docs.scipy.org/doc/scipy/tutorial/integrate.html#ordinary-differential-equations-solve-ivp>\n",
    "\n",
    "The main functions you can use are `integrate.solve_ivp` and `integrate.odeint`.\n",
    "\n",
    "Be aware that the latter has a different (and older) API than the former, but are still commonly used.\n",
    "\n",
    "> They wrap older solvers implemented in Fortran (mostly ODEPACK). \n",
    "> While the interface to them is not particularly convenient and certain features are missing compared to the new API, the solvers themselves are of good quality and work fast as compiled Fortran code. \n",
    "> In some cases, it might be worth using this old API.\n",
    "\n",
    "The application of the LSODA vs RK45 methods to a stiff problem is illustrated in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12859e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_scipy():\n",
    "    def func(t, y):\n",
    "        return -100 * y + 100 * t + 101\n",
    "\n",
    "    y0 = np.array([10.99])\n",
    "    solution = integrate.solve_ivp(\n",
    "        func,\n",
    "        [0, 5],\n",
    "        y0,\n",
    "        t_eval=np.arange(0, 5.1, 0.1),\n",
    "        method=\"RK45\",\n",
    "        atol=1.49012e-8,\n",
    "        rtol=1.49012e-8,\n",
    "    )\n",
    "    print(\"RK45\\n\", solution.nfev, \"\\n\", solution.y)\n",
    "\n",
    "    solution = integrate.solve_ivp(\n",
    "        func,\n",
    "        [0, 5],\n",
    "        y0,\n",
    "        t_eval=np.arange(0, 5.1, 0.1),\n",
    "        method=\"LSODA\",\n",
    "        atol=1.49012e-8,\n",
    "        rtol=1.49012e-8,\n",
    "    )\n",
    "    print(\"LSODA\\n\", solution.nfev, \"\\n\", solution.y)\n",
    "\n",
    "    def func(y, t):\n",
    "        return -100 * y + 100 * t + 101\n",
    "\n",
    "    sol = integrate.odeint(func, y0, np.arange(0, 5.1, 0.1), full_output=False)\n",
    "    print(\"ODEINT (also LSODA)\\n\", sol.transpose())\n",
    "\n",
    "\n",
    "demo_scipy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97602db5-fd64-4d3b-80c1-dcae07a9afb0",
   "metadata": {},
   "source": [
    "Note that LSODA is much more accurate, while RK45 makes much more function evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1fc332",
   "metadata": {},
   "source": [
    "## Boundary Value Problems (BVP's) for ODE's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0608d6dc-a46d-489b-860b-375f82f0bda3",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18cd456",
   "metadata": {},
   "source": [
    "By itself, a differential equation does not uniquely determine a solution; additional side conditions must be imposed on the solution to make it unique. \n",
    "\n",
    "These side conditions prescribe values that the solution or its derivatives must have at some specified point or points. \n",
    "If all of the side conditions are specified at the same point, say $t_0$ , then we have an initial value problem, which we considered until now.\n",
    "\n",
    "If the side conditions are specified at more than one point, then we have a **boundary value problem, or BVP**.\n",
    "\n",
    "For an ordinary differential equation, the side conditions are typically specified at two points, namely the endpoints of some interval $[a, b]$, which is why the side conditions are called boundary conditions or boundary values. \n",
    "\n",
    "The remainder of this notebook will introduce a numerical method for solving such **two-point boundary value problems**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8b4bce",
   "metadata": {},
   "source": [
    "> **Examples**\n",
    ">\n",
    "> Newton’s Second Law of Motion, $F = ma$, which is a second-order ODE, involves two constants of integration, and hence two side conditions must be specified in order to determine a unique solution over the interval\n",
    "of integration, say $[a, b]$.\n",
    ">\n",
    "> In an initial value problem, both the position $y(a)$ and velocity $y'(a)$ would be specified at the initial point $a$, and this would uniquely determine the solution $y(t)$ over the entire interval.\n",
    ">\n",
    "> Other side conditions could be specified, however, such as the initial position $y(a)$ and final position $y(b)$, or the initial position $y(a)$ and final velocity $y'(b)$. \n",
    "> Indeed, any linear (or even nonlinear) combination of solution and derivative values at the endpoints could be specified, each giving a different two-point boundary value problem for this ODE.\n",
    "\n",
    "\n",
    "> Our primary focus will be on second-order scalar BVPs (and equivalent first-order systems) of exactly this type because many important physical problems have this form, including\n",
    "> \n",
    "> - The bending of an elastic beam under a distributed transverse load\n",
    "> - The distribution of electrical potential between two flat electrodes\n",
    "> - The temperature distribution in an internally heated homogeneous wall whose surfaces are maintained at fixed temperatures\n",
    "> - The steady-state concentration of a pollutant in porous soil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb1391",
   "metadata": {},
   "source": [
    "The two-point boundary value problem for the second-order scalar ODE\n",
    "\n",
    "$$u''=f(t,u,u')$$\n",
    "\n",
    "with $a<t<b$\n",
    "\n",
    "and boundary conditions:\n",
    "\n",
    "- $u(a)=\\alpha$\n",
    "- $u(b)=\\beta$\n",
    "\n",
    "is equivalent to the first order system of ODEs:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}y'_1 \\\\\n",
    "y'_2\\end{bmatrix}=\\begin{bmatrix}\n",
    "y_2 \\\\\n",
    "f(t,(y1,y2)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "with $a<t<b$\n",
    "\n",
    "and with separated boundary conditions\n",
    " \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    " 1 & 0 \\\\\n",
    " 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " y_1(a) \\\\\n",
    " y_2(a)\n",
    "\\end{bmatrix}+\n",
    "\\begin{bmatrix}\n",
    " 0 & 0 \\\\\n",
    " 1 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " y_1(b) \\\\\n",
    " y_2(b) \\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    " \\alpha \\\\\n",
    " \\beta\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For the general first-order two-point boundary value problem\n",
    "\n",
    "$$\n",
    "\\mathbf{y}'=\\mathbf{f}(t,\\mathbf{y})\n",
    "$$\n",
    "\n",
    "with $a<t<b$ and boundary conditions\n",
    "\n",
    "$$\n",
    "\\mathbf{g}(\\mathbf{y}(a),\\mathbf{y}(b))=\\mathbf{0}\n",
    "$$\n",
    "\n",
    "let $\\mathbf{y}(t; \\mathbf{x})$ denote the solution to the associated initial value problem with initial condition $\\mathbf{y}(a) = x$.\n",
    "For a given $\\mathbf{x}$, the solution $\\mathbf{y}(t; \\mathbf{x})$ of the IVP is a solution of the BVP if\n",
    "\n",
    "$$\n",
    "\\mathbf{h}(\\mathbf{x}) \\equiv \\mathbf{g}(\\mathbf{x}, \\mathbf{y}(b;\\mathbf{x}))=\\mathbf{0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466128f2-92f3-49a0-9d91-ce863cec3148",
   "metadata": {},
   "source": [
    "### Shooting method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d44bc04",
   "metadata": {},
   "source": [
    "The **shooting method** replaces a given boundary value problem by a sequence of initial value problems.\n",
    "\n",
    "As indicate above, the general first-order two-point boundary value problem is equivalent to the system of nonlinear algebraic equations\n",
    "\n",
    "$$\\mathbf{h}(\\mathbf{x}) \\equiv \\mathbf{g}(\\mathbf{x},\\mathbf{y}(b;\\mathbf{x}))=\\mathbf{0}$$\n",
    "\n",
    "One way to solve the BVP, therefore, is to solve the nonlinear system $\\mathbf{h}(\\mathbf{x}) = \\mathbf{0}$ using any suitable method from the nonlinear systems notebook.\n",
    "\n",
    "Evaluation of $\\mathbf{h}(\\mathbf{x})$ for any given value $\\mathbf{x}$ will require solving an IVP to determine $\\mathbf{y(b}; \\mathbf{x})$, for which we can use any suitable method seen in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a493a",
   "metadata": {},
   "source": [
    "To make this approach more concrete, consider the two-point BVP for a scalar second-order ODE \n",
    "\n",
    "$$u''=f(t,u,u')$$\n",
    "\n",
    "with $a<t<b$ and boundary conditions:\n",
    "\n",
    "- $u(a)=\\alpha$\n",
    "- $u(b)=\\beta$\n",
    "\n",
    "where we are given the initial value $u(a)$ and final value $u(b)$ of the solution, but not the initial slope $u'(a)$.\n",
    "If we knew the latter, then we would have an IVP. \n",
    "We lack that information, but we can guess a value for the initial slope, solve the resulting IVP, and then check to see if the computed solution value at $t = b$ matches the desired boundary value, $u(b) = \\beta$.\n",
    "\n",
    "The basic idea is illustrated in the figure below.\n",
    "\n",
    "Each curve represents a solution of the same second-order ODE, with different values for the initial slope $u'(a)$ giving different solutions for the ODE. \n",
    "All of the solutions start with the given initial value $u(a) = \\alpha$, but for only one value of the initial slope does the resulting solution curve hit the desired boundary condition $u(b) = \\beta$.\n",
    "The motivation for the name **shooting method** should now be obvious: we keep adjusting our aim until we hit the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987dcbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_shooting_method():\n",
    "    \"\"\"Visually illustrate the shooting method.\n",
    "\n",
    "    ODE y''=-g\n",
    "\n",
    "    or  y'=v\n",
    "        v'=-g\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def func(t, y):\n",
    "        \"\"\"Compute derivatives of y.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        t\n",
    "            time\n",
    "        y\n",
    "            Array with [position, velocity]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        yprime\n",
    "            Array with [velocity, accelaration]\n",
    "        \"\"\"\n",
    "        return np.array([y[1], -9.8])\n",
    "\n",
    "    y0 = 0\n",
    "    solutions = np.zeros([10, 100])\n",
    "    guesses = np.linspace(10, 40, 10)\n",
    "\n",
    "    plt.close(\"shooting\")\n",
    "    fig, ax = plt.subplots(num=\"shooting\")\n",
    "    for i, v0 in enumerate(guesses):\n",
    "        sol = integrate.solve_ivp(\n",
    "            func, [0, 2], [y0, v0], t_eval=np.linspace(0, 2, 100)\n",
    "        )\n",
    "        solutions[i] = sol.y[0]\n",
    "        ax.plot(sol.t, solutions[i, :], c=\"k\")\n",
    "\n",
    "    ax.plot(0, 0, \"o\", c=\"k\")\n",
    "    ax.annotate(\"a,α\", (0.05, -2))\n",
    "    ax.plot(2, 34, \"o\", c=\"k\")\n",
    "    ax.annotate(\"b,β\", (1.9, 36))\n",
    "\n",
    "\n",
    "demo_shooting_method()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462bc05c",
   "metadata": {},
   "source": [
    "After transforming a general second-order BVP into a first-order system, the boundary conditions become\n",
    "\n",
    "$$\n",
    "\\mathbf{g}(\\mathbf{y}(a),\\mathbf{y}(b))=\n",
    "\\begin{bmatrix}\n",
    "  y_1(a)-\\alpha \\\\\n",
    "  y_1(b)-\\beta\n",
    "\\end{bmatrix}=\\mathbf{0}\n",
    "$$\n",
    "\n",
    "Thus, the nonlinear system to be solved is\n",
    "\n",
    "$$\n",
    "\\mathbf{h}(\\mathbf{x})=\n",
    "\\begin{bmatrix}\n",
    " y_1(a;\\mathbf{x})-\\alpha \\\\\n",
    " y_1(b;\\mathbf{x})-\\beta\n",
    "\\end{bmatrix}=\\mathbf{0}\n",
    "$$\n",
    "\n",
    "where x is the initial value. \n",
    "The first component of $\\mathbf{h}(\\mathbf{x})$ will be zero if $x_1 = \\alpha$, and the initial slope $x_2$ remains to be determined so that the second component of $\\mathbf{h}(\\mathbf{x})$ will be zero. \n",
    "\n",
    "In effect, therefore, we must solve the scalar nonlinear equation in $x_2$ ,\n",
    "\n",
    "$$\n",
    "h_2(\\alpha, x_2) = y_1 (b; \\alpha, x_2) = 0\n",
    "$$\n",
    "\n",
    "for which we can use a one-dimensional zero finder seen in the nonlinear systems notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409091ce",
   "metadata": {},
   "source": [
    "> **Example**\n",
    ">\n",
    "> We illustrate the shooting method on the two-point BVP for the second-order scalar ODE which describes the vertical trajectory of a bullet (neglecting air friction).\n",
    ">\n",
    "> We shoot the bullet in the air at time and position equal to zero, and find that it takes 40 seconds for the bullet to fall down again.\n",
    "> \n",
    ">\n",
    "> $$y''=-g$$\n",
    ">\n",
    "> with boundary conditions $y(0)=0$ and $y(40)=0$\n",
    ">\n",
    "> For each guess for $y'(0)$, we will integrate the ODE using the Dormand-Prince method (the default for `integrate.solve_ivp` to determine how close we come to hitting the desired solution value at $t = 40$. \n",
    ">\n",
    "> Before doing so, however, we must first transform the second-order ODE into a system of two first-order ODEs\n",
    ">\n",
    "> $$\n",
    "\\begin{bmatrix}\n",
    " y' \\\\\n",
    " v'\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    " v \\\\\n",
    " -g\n",
    "\\end{bmatrix}\n",
    "$$\n",
    ">\n",
    "> and write a function which gives us the resulting position at $t=40$ for an initial guess $v(0)$.\n",
    ">\n",
    "> and try  an initial velocity of $v(0)=100$ m/s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708aacc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess(v0):\n",
    "    \"\"\"Helper function to integrate the differential equation y''=-g\n",
    "    from t=0 to t=40, given an initial value v0.\n",
    "    \"\"\"\n",
    "\n",
    "    def func(t, y):\n",
    "        return np.array([y[1], -9.8])\n",
    "\n",
    "    # Fix the initial state, even when given as array,\n",
    "    # for compatibility with root finder.\n",
    "    y0 = 0\n",
    "    v0 = np.ravel([v0])[0]\n",
    "    return integrate.solve_ivp(func, [0, 40], [y0, v0], t_eval=[40]).y[0][0]\n",
    "\n",
    "\n",
    "guess(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b766f5",
   "metadata": {},
   "source": [
    "> This has now become a root finding problem, which we can solve using `optimize.root`, with the initial guess of $v(0)=100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09de4b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize.root(guess, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2063954c",
   "metadata": {},
   "source": [
    "> Resulting in the solution $v(0)=196$ m/s.\n",
    "\n",
    "---\n",
    "\n",
    "> We also could have solved this problem immediately using `integrate.solve_bvp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69832b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_bvp():\n",
    "    \"\"\"Illustrate integrate.solve_bvp to solve the boundary value problem\n",
    "    y''=-g with y(0)=y(40)=0 as boundary values.\n",
    "    \"\"\"\n",
    "\n",
    "    def func(t, y):\n",
    "        return np.array([y[1], -9.8 * np.ones(len(y[0]))])\n",
    "\n",
    "    def bc(ya, yb):\n",
    "        return np.array([ya[0], yb[0]])\n",
    "\n",
    "    x = np.linspace(0, 40, 100)\n",
    "    y = np.ones((2, 100))\n",
    "\n",
    "    sol = integrate.solve_bvp(func, bc, x, y)\n",
    "\n",
    "    print(\"v(0)= \", sol.y[1, 0])\n",
    "    plt.close(\"bvp\")\n",
    "    fig, ax = plt.subplots(num=\"bvp\")\n",
    "    ax.plot(sol.x, sol.y[0], c=\"k\")\n",
    "    ax.plot(0, 0, \"o\", c=\"k\")\n",
    "    ax.plot(40, 0, \"o\", c=\"k\")\n",
    "    ax.set_xlabel(\"time (s)\")\n",
    "    ax.set_ylabel(\"height (m)\")\n",
    "\n",
    "\n",
    "demo_bvp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbdbf75",
   "metadata": {},
   "source": [
    "The shooting method is conceptually simple and is easy to implement using existing software for initial value problems and for nonlinear equations. \n",
    "It has serious drawbacks, however. \n",
    "\n",
    "Chief among these is that the shooting method inherits the stability (or instability) of the associated IVP, which as we have seen may be unstable even when the BVP is stable. \n",
    "This potential ill-conditioning of the IVP may make it extremely difficult to achieve convergence of the iterative method for the nonlinear equation. \n",
    "Moreover, for some values of the starting guess for the initial value, the solution of the IVP may not exist over the entire interval of integration in that the solution may become unbounded before reaching the right-hand endpoint of the BVP.\n",
    "\n",
    "A potential remedy for the difficulties associated with simple shooting is provided by **multiple shooting**, in which the interval of integration $[a, b]$ is divided into subintervals and shooting is carried out on each subinterval separately. \n",
    "\n",
    "Requiring continuity at the internal mesh points provides boundary conditions for the individual subproblems. Restricting the length of its interval of integration improves the conditioning of each IVP, but it also results in a larger system of nonlinear equations to solve.\n",
    "Specifically, the new system of ODEs is of size $mn$, where $m$ is the number of subintervals and $n$ is the size of the original system.\n",
    "\n",
    "Multiple shooting requires starting guesses for the initial values and slopes at the mesh points, and it also requires some new choices, such as the number of subintervals to use. \n",
    "Although it is more robust than simple shooting, multiple shooting is hardly foolproof and must be used with considerable care."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "title": "Ordinary Differential Equations",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37edbfdc",
   "metadata": {},
   "source": [
    "# Monte Carlo (MC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c598d1-922a-4866-a5ff-1edb4a90492e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "from scipy import stats\n",
    "from scipy.integrate import quad, solve_ivp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8ea487",
   "metadata": {},
   "source": [
    "Broadly speaking, a Monte Carlo method is a **numerical technique that employs (pseudo) random numbers**.\n",
    "Despite the randomness, results obtained with a Monte Carlo method are **intended to be reproducible**.\n",
    "This seems counterproductive at first sight: why should one make use of random numbers, if the result of interest is not random?\n",
    "As we'll see in this chapter, Monte Carlo methods enable computations that would be intractable otherwise.\n",
    "\n",
    "In general, Monte Carlo belongs in a statistics course, yet **many physical systems show stochastic behavior**, such that Monte Carlo methods are a good fit to model them.\n",
    "The stochastic behavior can be due to the chaotic behavior of their dynamical equations (e.g. three-body problem) or simply because the governing laws of physics are inherently stochastic (e.g. radioactive decay).\n",
    "In such cases, the physics of interest is best described by a probability distribution.\n",
    "Properties of interest are then computed as expectation values of such a distribution.\n",
    "\n",
    "When a Monte Carlo method is used in a computational model of a physical system, one often uses the term *Monte Carlo simulation*.\n",
    "The name *Monte Carlo* was introduced by John von Neumann and Stanley Ulam as a code word in the Manhattan Project, for simulations used to design the first nuclear weapons.\n",
    "Their methodology became so popular, in all scientific disciplines, such that the code word quickly replaced more scientific terms such as \"Model Sampling\".\n",
    "\n",
    "This chapter presents merely a first introduction to the Monte Carlo method.\n",
    "Before explaining Monte Carlo, pseudo-random numbers are briefly reviewed.\n",
    "This is followed by two more sections.\n",
    "First, the \"basic Monte Carlo\" method is explained, in which case independent random numbers can be easily generated with library routines.\n",
    "Finally, \"Markov chain Monte Carlo\" methods are introduced, in which case a statistical process is implemented to sample a nontrivial distribution.\n",
    "In all sections, we will mainly focus on continuous probability densities, yet the same techniques can also be applied to discrete distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31318e11-6647-415f-8718-c4d96cd0c17d",
   "metadata": {},
   "source": [
    "## Pseudo-random number generator (PRNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0890248",
   "metadata": {},
   "source": [
    "Computer hardware is designed to carry out purely deterministic arithmetic operations.\n",
    "**Any computational result is therefore not truly random.**\n",
    "Nonetheless, one may design algorithms that produce a sequence of seemingly uncorrelated numbers.\n",
    "\n",
    "The simplest algorithm in this category is the *Linear Congruential Generator* (LCG).\n",
    "Note that simplicity is its only advantage: for most applications, the correlation between subsequent values is too obvious, resulting in biased outcomes of Monte Carlo simulation.\n",
    "It is only used here to illustrate the basic structure of any PRNG.\n",
    "**Never use LCG for serious applications, and keep in mind that [many software libraries still implement it](https://en.wikipedia.org/wiki/Linear_congruential_generator#Parameters_in_common_use).**\n",
    "\n",
    "The following code cell implements LCG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46843cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lcg(size, seed, factor, offset, divisor):\n",
    "    \"\"\"Generate a LCG sequence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    size\n",
    "        The number of generated pseudo-random numbers,\n",
    "        also the size of the returned 1D array.\n",
    "    seed\n",
    "        The initial value.\n",
    "    factor, offset, divisor\n",
    "        Parameters in the LCG algorithm.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sequence\n",
    "        A 1D array containing the LCG sequence.\n",
    "\n",
    "    \"\"\"\n",
    "    result = np.zeros(size, dtype=int)\n",
    "    state = seed\n",
    "    result[0] = state\n",
    "    for i in range(1, size):\n",
    "        state = (factor * state + offset) % divisor\n",
    "        result[i] = state\n",
    "    return result\n",
    "\n",
    "\n",
    "generate_lcg(24, 3, factor=7, offset=1, divisor=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795a1df-328a-4981-a4f1-f8844c34a228",
   "metadata": {},
   "source": [
    "### Key ingredients of a PRNG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f186f4e",
   "metadata": {},
   "source": [
    "The LCG algorithm contains the four **key ingredients** also found in other PRNGs:\n",
    "\n",
    "1. A `seed` must be provided, which determines the entire random sequence.\n",
    "   Some implementations use the current time as the seed when no seed is provided by the user.\n",
    "   For example, one may use the number of seconds since the epoch (January 1st, 1970).\n",
    "\n",
    "2. The algorithm has an internal `state`, which is initialized with the seed and is updated after a new number is generated.\n",
    "   Advanced PRNGs conceptually extend the state in several ways:\n",
    "  \n",
    "   - In general, the state can be an array of numbers, or a binary (0 or 1) array of some size.\n",
    "   - When the seed contains too few bits of information, it can be padded with other values to fill up the initial state.\n",
    "   - The random number generated at each sequence may be a function of the state, rather than being equal to the state in the simple case of LCG.\n",
    "  \n",
    "3. A **recurrence relation** is used to derive the next state from the current one.\n",
    "\n",
    "4. Fixed **parameters** appearing in the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c6bb3c-e323-4972-928e-812786b54752",
   "metadata": {},
   "source": [
    "### Properties of pseudo-random sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b577bd",
   "metadata": {},
   "source": [
    "All pseudo-random sequences have the following **properties**, which can be recognized in the output of the LCG example above:\n",
    "\n",
    "1. The sequence is **deterministic**: repeating the calculation with the same seed and parameters gives the same result.\n",
    "\n",
    "2. There is only a **limited number of pseudo-random values**.\n",
    "   The upper limit is given by $2^N$, where $N$ is the number of bits used to represent the state.\n",
    "   In practice, algorithms have fewer different random numbers:\n",
    "   \n",
    "   - The algorithm may limit the range of the pseudo-random numbers by construction.\n",
    "   \n",
    "     > In the LCG example, the modulo operator limits the values to the set $\\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\\}$.\n",
    "   - Less obvious, the recurrence relation may not be capable of visiting all potential values.\n",
    "   \n",
    "     > In the LCG example, 2 is not present in the sequence.\n",
    "     > (Try 2 as a seed.)\n",
    "     \n",
    "3. Due to the previous points, **pseudo-random sequences must be periodic** (possibly after an initialization phase).\n",
    "   When the same state is encountered as before, all subsequent states will be repeated as well.\n",
    "   Because the number of states is limited, it is unavoidable that, after some time, the same state appears again.\n",
    "   In the above LCG example, the period is 12, not 13.\n",
    "   \n",
    "   In practice, PRNGs are designed with well-defined periods and without any initialization phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f961e3d8-7aee-4d7b-9c76-2e679b9419ab",
   "metadata": {},
   "source": [
    "### Desirable characteristics of a PRNG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e6c472",
   "metadata": {},
   "source": [
    "To avoid bias in simulations with pseudo-random sequences, a good PRNG algorithm should have the following two main characteristics:\n",
    "\n",
    "1. The pseudo-random numbers should be **uniformly distributed**.\n",
    "\n",
    "   - Ideally, all possible states in a sequence can be visited (from any seed).\n",
    "     This would imply that each state appears with the same probability, resulting in a uniform distribution.\n",
    "   - Several isolated periodic pseudo-random subsequences may exist in the space of all states.\n",
    "     In the LCG example above, the subsequences are $\\{2\\}$ and $\\{3, 9, 12, 7, 11, 0, 1, 8, 5, 10, 6, 4\\}$.\n",
    "     When using a seed different from 2, the distribution is not uniform and has a gap at 2.\n",
    "\n",
    "2. There should be **no apparent statistical correlations** between subsequent values.\n",
    "\n",
    "   - A minimal requirement is that pseudo-random sequences have **long periods**, ideally much longer than the amount of random numbers needed in any application.\n",
    "   - Also, after **differentiating** the sequence, no obvious correlation should appear.\n",
    "     For example, the spacing between two subsequent random numbers should also be pseudo-random.\n",
    "   - In addition, when generating $N$-dimensional **vectors of random numbers**, these should be distributed uniformly throughout their space.\n",
    "\n",
    "The desirable characteristics can to some extent be assessed by a pen-and-paper analysis, which is useful for designing new algorithms.\n",
    "Moreover, standard numerical tests exist to validate the desirable characteristics of a PRNG, most notably [TestU01](http://simul.iro.umontreal.ca/testu01/tu01.html).\n",
    "For an effective algorithm, both the operations and the parameters need to be carefully selected.\n",
    "NumPy uses the [PCG64](https://www.pcg-random.org/) algorithm by default, which performs well in both categories and has several additional technical advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca8ea2-92d8-4615-975e-d39fad61f1d9",
   "metadata": {},
   "source": [
    "### Modern PRNG algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaaf4dd",
   "metadata": {},
   "source": [
    "A complete exposition of the history of PRNG development goes beyond the scope of this course.\n",
    "However, it is worth looking at one popular family of modern PRNGs, namely the [Xorshift](https://en.wikipedia.org/wiki/Xorshift) methods by Marsaglia.\n",
    "\n",
    "Modern algorithms often make use of binary operators such as `xor` and `shift`, because they are computationally efficient:\n",
    "\n",
    "- In Python, `or` is implemented with the operator `|`.\n",
    "  It applies the *bitwise or* to a pair of integers.\n",
    "  For every corresponding pair of bits, `or` is computed as follows:\n",
    "  \n",
    "  | In1 | In2 | Out |\n",
    "  |-----|-----|-----|\n",
    "  | 0   | 0   | 0   |\n",
    "  | 0   | 1   | 1   |\n",
    "  | 1   | 0   | 1   |\n",
    "  | 1   | 1   | 1   |\n",
    "  \n",
    "  When applied to integer numbers, `|` has the following effect.\n",
    "  \n",
    "  | Python   | Decimal | Binary |\n",
    "  |----------|---------|--------|\n",
    "  | `a`      | 5       | 0101   |\n",
    "  | `b`      | 12      | 1100   |\n",
    "  | `a \\| b` | 13      | 1101   |\n",
    "\n",
    "- `xor` is implemented with the operator `^`.\n",
    "  It applies the *bitwise exclusive or* to a pair of integers.\n",
    "  For every corresponding pair of bits, `xor` is computed as follows:\n",
    "  \n",
    "  | In1 | In2 | Out |\n",
    "  |-----|-----|-----|\n",
    "  | 0   | 0   | 0   |\n",
    "  | 0   | 1   | 1   |\n",
    "  | 1   | 0   | 1   |\n",
    "  | 1   | 1   | 0   |\n",
    "  \n",
    "  When applied to integer numbers, `^` has the following effect.\n",
    "  \n",
    "  | Python  | Decimal | Binary |\n",
    "  |---------|---------|--------|\n",
    "  | `a`     | 5       | 0101   |\n",
    "  | `b`     | 12      | 1100   |\n",
    "  | `a ^ b` | 9       | 1001   |\n",
    "\n",
    "- `shift` shifts all the bits in the binary representation of an integer to the left or the right.\n",
    "  Left and right shifts are implemented in Python with the `<<` and `>>` operators, respectively.\n",
    "  For example:\n",
    "\n",
    "  | Python   | Decimal | Binary |\n",
    "  |----------|---------|--------|\n",
    "  |          | 5       | 00101  |\n",
    "  | `5 << 1` | 10      | 01010  |\n",
    "  | `5 << 2` | 20      | 10100  |\n",
    "  | `5 >> 1` | 2       | 00010  |\n",
    "  \n",
    "  Shifting to the left multiplies by 2, while shifting to the right is a division by 2.\n",
    "  Whenever bits are shifted out of the register, they are discarded.\n",
    "\n",
    "- `bitroll` is not a low-level operation (and has no official name either), but is popular in modern PRNGs.\n",
    "  It combines two shift operators to permute bits in a binary number.\n",
    "  The following table contains some examples for 4-bit integers:\n",
    "  \n",
    "  | Input | Decimal | roll | Output | Decimal |\n",
    "  |-------|---------|------|--------|---------|\n",
    "  | 0010  | 2       | 1    | 0100   | 4       |\n",
    "  | 0101  | 5       | 1    | 1010   | 10      |\n",
    "  | 1001  | 9       | 1    | 0011   | 3       |\n",
    "  | 0011  | 3       | 2    | 1100   | 12      |\n",
    "  | 0011  | 3       | 3    | 1001   | 9       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf41095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitroll64(x, shift):\n",
    "    \"\"\"Apply a bitwise roll operation on 64-bit integers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "        The number to be bitrolled.\n",
    "    shift\n",
    "        The number of bits to be shifted to the left.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xroll\n",
    "        The bits of x are shifted ``shift`` to the left.\n",
    "        Any bits exceeding the 64-bit register are inserted\n",
    "        again on the right.\n",
    "\n",
    "    \"\"\"\n",
    "    return (np.uint64(x) << np.uint64(shift)) | (\n",
    "        np.uint64(x) >> np.uint64(64 - shift)\n",
    "    )\n",
    "\n",
    "\n",
    "assert bitroll64(0, 45) == 0\n",
    "assert bitroll64(1, 1) == 2\n",
    "assert bitroll64(3, 2) == 12\n",
    "assert bitroll64(3, 63) == 1 + 2**63"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a301671",
   "metadata": {},
   "source": [
    "The following code cell implements the `xoshiro256**` variant in the Xorshift family, proposed in 2018, and gives you a reasonable idea of a state-of-the-art algorithm.\n",
    "The state consists of four unsigned 64-bit integers.\n",
    "\n",
    "The code below was derived from a [C implementation](https://prng.di.unimi.it).\n",
    "\n",
    "Note that the C source code is much simpler, because it relies on something that is typically avoided in Python: when the product of two integers does not fit in the 64-bit register, the most significant bits are just discarded, a counter-intuitive behavior exploited by PRNGs.\n",
    "Because this typical C behavior easily leads to bugs, Python and NumPy complain when this happens, and some extra effort is needed to suppress such warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db26cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_xoshiro256ss(size, seed, pars=None):\n",
    "    \"\"\"Generate a xoshiro256** sequence of unsigned 64-bit integers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    size\n",
    "        The amount of numbers to generate.\n",
    "    seed\n",
    "        An initial state for the generator.\n",
    "        This must be a 1D array with exactly 4 unsigned\n",
    "        64-bit integers.\n",
    "    pars\n",
    "        The five integer parameters for the algorithm.\n",
    "        The default is [5, 7, 9, 17, 45]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sequence\n",
    "        A 1D array containing the pseudo-random sequence.\n",
    "\n",
    "    \"\"\"\n",
    "    state = np.array(seed, dtype=np.uint64)\n",
    "    if state.shape != (4,):\n",
    "        raise TypeError(\"The seed must be a sequence of 4 integers.\")\n",
    "\n",
    "    if pars is None:\n",
    "        pars = np.array([5, 7, 9, 17, 45], np.uint64)\n",
    "    else:\n",
    "        pars = np.asarray(pars, dtype=np.uint64)\n",
    "        if pars.shape != (5,):\n",
    "            raise TypeError(\"The parameters must be a sequence of 5 integers.\")\n",
    "\n",
    "    result = np.zeros(size, dtype=np.uint64)\n",
    "    for i in range(size):\n",
    "        result[i] = np.multiply(\n",
    "            bitroll64(np.multiply(state[1], pars[0], dtype=np.uint64), pars[1]),\n",
    "            pars[2],\n",
    "            dtype=np.uint64,\n",
    "        )\n",
    "        t = state[1] << pars[3]\n",
    "        state[2] ^= state[0]\n",
    "        state[3] ^= state[1]\n",
    "        state[1] ^= state[2]\n",
    "        state[0] ^= state[3]\n",
    "        state[2] ^= t\n",
    "        state[3] = bitroll64(state[3], pars[4])\n",
    "    return result\n",
    "\n",
    "\n",
    "assert (\n",
    "    generate_xoshiro256ss(4, [1, 2, 3, 4])\n",
    "    == [11520, 0, 1509978240, 1215971899390074240]\n",
    ").all()\n",
    "print(generate_xoshiro256ss(9, [1, 2, 3, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a70e664",
   "metadata": {},
   "source": [
    "The first three values are noticeably low due to the choice of the seed.\n",
    "Afterward, the distribution is uniform over the whole range of unsigned 64-bit integers.\n",
    "For this algorithm, it is recommended to use another PRNG (`splitmix64`) to initialize the seed.\n",
    "\n",
    "One can easily convert 64-bit integers to uniformly distributed random numbers with double precision on the interval $[0, 1]$ by taking the leading 53 bits of each number and multiplying them with `2e-53`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_xoshiro256ss():\n",
    "    \"\"\"Plot the sequence of random numbers generated by xoshiro256ss.\"\"\"\n",
    "    plt.close(\"xoshiro256ss\")\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(7, 4), sharey=True, num=\"xoshiro256ss\")\n",
    "    seq_int64 = generate_xoshiro256ss(4000, [1, 2, 3, 4])\n",
    "    seq_flt64 = (seq_int64 >> 11) * np.finfo(np.float64).epsneg\n",
    "    axs[0].plot(seq_flt64, \".\", color=\"C3\", alpha=0.5, mew=0)\n",
    "    axs[0].set_ylabel(\"Random number\")\n",
    "    axs[0].set_xlabel(\"Sequence index\")\n",
    "    axs[1].hist(seq_flt64, bins=10, orientation=\"horizontal\")\n",
    "    axs[1].set_xlabel(\"Count\")\n",
    "\n",
    "\n",
    "plot_xoshiro256ss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab85935",
   "metadata": {},
   "source": [
    "For Monte Carlo, **Xorshift methods (and many others) are practically sufficient and have a low computational cost**.\n",
    "An older popular method is the Mersenne-Twister algorithm, proposed in 1997, but both its computational performance and quality of randomness are outperformed by more recent algorithms.\n",
    "Ongoing research on PRNG algorithms is trying to establish a better trade-off between the desirable characteristics and computational cost.\n",
    "\n",
    "Because the **recurrence relations are inherently serial**, one cannot simply generate random numbers in parallel.\n",
    "Hence, for the sake of computational efficiency, PRNGs are typically implemented in low-level code (not Python).\n",
    "Vectorization and parallelism are sometimes used to produce multiple streams of parallel random numbers for high-performance applications.\n",
    "\n",
    "In addition to Monte Carlo, another major application of random numbers is **cryptography**. \n",
    "This application comes with additional algorithm requirements, generally related to the predictability of pseudo-random sequences.\n",
    "\n",
    "- Given an example sequence, one may easily detect the algorithm that was used and its internal state. Once these are determined, an adversary knows your future random numbers and can guess your random encryption keys.\n",
    "  This can be prevented by applying a non-invertible [hash function](https://en.wikipedia.org/wiki/Hash_function) to random data.\n",
    "\n",
    "- In extreme cases, predictability can be further reduced by including stochastic seeds from physical processes, such as radio-active decay, thermal noise, [lava lamps](https://www.youtube.com/watch?v=1cUUfMeOijg), etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7b664-a303-468f-87a8-4557cb100597",
   "metadata": {},
   "source": [
    "### Transformations of univariate continuous distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e395ac53",
   "metadata": {},
   "source": [
    "Uniformly distributed numbers can be transformed, to sample other continuous univariate distributions. Two common methods are mentioned here for the sake of completeness:\n",
    "\n",
    "- [Inverse transform sampling](https://en.wikipedia.org/wiki/Inverse_transform_sampling). Given a random variable $X$ uniformly distributed over $[0, 1]$, it can be transformed to $Y=F_Y^{-1}(X)$, where $F_y$ is the cumulative distribution of the quantity $Y$. \n",
    "\n",
    "- [The Box-Muller transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform) is an efficient method for sampling a standard normal distribution.\n",
    "\n",
    "The details of both methods are skipped here, because the Monte Carlo examples below rely on functionality built into NumPy and SciPy to generate random numbers for various distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6ab5b5-a1b8-4d74-aced-4dc7d82f494a",
   "metadata": {},
   "source": [
    "### Built-in, NumPy and SciPy implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d2682",
   "metadata": {},
   "source": [
    "#### Built-in [`random`](https://docs.python.org/3/library/random.html) (which should not be used)\n",
    "\n",
    "The built-in Python package [`random`](https://docs.python.org/3/library/random.html) uses the good old Mersenne Twister algorithm.\n",
    "While this is proven technology, the built-in package has some limitations that make it unappealing for computational use:\n",
    "\n",
    "- More efficient PRNGs have been developed.\n",
    "- The built-in package cannot efficiently create arrays of random numbers.\n",
    "- The number of implemented statistical distributions is limited.\n",
    "\n",
    "As a rule of thumb, never use the built-in `random` package for scientific purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de2058-77b5-4ae8-b65d-c38e396b8c62",
   "metadata": {},
   "source": [
    "#### NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d36840",
   "metadata": {},
   "source": [
    "The default random number generator in NumPy is the 64-bit [Permuted Congruential Generator](https://numpy.org/doc/stable/reference/random/bit_generators/pcg64.html#numpy.random.PCG64), which is another modern PRNG.\n",
    "NumPy implements a reasonable set of statistical distributions, see [numpy.random.Generator](https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator).\n",
    "\n",
    "Example showing how to fill a $5\\times 3$ array by uniformly sampling over $[10^{-3}, 10^{3}]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca804819",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform(-1e3, 1e3, (5, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aabcad",
   "metadata": {},
   "source": [
    "The example above uses the outdated function-based interface to `numpy.random`, which is no longer recommended as of NumPy 1.17.0 (July 2019).\n",
    "\n",
    "It is recommended to use the new object-oriented interface: you first create an `rng` object with its own internal state and methods of the `rng` object can be called to get the actual random numbers.\n",
    "\n",
    "The advantage of the `rng` object is that you have full control over changes to the internal state of the RNG algorithm, which is not the case with the old function-based API.\n",
    "If your program and another library both use the same RNG state, you may get unpredictable random numbers, even with a fixed seed.\n",
    "This can be very confusing when trying to debug a program using random numbers.\n",
    "\n",
    "For example, you can create an `rng` object with a fixed seed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a5cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_numpy_rng():\n",
    "    rng = np.random.default_rng(1)\n",
    "    print(rng.uniform(-1e3, 1e3, (5, 3)))\n",
    "\n",
    "\n",
    "demo_numpy_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ff57d-0029-4bec-b655-cea5996d9f73",
   "metadata": {},
   "source": [
    "#### SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452be6f4",
   "metadata": {},
   "source": [
    "SciPy supports a much broader selection of probability densities through the [`scipy.stats`](https://scipy.github.io/devdocs/reference/stats.html) module.\n",
    "\n",
    "Example showing how to fill a $5\\times 3$ array by sampling a continuous Chi-squared distribution with 8 degrees of freedom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3438e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_scipy_rng():\n",
    "    rng = np.random.default_rng(1)\n",
    "    print(stats.chi2.rvs(8, size=(5, 3), random_state=rng))\n",
    "\n",
    "\n",
    "demo_scipy_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c98d5b",
   "metadata": {},
   "source": [
    "The `scipy.stats` module can also compute various other properties of distributions, such as the PDF, CDF, etc. See, for example, the [examples for the Chi-squared distribution](https://scipy.github.io/devdocs/reference/generated/scipy.stats.chi2.html#scipy.stats.chi2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ffb42e-9e4e-4e62-a517-334c3b6e19ef",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98136423",
   "metadata": {},
   "source": [
    "**Mersenne Twister**\n",
    "\n",
    "* Matsumoto, M.; Nishimura, T. (1998). \"Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random number generator\". ACM Trans. Mod. Comput. Sim. **8** (1), pp 3–30. [10.1145/272991.272995](https://doi.org/10.1145/272991.272995).\n",
    "\n",
    "**Xorshift**\n",
    "\n",
    "- Marsaglia, G. (2003). \"Xorshift RNGs\". *J. Stat. Soft.*, **8** (14), 1–6. [10.18637/jss.v008.i14](https://doi.org/10.18637/jss.v008.i14)\n",
    "\n",
    "**Xoshira256\\*\\***\n",
    "\n",
    "- Blackman, D.; Vigna, S. (2018). \"Scrambled Linear Pseudorandom Generators\". [arXiv:1805.01407](https://arxiv.org/abs/1805.01407)\n",
    "\n",
    "- Blackman, D.; Vigna, S. (2021). \"Scrambled Linear Pseudorandom Generators\". ACM Trans. Math. Soft. **47** (4) article no. 36, pp 1–32 [10.1145/3460772](https://doi.org/10.1145/3460772)\n",
    "\n",
    "**PCG64** (Used by NumPy)\n",
    "\n",
    "- https://www.pcg-random.org/\n",
    "\n",
    "- O'Neill, M.E. (2014). \"PCG: A Family of Simple Fast Space-Efficient Statistically Good Algorithms for Random Number Generation\". <https://www.cs.hmc.edu/tr/hmc-cs-2014-0905.pdf>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c754c02-9887-49d7-8ad7-df3427f93eee",
   "metadata": {},
   "source": [
    "## Basics of the Monte Carlo method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc4b01",
   "metadata": {},
   "source": [
    "The Monte Carlo method relies on the following identity from statistics:\n",
    "\n",
    "$$\n",
    "\\mathrm{E}[g(\\mathbf{X})] = \\int g(\\mathbf{x}) p_{\\mathbf{X}}(\\mathbf{x}) \\,\\mathrm{d}\\mathbf{x}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbf{X}$ is a stochastic vector in $\\mathbb{R}^n$.\n",
    "- $\\mathbf{x}$ is an (ordinary) vector in $\\mathbb{R}^n$.\n",
    "- $p_{\\mathbf{X}}(\\mathbf{x})$ is the probability density.\n",
    "- $p_{\\mathbf{X}}(\\mathbf{x})\\,\\mathrm{d}\\mathbf{x}$ is the probability of finding a sample point $\\mathbf{X}$ in a region of size $\\mathrm{d}\\mathbf{x}$ around $\\mathbf{x}$.\n",
    "- $g(\\cdot)$ can be any function $\\mathbf{x} \\mapsto g(\\mathbf{x}) \\in \\mathbb{R}^m$.\n",
    "- The integral is taken over the entire domain, where $p_{\\mathbf{X}}(\\mathbf{x})$ is non-zero.\n",
    "- $\\mathrm{E}[\\cdot]$ stands for \"the expectation value\", assuming $\\mathbf{X}$ is distributed according to the probability density $p_{\\mathbf{X}}(\\mathbf{x})$.\n",
    "\n",
    "For many applications, the function $g$ is scalar.\n",
    "For several examples below, also $\\mathbf{X}$ and $\\mathbf{x}$ are also scalar quantities.\n",
    "\n",
    "With the above identity, one may approximate the integral in the right-hand side, just by taking $N$ sample points $\\mathbf{X}_i$ from the distribution $p_{\\mathbf{X}}$, computing all $g(\\mathbf{X}_i)$ and averaging over all results.\n",
    "\n",
    "$$\n",
    "\\int g(\\mathbf{x}) p_{\\mathbf{X}}(\\mathbf{x}) \\,\\mathrm{d}\\mathbf{x} \\approx\n",
    "\\frac{1}{N}\\sum_{i=1}^N g(\\mathbf{X}_i) = \\overline{g(\\mathbf{X}_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d78b8",
   "metadata": {},
   "source": [
    "> **Example: a one-dimensional integral**\n",
    ">\n",
    "> Let's compute the following integral  numerically, for which the analytical solution is known.\n",
    ">\n",
    "> $$\n",
    "\\int_{-\\infty}^{+\\infty} \\cos(1/x) \\frac{1}{\\pi(1+x^2)}\\,\\mathrm{d}{x} = \\frac{1}{e} \\approx 0.367879441171442\n",
    "$$\n",
    ">\n",
    "> This integral is challenging for quadrature methods due to its highly oscillatory nature.\n",
    "> \n",
    "> The second factor is a standard [Cauchy distribution](https://en.wikipedia.org/wiki/Cauchy_distribution).\n",
    "> Therefore, the integral can be approximated by $\\overline{\\cos(1/X_i)}$ where $X_i$ are standard-Cauchy-distributed numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253ca624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrand_1d(x):\n",
    "    return np.cos(1 / x) / np.pi / (1 + x**2)\n",
    "\n",
    "\n",
    "def plot_integrand(bound):\n",
    "    \"\"\"Visualization of the integrand.\"\"\"\n",
    "    plt.close(\"integrand\")\n",
    "    fig, ax = plt.subplots(figsize=(7, 4), num=\"integrand\")\n",
    "    xs = np.linspace(-bound, bound, 5000)\n",
    "    ax.plot(xs, integrand_1d(xs))\n",
    "    ax.set_xlim(-bound, bound)\n",
    "    ax.set_title(\"Visualization of the integrand\")\n",
    "    ax.grid()\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(r\"$\\cos(1/x)/(\\pi(1 + x^2))$\")\n",
    "\n",
    "\n",
    "plot_integrand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c67170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_mc_1d(size):\n",
    "    \"\"\"Simple demonstration of MC.\"\"\"\n",
    "    rng = np.random.default_rng()\n",
    "    xs = rng.standard_cauchy(size)\n",
    "    return np.cos(1 / xs).mean()\n",
    "\n",
    "\n",
    "demo_mc_1d(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c916503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_quad_1d(size):\n",
    "    \"\"\"Integration with SciPy's quad function.\"\"\"\n",
    "    return quad(integrand_1d, -np.inf, np.inf, limit=size)\n",
    "\n",
    "\n",
    "demo_quad_1d(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876f265d",
   "metadata": {},
   "source": [
    "> The `quad` function complains about convergence, but all in all, it performs surprisingly well for such a difficult case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c2e7bf-284e-490e-987f-bfee109427d4",
   "metadata": {},
   "source": [
    "### Error estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dbc1e7",
   "metadata": {},
   "source": [
    "Any Monte Carlo (MC) estimate is a stochastic quantity, simply because it is a function of (many) stochastic quantities, i.e. the sample points.\n",
    "Hence, the result is never exact.\n",
    "\n",
    "Luckily, one can estimate the error quite easily.\n",
    "The variance of the Monte Carlo estimate is\n",
    "\n",
    "$$\n",
    "\\mathrm{VAR}\\left[\\frac{1}{N}\\sum_{i=1}^N g(\\mathbf{X}_i) \\right]\n",
    "=\\frac{1}{N^2}\\sum_{i=1}^N\\mathrm{VAR}\\left[g(\\mathbf{X}_i)\\right]\n",
    "=\\frac{1}{N}\\mathrm{VAR}[g(\\mathbf{X})]\n",
    "$$\n",
    "\n",
    "In the first step, we utilized the propagation of variance for a linear combination of two stochastic variables:\n",
    "\n",
    "$$\n",
    "\\mathrm{VAR}[a\\mathbf{X}+b\\mathbf{Y}]=a^{2}\\mathrm{VAR}[\\mathbf{X}]+b^{2}\\mathrm{VAR} [\\mathbf{Y}]+2ab\\,\\mathrm{COV}[\\mathbf{X},\\mathbf{Y}]\n",
    "$$\n",
    "\n",
    "where $\\mathrm{COV}[\\mathbf{X},\\mathbf{Y}]$ stands for the covariance of two stochastic quantities.\n",
    "No covariance is taken into account, because we assume independent sample points.\n",
    "The sum contains $N$ times the same term because the samples $\\mathbf{X}_i$ are drawn from the same distribution.\n",
    "\n",
    "The standard error on the MC estimate is simply the square root of the variance:\n",
    "\n",
    "$$\n",
    "\\mathrm{Std.Err.} = \\sqrt{\\frac{\\mathrm{VAR}[g(\\mathbf{X}_i)]}{N}}\n",
    "$$\n",
    "\n",
    "The error decreases proportionally to $1/\\sqrt{N}$ with increasing $N$.\n",
    "Hence, by taking a sufficiently large sample, the error can be made arbitrarily small.\n",
    "\n",
    "Put differently, the number of required points is proportional to $1/(\\mathrm{Std.Err.})^2$.\n",
    "Note that this scaling is independent of the dimension of $\\mathbf{X}$ and remains the same for high-dimensional integrals.\n",
    "This is very different from conventional quadrature methods, where the number of required points scales exponentially with the dimension of $\\mathbf{X}$.\n",
    "\n",
    "Numerical estimates of the standard error can be derived from an unbiased estimate of the variance:\n",
    "\n",
    "$$\n",
    "\\mathrm{VAR}\\left[g(\\mathbf{X}_i)\\right]\n",
    "\\approx \\frac{1}{N-1}\\sum_{i=1}^N \\left(g(\\mathbf{X}_i) - \\overline{g(\\mathbf{X}_i)}\\right)^2\n",
    "$$\n",
    "\n",
    "Just keep in mind that such estimates may not always be reliable and should not be trusted blindly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e68ae41",
   "metadata": {},
   "source": [
    "> **Error estimate for the one-dimensional integral**\n",
    "> \n",
    "> The code below computes the error estimate and shows the convergence with an increasing sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8065cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_mc_1d_error(size):\n",
    "    rng = np.random.default_rng()\n",
    "    x = rng.standard_cauchy(size)\n",
    "    values = np.cos(1 / x)\n",
    "    return values.mean(), values.std() / np.sqrt(size)\n",
    "\n",
    "\n",
    "def plot_convergence_1d():\n",
    "    plt.close(\"1d\")\n",
    "    sizes = np.array(\n",
    "        [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 50000]\n",
    "    )\n",
    "    results_mc = np.array([demo_mc_1d_error(size) for size in sizes])\n",
    "    results_quad = np.array([demo_quad_1d(size) for size in sizes])\n",
    "    fig, axs = plt.subplots(1, 2, num=\"1d\", figsize=(7, 4))\n",
    "\n",
    "    def plot_integral(ax):\n",
    "        ax.axhline(np.exp(-1), color=\"k\")\n",
    "        ax.errorbar(sizes, results_mc[:, 0], results_mc[:, 1], fmt=\"o\")\n",
    "        ax.errorbar(sizes, results_quad[:, 0], results_quad[:, 1], fmt=\"o\")\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_xlabel(\"Sample size\")\n",
    "        ax.set_ylabel(\"Integral\")\n",
    "        ax.grid()\n",
    "\n",
    "    def plot_error(ax):\n",
    "        ax.plot(sizes, results_mc[:, 1], label=\"MC\")\n",
    "        ax.plot(sizes, results_quad[:, 1], label=\"quad\")\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_xlabel(\"Sample size\")\n",
    "        ax.set_ylabel(\"Error estimate\")\n",
    "        ax.grid()\n",
    "        ax.legend(loc=0)\n",
    "\n",
    "    plot_integral(axs[0])\n",
    "    plot_error(axs[1])\n",
    "\n",
    "\n",
    "plot_convergence_1d()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf2b093",
   "metadata": {},
   "source": [
    "> A few remarks about the results from the last example:\n",
    ">\n",
    "> - The scaling of the MC error is clearly recognizable: for two additional orders in the number of points, the error decreases by one order.\n",
    "> - The scaling of the traditional quadrature error is much better, because this is a one-dimensional integral.\n",
    ">   When the number of dimensions increases, this quickly changes in favor of MC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6fe95b-62e6-4583-b067-cc16424c9416",
   "metadata": {},
   "source": [
    "### Application to error propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59e131b",
   "metadata": {},
   "source": [
    "A straightforward application of MC is error propagation.\n",
    "\n",
    "Consider any calculation for which the inputs, $\\mathbf{X}$, are uncertain, and you would like to estimate the uncertainties on the outcome.\n",
    "\n",
    "One numerical solution to this problem is to repeat the calculation many times, each time drawing different random vaules from distributions that represent the uncertain inputs.\n",
    "This will result in a sample distribution of the calculation outcomes. All moments of the outcome samples can be analyzed, e.g. the mean and the standard deviation, the latter being a model of the uncertainty on the outcome.\n",
    "Note that all these moments are MC estimates with a mean and an uncertainty due to the finite number of sample points.\n",
    "\n",
    "> **Example: error propagation in [Torricelli's law](https://en.wikipedia.org/wiki/Torricelli%27s_law)**\n",
    ">\n",
    "> Torricelli observed the following in 1643:\n",
    ">\n",
    "> > The velocity with which a liquid leaves the opening of a vessel is equal to\n",
    "> > \n",
    "> > $v=\\sqrt{2 g h}$\n",
    "> >\n",
    "> > where $h$ is the height of the liquid level above the opening, and $g$ is Standard gravity.\n",
    ">\n",
    "> Let's assume that we are on the (fictitious) planet Zork and observe the following results after repeated measurements:\n",
    "> \n",
    "> $$\n",
    "\\begin{aligned}\n",
    "v & \\approx 3.20 \\pm 0.86\\,\\mathrm{m/s} \\\\\n",
    "h & \\approx 1.00 \\pm 0.05\\,\\mathrm{m}\n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    "> What is then the gravitational acceleration on Zork and the uncertainty in this result?\n",
    ">\n",
    "> The following code cell shows how to compute the estimates and sampling uncertainties with MC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b297e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_torricelli(size):\n",
    "    if size % 100 != 0:\n",
    "        raise ValueError(\"Size must be a multiple of 100.\")\n",
    "    vs = np.random.normal(3.20, 0.86, size)\n",
    "    hs = np.random.normal(1.00, 0.05, size)\n",
    "    gs = vs**2 / (2 * hs)\n",
    "\n",
    "    # Calculation of the mean and its uncertainty.\n",
    "    eg = gs.mean()\n",
    "    ug = gs.std() / np.sqrt(gs.size)\n",
    "    print(f\"Mean g on Zork [m/s^2]:  {eg:.3f} ± {ug:.3f}\")\n",
    "\n",
    "    # Calculation of the standard error and its uncertainty,\n",
    "    # using batch size 100.\n",
    "    eeg = gs.reshape(-1, 100).std(axis=0).mean()\n",
    "    ueg = gs.reshape(-1, 100).std(axis=0).std() / np.sqrt(size / 100)\n",
    "    print(f\"Error g on Zork [m/s^2]: {eeg:.3f} ± {ueg:.3f}\")\n",
    "\n",
    "    # Other properties can be estimate from the sample.\n",
    "    ep = (gs > 9.81).mean() * 100\n",
    "    up = (gs > 9.81).std() / np.sqrt(gs.size) * 100\n",
    "    print(\"Probability that Zork has a higher gravitational constant than Earth:\")\n",
    "    print(f\"  {ep:.1f}% ± {up:.1f}%\")\n",
    "\n",
    "\n",
    "demo_torricelli(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a766b42",
   "metadata": {},
   "source": [
    "Note that the values mentioned after the **`±` are uncertainties inherent to Monte Carlo** sampling.\n",
    "Larger samples will reduce these uncertainties for the same measurement.\n",
    "They are not related to the (propagation of) measurement errors.\n",
    "\n",
    "The above example is only a simple demonstration.\n",
    "The same technique is applicable to a wide variety of more complicated calculations, e.g. weather forecasts, epidemiological models, rocket trajectories, investment portfolios, ray tracing, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a5224-930a-4045-b9d9-dc2c832bad72",
   "metadata": {},
   "source": [
    "### Common pitfall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f84e392",
   "metadata": {},
   "source": [
    "For clarity, let's repeat the basic recipe of the Monte Carlo method:\n",
    "\n",
    "$$\n",
    "\\int g(\\mathbf{x}) p_{\\mathbf{X}}(\\mathbf{x}) \\,\\mathrm{d}\\mathbf{x} \\approx\n",
    "\\frac{1}{N}\\sum_{i=1}^N g(\\mathbf{X}_i) = \\overline{g(\\mathbf{X})}\n",
    "$$\n",
    "\n",
    "For some choices of $g$, it becomes infeasible to converge the MC estimate.\n",
    "In qualitative terms, this happens when:\n",
    "\n",
    "- $g$ is virtually zero in regions where $p_{\\mathbf{X}}$ is significant, and\n",
    "- $g$ becomes very large when the probability density nearly vanishes.\n",
    "\n",
    "For such a function $g$, outliers of the distribution will have a large contribution to the average.\n",
    "At best, this results in a large sampling error.\n",
    "At worst, there are no such outliers in the sample and the error goes unnoticed.\n",
    "\n",
    "> **Example: an evil one-dimensional integral**\n",
    ">\n",
    "> Let's try to solve the following integral with MC:\n",
    ">\n",
    "> $$\n",
    "\\int_{-\\infty}^{+\\infty} x^{10} \\frac{\\exp(-x^2/2)}{\\sqrt{2\\pi}}\\,\\mathrm{d}{x} = 945\n",
    "$$\n",
    ">\n",
    "> The integrand is the product of $x^{10}$ and a standard normal probability density.\n",
    ">\n",
    "> The following code plots the integrand and the two separate factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f739a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_integrand_evil():\n",
    "    \"\"\"Visualization of the integrand.\"\"\"\n",
    "    plt.close(\"integrand_evil\")\n",
    "    fig, ax = plt.subplots(figsize=(7, 4), num=\"integrand_evil\")\n",
    "    xs = np.linspace(-10, 10, 5000)\n",
    "    ax.plot(xs, xs**10, label=\"g(x)\")\n",
    "    ax.plot(xs, np.exp(-(xs**2) / 2) / np.sqrt(2 * np.pi), label=\"prob. density\")\n",
    "    ax.plot(\n",
    "        xs,\n",
    "        xs**10 * np.exp(-(xs**2) / 2) / np.sqrt(2 * np.pi),\n",
    "        \"k\",\n",
    "        label=\"integrand\",\n",
    "    )\n",
    "    ax.legend(loc=0)\n",
    "    ax.set_ylim(0, 2)\n",
    "\n",
    "\n",
    "plot_integrand_evil()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82119a7",
   "metadata": {},
   "source": [
    "> This plot shows that the integrand is significant in regions where the probability density of drawing a sample point is very low.\n",
    "> There is only a tiny probability of observing a large value of $x^{10}$.\n",
    "> As a consequence, significant contributions to the integrand can easily be missed by drawing (only a few) random numbers from the standard normal distribution.\n",
    ">\n",
    "> The following cell visualizes the consequences: when using a small sample, the MC estimate of the integral can be very wrong (too low in this case).\n",
    "> For the same reason, the uncertainty estimate will also be misleading in such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e1529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_mc_1d_evil_error(size):\n",
    "    rng = np.random.default_rng()\n",
    "    x = rng.standard_normal(size)\n",
    "    values = x**10\n",
    "    return values.mean(), values.std() / np.sqrt(size)\n",
    "\n",
    "\n",
    "def plot_convergence_1d_evil():\n",
    "    plt.close(\"evil\")\n",
    "    fig, ax = plt.subplots(figsize=(7, 4), num=\"evil\")\n",
    "    sizes = np.array(\n",
    "        [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 50000]\n",
    "    )\n",
    "    results = np.array([demo_mc_1d_evil_error(size) for size in sizes])\n",
    "    ax.axhline(954, color=\"k\")\n",
    "    ax.errorbar(sizes, results[:, 0], results[:, 1], fmt=\"o\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(\"Sample size\")\n",
    "    ax.set_ylabel(\"Integral\")\n",
    "    ax.grid()\n",
    "\n",
    "\n",
    "plot_convergence_1d_evil()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001856d6",
   "metadata": {},
   "source": [
    "For the one-dimensional case, one can understand the problem intuitively, but for higher-dimensional integrals, intuition easily falls short and insufficient sampling is harder to recognize.\n",
    "\n",
    "A more formal way of describing the problem is that MC has convergence issues when there is a large variance on $g(\\mathbf{X}_i)$.\n",
    "The errors still scale proportionally with $1/\\sqrt{N}$, but the prefactor, $\\sqrt{\\mathrm{VAR}[g(\\mathbf{X}_i)]}$, is huge.\n",
    "In such cases, the uncertainty of the sampling estimate of this prefactor is also large (for the same reason), making it difficult to detect this pitfall empirically.\n",
    "\n",
    "To solve this problem, one should construct another $g(\\mathbf{X}_i)$, which reduces the risk of huge outliers with a small probability.\n",
    "This is not always straightforward.\n",
    "\n",
    "> **Attempt to fix the evil example:**\n",
    ">\n",
    "> One may rewrite the integral as follows:\n",
    ">\n",
    "> $$\n",
    "\\int_{-\\infty}^{+\\infty}\n",
    "x^{10}\n",
    "a \\exp\\left(-\\frac{a^2-1}{a^2}\\frac{x^2}{2}\\right)\n",
    "\\frac{\\exp(-(x/a)^2/2)}{a\\sqrt{2\\pi}}\n",
    "\\,\\mathrm{d}{x} = 945\n",
    "$$\n",
    ">\n",
    "> With $a>1$, the last factor becomes a broader normal distribution and the remainder of the integrand is suppressed for larger values of $x$, effectively limiting the pernicious behavior of $x^{10}$.\n",
    ">\n",
    "> The following figure shows both factors in the integrand.\n",
    "> At $a=4$ the function $g$ is only significant where the probability density is not negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cdbc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_integrand_fixed(a=4):\n",
    "    plt.close(\"integrand_fixed\")\n",
    "    fig, ax = plt.subplots(figsize=(7, 4), num=\"integrand_fixed\")\n",
    "    xs = np.linspace(-2 * a, 2 * a, 500)\n",
    "    ax.plot(\n",
    "        xs,\n",
    "        xs**10 * a * np.exp(-(a**2 - 1) / a**2 * (xs**2 / 2)),\n",
    "        label=\"new g(x)\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        xs,\n",
    "        np.exp(-((xs / a) ** 2) / 2) / np.sqrt(2 * np.pi) / a,\n",
    "        label=\"new prob. density\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        xs,\n",
    "        xs**10 * np.exp(-(xs**2) / 2) / np.sqrt(2 * np.pi),\n",
    "        \"k\",\n",
    "        label=\"integrand\",\n",
    "    )\n",
    "    ax.legend(loc=0)\n",
    "    ax.set_ylim(0, 2)\n",
    "\n",
    "\n",
    "plot_integrand_fixed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f10fd3",
   "metadata": {},
   "source": [
    "> We can now use the rewritten integral to implement the MC method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f472513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_mc_1d_fixed_error(size, a=4):\n",
    "    rng = np.random.default_rng()\n",
    "    xs = rng.normal(0, a, size)\n",
    "    values = xs**10 * a * np.exp(-(a**2 - 1) / a**2 * (xs**2 / 2))\n",
    "    return values.mean(), values.std() / np.sqrt(size)\n",
    "\n",
    "\n",
    "def plot_convergence_1d_fixed():\n",
    "    plt.close(\"fixed\")\n",
    "    fig, ax = plt.subplots(figsize=(7, 4), num=\"fixed\")\n",
    "    sizes = np.array(\n",
    "        [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 50000]\n",
    "    )\n",
    "    results = np.array([demo_mc_1d_fixed_error(size) for size in sizes])\n",
    "    ax.axhline(954, color=\"k\")\n",
    "    ax.errorbar(sizes, results[:, 0], results[:, 1], fmt=\"o\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(\"Sample size\")\n",
    "    ax.set_ylabel(\"Integral\")\n",
    "    ax.grid()\n",
    "\n",
    "\n",
    "plot_convergence_1d_fixed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79814d",
   "metadata": {},
   "source": [
    "> The numerical integration improves, and also the error estimates become reliable again.\n",
    "\n",
    "Broadening or modifying the distribution in the integral is a common technique with different names, depending on the application.\n",
    "Methods like *importance sampling*, *reweighting*, *biased sampling*, ... are all based on the same concept.\n",
    "You are free to choose which probability density to use, and you can always compensate for that choice in the function $g$.\n",
    "\n",
    "> **Note.** More traditional quadrature methods would be more appropriate for the evil example.\n",
    "> However, they would be of no use when similar difficulties arise in high-dimensional integrals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7260e01b-0506-4490-b823-bcdff83b7ce1",
   "metadata": {},
   "source": [
    "### Monte Carlo integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c5f23",
   "metadata": {},
   "source": [
    "The name **Monte Carlo integration** is often used for the following special case:\n",
    "\n",
    "$$\n",
    "I = \\int_\\Omega g(\\mathbf{x})\\,\\mathrm{d}{\\mathbf{x}}\n",
    "$$\n",
    "\n",
    "where, at first sight, the probability density is missing.\n",
    "The integral runs over a finite domain $\\Omega$ instead of over the whole space.\n",
    "\n",
    "One may always insert a uniform distribution and divide out its normalization:\n",
    "\n",
    "$$\n",
    "I = V_\\Omega \\int_\\Omega g(\\mathbf{x}) p_{\\mathbf{X},\\text{uniform}}(\\mathbf{x}) \\,\\mathrm{d}{\\mathbf{x}}\n",
    "$$\n",
    "\n",
    "where $V_\\Omega$, the volume of the domain, is also the inverse of the normalization constant of the uniform distribution, $p_{\\mathbf{X},\\text{uniform}}(\\mathbf{x})$. In this form, the Monte Carlo method described above is applicable.\n",
    "\n",
    "Monte Carlo integration thus is essentially a quadrature method with equal weights and randomized grid points.\n",
    "It is numerically well-behaved as long as $g$ varies smoothly.\n",
    "In line with numerical quadrature in general, MC integration may become problematic when $g$ is negligible nearly everywhere in the domain.\n",
    "For such ill-posed cases, adaptive methods such as the [MISER algorithm](https://en.wikipedia.org/wiki/Monte_Carlo_integration#MISER_Monte_Carlo) have been developed to focus on subdomains where $g$ is more informative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0df5f",
   "metadata": {},
   "source": [
    "> **Example**\n",
    "> \n",
    "> An easy example of Monte Carlo integration estimates $\\pi$.\n",
    "> \n",
    "> The domain $\\Omega$ in this example is a unit square: $x_0 \\in [0, 1]$ and $x_1\\in[0, 1]$, for which $V_\\Omega=1$.\n",
    "> The function $g(\\mathbf{x})$ is 1 for all points within a distance of 1 from the origin and $0$ outside:\n",
    "> \n",
    "> $$g(\\mathbf{x})=H(1 - x_0^2 - x_1^2),$$\n",
    ">\n",
    "> where $H$ is the [Heaviside step function](https://en.wikipedia.org/wiki/Heaviside_step_function).\n",
    "> The integral of $g$ over $\\Omega$ is then the quarter of the area of a unit circle, i.e. $\\pi/4$:\n",
    ">\n",
    "> $$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\pi}{4}\n",
    "    &= \\int_0^1 \\mathrm{d}x_0 \\int_0^1 \\mathrm{d}x_1\\,\n",
    "       g(x_0, x_1) \\\\\n",
    "    &= V_\\Omega \\int_0^1 \\mathrm{d}x_0 \\int_0^1\n",
    "       \\mathrm{d}x_1\\,\n",
    "       g(x_0, x_1)\\,\n",
    "       p_{X_0X_1,\\text{uniform}}(x_0, x_1) \\\\\n",
    "    &\\approx \\frac{1}{N}\\sum_{k=1}^N g(\\mathbf{X}_k)\n",
    "\\end{aligned}\n",
    "$$\n",
    "> \n",
    "> This shows that $\\pi/4$ can be approximated as the ratio of the number of samples that fall inside the circle over the total number of samples, $N$.\n",
    ">\n",
    "> The code and plot for this example is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a6fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_pi_mc(n):\n",
    "    \"\"\"Estimate the value of π using Monte Carlo integration.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of random points to generate.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate random points (x, y) inside the unit square\n",
    "    random_points = np.random.rand(n, 2)\n",
    "\n",
    "    # Check which points are inside the quarter circle (x^2 + y^2 <= 1)\n",
    "    inside_circle = np.sum(random_points**2, axis=1) <= 1\n",
    "\n",
    "    # Estimate π using the ratio of samples inside the circle over the total.\n",
    "    estimate_pi_over_four = np.sum(inside_circle) / n\n",
    "\n",
    "    # Visualize the points and the quarter circle\n",
    "    plt.close(\"pi_mc\")\n",
    "    fig, ax = plt.subplots(num=\"pi_mc\")\n",
    "    ax.scatter(\n",
    "        random_points[:, 0],\n",
    "        random_points[:, 1],\n",
    "        c=inside_circle,\n",
    "        cmap=\"viridis\",\n",
    "        s=0.5,\n",
    "    )\n",
    "    circle = plt.Circle((0, 0), 1, color=\"red\", fill=False)\n",
    "    ax.add_patch(circle)\n",
    "    ax.set_title(r\"Monte Carlo Estimate of $\\pi/4$\")\n",
    "    ax.set_xlabel(\"$x_0$\")\n",
    "    ax.set_ylabel(\"$x_1$\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "\n",
    "    print(f\"Monte Carlo Estimate of π/4: {estimate_pi_over_four:8.5f}\")\n",
    "    print(f\"Monte Carlo Estimate of π:   {4 * estimate_pi_over_four:8.5f}\")\n",
    "\n",
    "\n",
    "demo_pi_mc(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b22a15-8cdc-4666-8075-7c67ddcfc6f6",
   "metadata": {},
   "source": [
    "## Discrete Markov chain Monte Carlo methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5184ce81",
   "metadata": {},
   "source": [
    "In the previous section, the stochastic variables could be sampled with random number generators in NumPy.\n",
    "Sometimes, functions of these random numbers had to be computed.\n",
    "For practically all univariate distributions, uniform samples can be transformed, e.g. with inverse transform sampling.\n",
    "Such transformations are not always practical, and are usually impossible for complicated multivariate distributions.\n",
    "\n",
    "In this section, we'll discuss Markov chain Monte Carlo (MCMC), which can be used to draw samples from any distribution, even when the methods above fall short.\n",
    "To use the MCMC method, only the probability density up to a constant factor must be known.\n",
    "\n",
    "This section covers the very basics of discrete Markov chain Monte Carlo.\n",
    "The field has been under development since the early days of electronic computers and is a vast topic by itself.\n",
    "The [book of David MacKay](https://www.inference.org.uk/mackay/itprnn/book.html) is a classic reference (written from a statistics perspective)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f31ffdd-df95-4535-87e3-befb76dfe8a6",
   "metadata": {},
   "source": [
    "### Definition of a discrete Markov chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245185c1",
   "metadata": {},
   "source": [
    "A **discrete Markov chain** is a stochastic process, i.e. a sequence of stochastic quantities $\\{\\mathbf{X}_k\\}$, in which the probability of observing $\\mathbf{x}_{i+1}$ is only determined by:\n",
    "\n",
    "1. the previous state $x_i$, and\n",
    "2. the index $i$.\n",
    "\n",
    "Formally, one may write\n",
    "\n",
    "$$\n",
    "p_{\\mathbf{X}_{i+1}}(\\mathbf{x}_{i+1}) = \\int T_{i\\rightarrow i+1}(\\mathbf{x}_{i+1}|\\mathbf{x}_{i})\\,p_{\\mathbf{X}_i}(\\mathbf{x}_i)\\,\\mathrm{d}\\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "One can interpret $T_{i\\rightarrow i+1}$ as a conditional probability density, and it is often called the \"transition probability density\".\n",
    "\n",
    "**Some remarks:**\n",
    "\n",
    "- It is important that the function $T_{i\\rightarrow i+1}$ is *not explicitly dependent on older states*, such as $\\mathbf{x}_{i-1}$.\n",
    "  Such a dependency would result in a non-Markov chain.\n",
    "- When the function $T_{i\\rightarrow i+1}$ is the same for all $i$, the chain is called *time-homogeneous* and one may just write $T$.\n",
    "  For ease of notation, this convention will always be followed below.\n",
    "- The adjective *discrete* means that the states are labeled by a discrete index $i$.\n",
    "  In continuous Markov chains, the states are labeled by a continuous variable, e.g. a time $t$.\n",
    "  \n",
    "An **important property** of the transition probability is the following normalization:\n",
    "\n",
    "$$\\int T(\\mathbf{x}_{i+1}|\\mathbf{x}_{i})\\,\\mathrm{d}\\mathbf{x}_{i+1} = 1$$\n",
    "\n",
    "> **Proof.** One should simply require that the probability density of state $i+1$ is properly normalized when state $i$ is a Dirac delta distribution.\n",
    ">\n",
    "> $$\n",
    "1 = \\int p_{\\mathbf{X}_{i+1}}(\\mathbf{x}_{i+1}) \\,\\mathrm{d}\\mathbf{x}_{i+1}\n",
    "= \\iint T(\\mathbf{x}_{i+1}|\\mathbf{x}_{i})\\,\\delta(\\mathbf{x}^0_{i}-\\mathbf{x}_{i})\\,\\mathrm{d}\\mathbf{x}_{i+1}\\,\\mathrm{d}\\mathbf{x}_{i}\n",
    "= \\int T(\\mathbf{x}_{i+1}|\\mathbf{x}^0_{i})\\,\\mathrm{d}\\mathbf{x}_{i+1}\n",
    "$$\n",
    ">\n",
    "> Note that this also shows how to sample a Markov chain in practice.\n",
    "> At the point that we have a sample point $\\mathbf{x}^0_{i}$, its position is fixed, as described by the Dirac delta function, and the next point is simple generated by sampling the transition probability in which $\\mathbf{x}^0_{i}$ appears as a parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a30eace",
   "metadata": {},
   "source": [
    "> **Simple example: a random walk.**\n",
    ">\n",
    "> Consider a one-dimensional particle with position $X_i$ in state $i$.\n",
    "> Between two states, the particle can make a stochastic step, sampled from a unit normal distribution:\n",
    ">\n",
    "> $$T(x_{i+1}|x_i) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_{i+1} - x_i)^2}{2}\\right)$$\n",
    ">\n",
    "> The function below visualizes such a random walk, which represents one sample point of the entire Markov chain.\n",
    "> (Note that subsequent states in the chain are statistically correlated.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2410df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_walk(size=100):\n",
    "    # Generate the chain.\n",
    "    rng = np.random.default_rng()\n",
    "    states = np.zeros(size)\n",
    "    for i in range(1, size):\n",
    "        states[i] = states[i - 1] + rng.standard_normal()\n",
    "\n",
    "    # Plot the chain\n",
    "    plt.close(\"random_walk\")\n",
    "    fig, ax = plt.subplots(num=\"random_walk\")\n",
    "    ax.plot(states)\n",
    "    ax.set_xlabel(\"Chain index $i$\")\n",
    "    ax.set_ylabel(\"Position $x_i$\")\n",
    "\n",
    "\n",
    "plot_random_walk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408bdb6",
   "metadata": {},
   "source": [
    "> **Remark**: all PRNGs behave like Markov chains, except that they are only pseudo-random.\n",
    "> They share the property that the next state is only determined by the current state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bb6028-11cb-463a-a45b-2a69808a0ebc",
   "metadata": {},
   "source": [
    "### The stationary distribution of a discrete Markov chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dfe45b",
   "metadata": {},
   "source": [
    "As mentioned above, we assume that the Markov chain is *time-homogeneous*. All transition are described by the same transition probability $T$.\n",
    "\n",
    "A stationary distribution has the following property:\n",
    "\n",
    "$$\\int T(\\mathbf{x}_2|\\mathbf{x}_1) p(\\mathbf{x}_1)\\, \\mathrm{d}\\mathbf{x}_1 = p(\\mathbf{x}_2)$$\n",
    "\n",
    "In words, the transition to the next state leads to the same probability density.\n",
    "\n",
    "One can therefore consider **the chain as a random number generator for its corresponding stationary distribution**.\n",
    "It can be used as follows:\n",
    "\n",
    "1. Draw an initial sample point with a non-zero probability in the stationary distribution: $\\mathbf{x}_1^0$.\n",
    "   In this case, one can claim that the initial sample point is  drawn from the stationary distribution, even if it is improbable.\n",
    "2. Generate a next sample point, using the transition probability $T(\\mathbf{x}_2|\\mathbf{x}_1^0)$.\n",
    "   Because the previous sample point could have been drawn from the stationary distribution, the current sample is also a valid sample point.\n",
    "3. Repeat step 2, now with $T(\\mathbf{x}_3|\\mathbf{x}_2^0)$, $T(\\mathbf{x}_4|\\mathbf{x}_3^0)$, ... until you have enough samples.\n",
    "\n",
    "We will not elaborate on the statistical technicalities here, but the following are worth mentioning:\n",
    "\n",
    "- The stationary distribution **does not always exist**. \n",
    "  > For example, in the random walk example, the distribution will always become broader and flatter, without ever reaching a stationary regime.\n",
    "  > This behavior is recovered for any initial distribution.\n",
    "- When a stationary distribution exists, it is **not necessarily unique**.\n",
    "  > Consider for example a random walk that is only allowed within two intervals $[-3, -1]$ and $[1, 3]$.\n",
    "  > Uniform distributions in either interval are stationary, as well as a uniform distribution over both intervals at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b8342",
   "metadata": {},
   "source": [
    "> **Example 1** \n",
    ">\n",
    "> Consider the following transition probability\n",
    ">\n",
    "> $$T(x_{i+1}|x_i) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{(x_{i+1} - x_{i}/2)^2}{2}\\right)$$\n",
    ">\n",
    "> This means the previous position is scaled by a factor 1/2 and then randomly displaced by a standard-normally distributed step.\n",
    ">\n",
    "> One may derive the stationary distribution analytically for this case.\n",
    "> It is also a normal distribution with mean $0$ and standard deviation $\\sqrt{4/3}$.\n",
    "> (The derivation is a nice statistics exercise.)\n",
    ">\n",
    "> The following code illustrates that the stationary distribution is not affected by the initial state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c244be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stationary_example(size=10000):\n",
    "    rng = np.random.default_rng()\n",
    "    bins = np.linspace(-5, 5, 20)\n",
    "    plt.close(\"stationary_example\")\n",
    "    fig, ax = plt.subplots(num=\"stationary_example\")\n",
    "    for _ in range(10):\n",
    "        # Generate the chain\n",
    "        xs = np.zeros(size)\n",
    "        xs[0] = rng.normal(-10, 10)\n",
    "        for i in range(1, size):\n",
    "            # xs[i] = xs[i - 1] / 2 + rng.uniform(-1, 1)\n",
    "            xs[i] = xs[i - 1] / 2 + rng.normal(0, 1)\n",
    "\n",
    "        # Plot the histogram\n",
    "        ax.hist(xs, bins, histtype=\"step\", density=True)\n",
    "\n",
    "    # Plot the stationary distribution.\n",
    "    sigma = np.sqrt(4 / 3)\n",
    "    xs = np.linspace(-5, 5, 200)\n",
    "    ax.plot(\n",
    "        xs,\n",
    "        np.exp(-((xs / sigma) ** 2) / 2) / (sigma * np.sqrt(2 * np.pi)),\n",
    "        color=\"k\",\n",
    "    )\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$p_X(x)$\")\n",
    "\n",
    "\n",
    "plot_stationary_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f70bb4",
   "metadata": {},
   "source": [
    "> **Example 2: Eigenvalue problem** \n",
    ">\n",
    "> Finding the stationary distribution can also be seen as an eigenvalue problem.\n",
    "> To clarify this, consider an example of a discrete system that can be in of three states $s \\in \\{0, 1, 2\\}$, as opposed to a continues state $x$ in the rest of this section.\n",
    "> The probability to move from state $s_i$ to $s_{i+1}$ is given by the transition matrix $T_{i+1,i}$:\n",
    "> \n",
    "> $$\n",
    "T = \\begin{bmatrix}\n",
    "    0.6 & 0.3 &0.1 \\\\ \n",
    "    0.3 & 0.2 & 0.5 \\\\\n",
    "    0.1 & 0.5 & 0.4 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "> If the stationary distribution exists and is unique, many applications $T$ to an initial state $p^\\circ$ will converge toward $p$:\n",
    ">\n",
    "> $$p = \\lim_{N\\rightarrow \\infty} T^N p^\\circ$$\n",
    ">\n",
    "> This is infact a form of power iteration with eigenvalue 1.\n",
    "> \n",
    "> In line with the theory above, the stationary distribution is described by a vector $p\\in\\mathbb{R}^3$ that satisfies the following equation:\n",
    ">\n",
    "> $$T p = p$$\n",
    ">\n",
    "> This means that the stationary distribution is an eigenvector with eigenvalue 1,\n",
    "> which is consistent with the power iteration.\n",
    "> We compute the eigenvector $p$ of $\\lambda = 1$ in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f71ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_markov_chain():\n",
    "    # Define the transition matrix\n",
    "    T = np.array([[0.6, 0.3, 0.1], [0.3, 0.2, 0.5], [0.1, 0.5, 0.4]])\n",
    "\n",
    "    # Set the initial distribution\n",
    "    probabilities = np.array([1, 0, 0])\n",
    "\n",
    "    # Simulate 50 iteration steps to find the stationary distribution\n",
    "    for _ in range(50):\n",
    "        probabilities = T @ probabilities\n",
    "    print(\"The stationary probabilities are:\\n  \", probabilities)\n",
    "\n",
    "    # Compute eigenvalues and the eigenvector corresponding to the eigenvalue 1.\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(T)\n",
    "    print(\"The eigenvalues are:\\n  \", eigenvalues)\n",
    "\n",
    "    # Check if there is a unique eigenvalue = 1, within some threshold.\n",
    "    eps = 1e-5\n",
    "    is_one = abs(eigenvalues - 1) < eps\n",
    "    if any(is_one):\n",
    "        # Find the corresponding stationary eigenvector\n",
    "        idx_one = is_one.nonzero()[0][0]\n",
    "        stationary_vector = eigenvectors[:, idx_one]\n",
    "        print(\"The eigenvector with eigenvalue 1 is:\\n  \", stationary_vector)\n",
    "        stationary_vector /= stationary_vector.sum()\n",
    "        print(\"After L1 normalization:\\n  \", stationary_vector)\n",
    "    else:\n",
    "        print(\"No unique eigenvalue of 1 found.\")\n",
    "\n",
    "\n",
    "demo_markov_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939035c3",
   "metadata": {},
   "source": [
    "> As we can see after some time there is an equal chance of being in any of the 3 states! \n",
    ">\n",
    "> Markov chains can be used as a random number generator for its corresponding stationary distribution.\n",
    "> In this case the stationary distribution is quite simple (the uniform distribution), but for more complex distributions this is very helpful.\n",
    "> Below we show a random number generator that draws from the distribution corresponding to the stationary state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d02a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_random_sampling():\n",
    "    # Define the transition matrix\n",
    "    T = np.array([[0.6, 0.3, 0.1], [0.3, 0.2, 0.5], [0.1, 0.5, 0.4]])\n",
    "\n",
    "    # Start the chain with an initial state.\n",
    "    chain = [0]\n",
    "\n",
    "    # Extend the chain, each time use the transition probability\n",
    "    # to sample the next point.\n",
    "    for _ in range(100):\n",
    "        state_previous = chain[-1]\n",
    "        state_next = np.random.choice([0, 1, 2], p=T[state_previous])\n",
    "        chain.append(state_next)\n",
    "    chain = np.array(chain)\n",
    "\n",
    "    print(\"100 random numbers:\\n\", chain)\n",
    "    print(\"Number of 0s:\", (chain == 0).sum())\n",
    "    print(\"Number of 1s:\", (chain == 1).sum())\n",
    "    print(\"Number of 2s:\", (chain == 2).sum())\n",
    "\n",
    "\n",
    "demo_random_sampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3cdfce-551c-4742-9050-f9244dfd624a",
   "metadata": {},
   "source": [
    "> **Remarks:**\n",
    "> \n",
    "> 1. Each state appears with approximately the same probability, as expected.\n",
    "> 2. The states exhibit time-correlation.\n",
    ">    This is expected from the probability matrix:\n",
    ">    once in state zero, there is a 60% chance to go to zero again, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70577d8b-720d-4fc4-b711-011c3608f2fd",
   "metadata": {},
   "source": [
    "### Designing a Markov chain for any stationary distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db681e89-77b1-4c54-a460-ca138eb15303",
   "metadata": {},
   "source": [
    "Finding the stationary distribution of a chain is generally challenging.\n",
    "However, for a given stationary distribution, there are an uncountable infinite number of Markov chains.\n",
    "In practice, it is even fairly straightforward to construct a Markov chain for any given stationary distribution.\n",
    "Once the Markov chain is defined, sampling the stationary distribution is trivial, as explained in the previous section.\n",
    "\n",
    "The most general approach for constructing a suitable Markov chain is the Metropolis-Hastings algorithm. Many other methods can be seen as special cases of this general framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdecabe2-ee5e-48a4-9e58-6206cc984e3b",
   "metadata": {},
   "source": [
    "### Metropolis-Hastings algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76a4e22-3422-4c16-931e-425e9ce79206",
   "metadata": {},
   "source": [
    "#### Global Balance versus Detailed Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48486e4f",
   "metadata": {},
   "source": [
    "- A stationary distribution satisfies the \"global balance\" condition, that is, after convolution with the transition probability, the same probability is recovered. \n",
    "\n",
    "  $$\\int T(\\mathbf{x}_2|\\mathbf{x}_1) p(\\mathbf{x}_1)\\, \\mathrm{d}\\mathbf{x}_1 = p(\\mathbf{x}_2)$$\n",
    "\n",
    "  This condition does not impose much structure on $T$ and leaves plenty of freedom to decide from where to where the transition probability displaces sample points.\n",
    "\n",
    "- A more restrictive requirement is called \"detailed balance\":\n",
    "\n",
    "    $$T(\\mathbf{x}_2|\\mathbf{x}_1) p(\\mathbf{x}_1) = T(\\mathbf{x}_1|\\mathbf{x}_2) p(\\mathbf{x}_2)$$\n",
    "  \n",
    "    This can be interpreted as follows: for a stationary distribution, the following two probabilities must be equal:\n",
    "  \n",
    "    - The probability density of finding a sample point at $\\mathbf{x}_1$ and moving it to $\\mathbf{x}_2$.\n",
    "    - The probability density of finding a sample point at $\\mathbf{x}_2$ and moving it to $\\mathbf{x}_1$.\n",
    " \n",
    "One may also show that detailed balance is a sufficient condition for global balance.\n",
    "\n",
    "> **Proof**:\n",
    ">\n",
    "> $$\n",
    "\\int T(\\mathbf{x}_2|\\mathbf{x}_1) p(\\mathbf{x}_1)\\, \\mathrm{d}\\mathbf{x}_1\n",
    "= \\int T(\\mathbf{x}_1|\\mathbf{x}_2) p(\\mathbf{x}_2)\\, \\mathrm{d}\\mathbf{x}_1\n",
    "= p(\\mathbf{x}_2) \\int T(\\mathbf{x}_1|\\mathbf{x}_2) \\, \\mathrm{d}\\mathbf{x}_1\n",
    "= p(\\mathbf{x}_2)\n",
    "$$\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e557a87b-7a5c-4ed9-9159-807ab5552b8e",
   "metadata": {},
   "source": [
    "#### Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b60b93",
   "metadata": {},
   "source": [
    "The Metropolis-Hastings algorithm defines a Markov chain that satisfies detailed balance for any given stationary distribution.\n",
    "\n",
    "The transitions in Metropolis-Hastings Markov chain are constructed in two steps:\n",
    "\n",
    "- **Generation step.** A displacement of $\\mathbf{x}_1$ to $\\mathbf{x}_2$ is generated with a *proposal* distribution $g(\\mathbf{x}_2|\\mathbf{x}_1)$.\n",
    "\n",
    "  > For example, this can be the same normally distributed step size as in the random-walk example above.\n",
    "\n",
    "- **Acceptance or rejection step.** After constructing $\\mathbf{x}_2$ from $\\mathbf{x}_1$, it is further assessed and accepted with a probability $A(\\mathbf{x}_2,\\mathbf{x}_1)$.\n",
    "   If not accepted, the next state is identical to the current.\n",
    "\n",
    "  > The exact form of the acceptance probability will be specified later.\n",
    "  > It is the crucial component of the algorithm.\n",
    "\n",
    "The total transition probability is the product of these two probabilities: $T(\\mathbf{x}_2|\\mathbf{x}_1)=g(\\mathbf{x}_2|\\mathbf{x}_1)A(\\mathbf{x}_2,\\mathbf{x}_1)$.\n",
    "\n",
    "To find *a* correct expression for the acceptance probability, detailed balance is imposed:\n",
    "\n",
    "$$\n",
    "g(\\mathbf{x}_2|\\mathbf{x}_1)A(\\mathbf{x}_2,\\mathbf{x}_1)p(\\mathbf{x}_1)\n",
    "= g(\\mathbf{x}_1|\\mathbf{x}_2)A(\\mathbf{x}_1,\\mathbf{x}_2)p(\\mathbf{x}_2)\n",
    "$$\n",
    "\n",
    "and solved towards the ratio of acceptance probabilities:\n",
    "\n",
    "$$\n",
    "\\frac{A(\\mathbf{x}_2,\\mathbf{x}_1)}{A(\\mathbf{x}_1,\\mathbf{x}_2)}\n",
    "= \\frac{p(\\mathbf{x}_2)}{p(\\mathbf{x}_1)}\n",
    "\\frac{g(\\mathbf{x}_1|\\mathbf{x}_2)}{g(\\mathbf{x}_2|\\mathbf{x}_1)}\n",
    "$$\n",
    "\n",
    "The highest possible acceptance probabilities satisfying this equation are\n",
    "\n",
    "$$\n",
    "A(\\mathbf{x}_2,\\mathbf{x}_1) = \\min\\left(\n",
    "1, \n",
    "\\frac{p(\\mathbf{x}_2)}{p(\\mathbf{x}_1)}\n",
    "\\frac{g(\\mathbf{x}_1|\\mathbf{x}_2)}{g(\\mathbf{x}_2|\\mathbf{x}_1)}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "This expression becomes more intuitive when the *proposal* distributions become symmetric, i.e. $g(\\mathbf{x}_1|\\mathbf{x}_2)=g(\\mathbf{x}_2|\\mathbf{x}_1)$. In that case, one gets:\n",
    "\n",
    "$$\n",
    "A(\\mathbf{x}_2,\\mathbf{x}_1) = \\min\\left(\n",
    "1, \n",
    "\\frac{p(\\mathbf{x}_2)}{p(\\mathbf{x}_1)}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "This means the following:\n",
    "\n",
    "- A transition to higher probability density is always accepted.\n",
    "- A transition to lower probability density is accepted with a probability $p(\\mathbf{x}_2)/p(\\mathbf{x}_1)$.\n",
    "\n",
    "To apply this method, one must merely be able to compute the probability density up to an unknown normalization factor. In practically all applications, this is possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b57f76-9dc6-4e30-af70-c95dec7a6640",
   "metadata": {},
   "source": [
    "#### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908573e2",
   "metadata": {},
   "source": [
    "Building on the above derivation, the Metropolis-Hastings algorithm works as follows:\n",
    "\n",
    "1. Start with an initial state for which the (stationary) probability is non-zero.\n",
    "\n",
    "2. Generate a step according to the proposal distribution $g(\\mathbf{x}_2|\\mathbf{x}_1)$.\n",
    "\n",
    "3. Compute the acceptance ratio\n",
    "\n",
    "    $$\n",
    "    \\mathrm{AR} =\n",
    "    \\frac{p(\\mathbf{x}_2)}{p(\\mathbf{x}_1)}\n",
    "    \\frac{g(\\mathbf{x}_1|\\mathbf{x}_2)}{g(\\mathbf{x}_2|\\mathbf{x}_1)}\n",
    "    $$\n",
    "   \n",
    "    If this is larger than one, accept the step.\n",
    "    If it is smaller than one, accept the step with probability $\\mathrm{AR}$.\n",
    "    If the step is not accepted, the new state becomes equal to the old state.\n",
    "\n",
    "4. Repeat steps 2 and 3 until the sample size is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0a5f3-3743-4c9f-a9dd-7f8c973f4823",
   "metadata": {},
   "source": [
    "### Simple examples of Metropolis-Hastings Markov chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a72cf",
   "metadata": {},
   "source": [
    "The following function is a generic MH Markov chain implementation.\n",
    "It will be used by the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a584e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mhmc_driver(size, xinit, ln_stat_dens, prop_gen, ln_prop_dens=None, rng=None):\n",
    "    \"\"\"Sample a Metropolis--Hastings Markov chain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    size\n",
    "        The number of samples to generate.\n",
    "    xinit\n",
    "        The initial state.\n",
    "        Any type is allowed, as long as it is understood by the\n",
    "        following three parameters, which are all functions.\n",
    "    ln_stat_dens\n",
    "        A function evaluating the logarithm of the stationary\n",
    "        probability density of interest, up to an unknown\n",
    "        normalization factor.\n",
    "        It takes one argument (same type as xinit) and returns\n",
    "        a floating point number.\n",
    "    prop_gen\n",
    "        A generator for the proposal distribution. It takes\n",
    "        the current state as an argument, and it returns a proposed\n",
    "        new state. Both argument and return value are of the\n",
    "        same type as xinit.\n",
    "    ln_prop_dens\n",
    "        A function evaluating the logarithm of the proposal density.\n",
    "        It takes two arguments: the final and initial state (in that\n",
    "        order, both of the same type as xinit) and returns\n",
    "        the probability density of the proposal.\n",
    "        When not given, the proposal density is assumed to be\n",
    "        symmetric.\n",
    "    rng\n",
    "        A random number generator, must have the uniform method.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    chain\n",
    "        A list of states.\n",
    "    ln_sds\n",
    "        Logarithm of the stationary probability density for each\n",
    "        state in the chain.\n",
    "\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    # Initialize algorithm.\n",
    "    xcur = xinit\n",
    "    ln_sd_cur = ln_stat_dens(xcur)\n",
    "    # Lists in which the output is stored.\n",
    "    chain = [xcur]\n",
    "    ln_sds = [ln_sd_cur]\n",
    "    while len(chain) < size:\n",
    "        # 1. Proposal\n",
    "        xnew = prop_gen(xcur)\n",
    "\n",
    "        # 2. Compute logarithm of acceptance ratio\n",
    "        ln_sd_new = ln_stat_dens(xnew)\n",
    "        ln_ratio = ln_sd_new - ln_sd_cur\n",
    "        if ln_prop_dens is not None:\n",
    "            ln_ratio += ln_prop_dens(xcur, xnew) - ln_prop_dens(xnew, xcur)\n",
    "\n",
    "        # 3. Accept or reject\n",
    "        accept = ln_ratio > 0 or np.exp(ln_ratio) > rng.uniform(0, 1)\n",
    "        if accept:\n",
    "            xcur = xnew\n",
    "            ln_sd_cur = ln_sd_new\n",
    "        chain.append(xcur)\n",
    "        ln_sds.append(ln_sd_cur)\n",
    "\n",
    "    return chain, ln_sds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114495e-84ef-4065-805f-e95159b8ab53",
   "metadata": {},
   "source": [
    "#### Numerical 1D demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9108ecd6",
   "metadata": {},
   "source": [
    "Consider a simple probability density with the shape of a cosine function.\n",
    "\n",
    "$$\n",
    "p_X(x) \\propto \\begin{cases} \\cos(x) & \\text{if}\\quad|x| \\le \\pi/2 \\\\ 0 & \\text{if}\\quad|x| > \\pi/2 \\end{cases}\n",
    "$$\n",
    "\n",
    "Note that we do not bother normalizing the density.\n",
    "(The norm of the given form is 2, which is used for the plot below.)\n",
    "\n",
    "The integral we would like to compute is:\n",
    "\n",
    "$$\n",
    "\\int_{-\\pi/2}^{\\pi/2} x^2 \\frac{\\cos(x)}{2} \\,\\mathrm{d}\\mathbf{x} = \\frac{\\pi^2}{4} - 2 \\approx 0.467401100272340\n",
    "$$ \n",
    "\n",
    "The following cell demonstrates how the MH algorithm can sample this distribution, and how the integral is calculated. A uniform proposal distribution is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fea184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_cosine():\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    def prop_gen(xcur):\n",
    "        return xcur + rng.uniform(-0.5, 0.5)\n",
    "\n",
    "    def ln_stat_dens(x):\n",
    "        if abs(x) > np.pi / 2:\n",
    "            return -np.inf\n",
    "        return np.log(np.cos(x))\n",
    "\n",
    "    chain, _ = mhmc_driver(10000, 0.0, ln_stat_dens, prop_gen)\n",
    "\n",
    "    # Plot the chain and the histogram including the analytical distribution\n",
    "    plt.close(\"cosine\")\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(7, 4), num=\"cosine\", sharey=True)\n",
    "    axs[0].plot(chain, lw=1)\n",
    "    axs[0].set_xlabel(\"State index $i$\")\n",
    "    axs[0].set_ylabel(\"State $x$\")\n",
    "    axs[0].set_yticks(\n",
    "        [-np.pi / 2, -np.pi / 4, 0, np.pi / 4, np.pi / 2],\n",
    "        [r\"$-\\pi/2$\", r\"$-\\pi/4$\", \"0\", r\"$\\pi/4$\", r\"$\\pi/2$\"],\n",
    "    )\n",
    "    axs[1].hist(\n",
    "        chain,\n",
    "        np.linspace(-np.pi / 2, np.pi / 2, 21),\n",
    "        orientation=\"horizontal\",\n",
    "        density=True,\n",
    "    )\n",
    "    axs[1].set_xlabel(\"Count\")\n",
    "    xs = np.linspace(-np.pi / 2, np.pi / 2, 111)\n",
    "    axs[1].plot(np.cos(xs) / 2, xs)\n",
    "\n",
    "    # Compute the integral and the uncertainty,\n",
    "    # assuming the samples are uncorrelated.\n",
    "    chain = np.array(chain)\n",
    "    eint = (chain**2).mean()\n",
    "    uint = (chain**2).std() / np.sqrt(chain.size)\n",
    "    print(f\"integral: {eint:.4f} ± {uint:.4f}\")\n",
    "\n",
    "\n",
    "demo_cosine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae63899",
   "metadata": {},
   "source": [
    "**A few remarks:**\n",
    "\n",
    "- When the number of samples is reduced to 1000, the quality of the histogram becomes poor.\n",
    "  The cause for this problem is shown in the left plot: successive samples are *NOT* independent, meaning that the actual information content is much lower than the number of samples.\n",
    "  \n",
    "- In addition, the error estimate is clearly too optimistic.\n",
    "  It is computed with the assumption that the samples are independent, which is clearly not the case.\n",
    "  A correct error estimate should take into account the covariance of successive steps, which goes beyond the scope of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e5da29-ae00-42e4-9546-62ae16921046",
   "metadata": {},
   "source": [
    "#### The Cannonball game with a timer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54f5198",
   "metadata": {},
   "source": [
    "The following plot illustrates a variation on a popular computer game, \"Cannonball\". The game proceeds as follows:\n",
    "\n",
    "1. Castle A fires an explosive with a timer at castle B.\n",
    "   Castle A can control the initial velocity vector of the explosive.\n",
    "2. Five seconds after launch, the explosive detonates.\n",
    "3. If the explosive reaches the roof of castle B after five seconds, castle A wins.\n",
    "4. The usual war rhetoric: If we do not destroy B, B will destroy us.\n",
    "   We only have one chance to win.\n",
    "\n",
    "**Note:** all numerical values in this game are in SI base units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f69a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_game(seed=1):\n",
    "    \"\"\"Generate a random terrain and castle positions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed\n",
    "        Seed for the random number generator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (xs, ys)\n",
    "        Coordinates describing the terrain, used for visual only.\n",
    "    castle_a\n",
    "        (x, y) coordinates for the position of Castle A.\n",
    "    castle_b\n",
    "        (x, y) coordinates for the position of Castle B.\n",
    "\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Compute the terrain with a stochastic process.\n",
    "    nterrain = 201\n",
    "    xs = np.linspace(-100, 100, nterrain)\n",
    "    ys = np.zeros(nterrain)\n",
    "    for i in range(0, nterrain - 1):\n",
    "        ys[i + 1] = ys[i] * 0.99 + rng.normal(0, 2)\n",
    "\n",
    "    # The positions of the two castles\n",
    "    ia = rng.integers(40, 60)\n",
    "    castle_a = xs[ia], ys[ia] - 3\n",
    "    ib = rng.integers(140, 160)\n",
    "    castle_b = xs[ib], ys[ib] - 3\n",
    "\n",
    "    return (xs, ys), castle_a, castle_b\n",
    "\n",
    "\n",
    "def compute_trajectory(pos0, vel0):\n",
    "    \"\"\"Compute the 5-second trajectory of the explosive.\n",
    "\n",
    "    The trajectory takes into account the gravitational\n",
    "    force, friction of the explosive with the air and\n",
    "    the wind.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pos0\n",
    "        The initial position vector,\n",
    "        the middle of the roof of a Castle.\n",
    "    vel0\n",
    "        The initial velocity vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    trajectory\n",
    "        The trajectory of the explosive as a 2D array.\n",
    "        First row contains x-positions, second-row y-positions.\n",
    "        Columns correspond to time steps.\n",
    "\n",
    "    \"\"\"\n",
    "    STD_GRAVITY = 9.81\n",
    "    FRICTION_COEFF = 0.3\n",
    "    WIND_VEL = -3.0\n",
    "    TIMER = 5.0\n",
    "\n",
    "    def odefun(time, velpos):\n",
    "        vel, pos = np.split(velpos, 2)\n",
    "        acc = np.array(\n",
    "            [\n",
    "                -FRICTION_COEFF * (vel[0] - WIND_VEL),\n",
    "                -STD_GRAVITY - FRICTION_COEFF * vel[1],\n",
    "            ]\n",
    "        )\n",
    "        return np.concatenate([acc, vel])\n",
    "\n",
    "    velpos0 = np.concatenate([vel0, pos0])\n",
    "    sol = solve_ivp(\n",
    "        odefun,\n",
    "        [0, TIMER],\n",
    "        velpos0,\n",
    "        t_eval=np.linspace(0, TIMER, 101),\n",
    "        rtol=1e-9,\n",
    "        atol=1e-9,\n",
    "    )\n",
    "    return np.split(sol.y, 2)[1]\n",
    "\n",
    "\n",
    "def plot_game(terrain, castle_a, castle_b, trajectory, num):\n",
    "    \"\"\"Plot the outcome of a game.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    terrain, castle_a, castle_b\n",
    "        Return values of the configure_game function.\n",
    "    trajectory\n",
    "        Return value of the compute_trajectory function.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    CASTLE_WIDTH = 10\n",
    "    CASTLE_HEIGHT = 12\n",
    "\n",
    "    plt.close(num)\n",
    "    fig, ax = plt.subplots(figsize=(7, 4), num=num)\n",
    "    ax.fill_between(terrain[0], terrain[1], -100, color=\"#333333\")\n",
    "    xya = (castle_a[0] - CASTLE_WIDTH / 2, castle_a[1])\n",
    "    ax.add_patch(Rectangle(xya, CASTLE_WIDTH, CASTLE_HEIGHT, color=\"C0\"))\n",
    "    ax.text(\n",
    "        castle_a[0],\n",
    "        castle_a[1] + CASTLE_HEIGHT / 2,\n",
    "        \"A\",\n",
    "        color=\"w\",\n",
    "        va=\"center\",\n",
    "        ha=\"center\",\n",
    "        fontsize=20,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "    xyb = (castle_b[0] - CASTLE_WIDTH / 2, castle_b[1])\n",
    "    ax.add_patch(Rectangle(xyb, CASTLE_WIDTH, CASTLE_HEIGHT, color=\"C1\"))\n",
    "    ax.text(\n",
    "        castle_b[0],\n",
    "        castle_b[1] + CASTLE_HEIGHT / 2,\n",
    "        \"B\",\n",
    "        color=\"w\",\n",
    "        va=\"center\",\n",
    "        ha=\"center\",\n",
    "        fontsize=20,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "    ax.plot(trajectory[0], trajectory[1], color=\"C3\", lw=2)\n",
    "    ax.plot(trajectory[0][-1], trajectory[1][-1], \"C3X\", ms=15)\n",
    "    ax.set_aspect(\"equal\", adjustable=\"datalim\")\n",
    "    ax.set_xlim(-100, 100)\n",
    "    ax.set_ylim(-50, 50)\n",
    "\n",
    "\n",
    "def demo_game_init(x_vel, y_vel):\n",
    "    \"\"\"Example game with fixed initial velocity vector.\n",
    "\n",
    "    In this example, castle A makes a catastrophic mistake.\n",
    "    \"\"\"\n",
    "\n",
    "    CASTLE_HEIGHT = 12\n",
    "\n",
    "    terrain, castle_a, castle_b = configure_game()\n",
    "    pos0_a = np.array([castle_a[0], castle_a[1] + CASTLE_HEIGHT])\n",
    "    vel0_a = np.array([x_vel, y_vel])\n",
    "    trajectory = compute_trajectory(pos0_a, vel0_a)\n",
    "    plot_game(terrain, castle_a, castle_b, trajectory, \"game_traj_init\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e4f4d-00fa-4fb3-a69e-2cea1c4dca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_game_init(5, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989369a1",
   "metadata": {},
   "source": [
    "To win the game, castle A must find the correct initial velocity vector.\n",
    "This problem is solved below by finding a distribution of initial velocities that will result in a desired distribution of final positions of the explosive.\n",
    "\n",
    "- The desired distribution for the final position is normally distributed at the center of the roof of castle B with a standard deviation of 1 meter.\n",
    "- The corresponding distribution of velocities has no closed analytical form.\n",
    "  This distribution is of interest, not only because the mean is a good choice of the initial velocity.\n",
    "  The spread of the distribution also tells us how precisely the initial velocity must be specified.\n",
    "\n",
    "The code below does the following:\n",
    "\n",
    "- Define MHMC sampling and collect 300 sample points\n",
    "- Visualize the initial velocity distribution.\n",
    "- Plot the trajectory using the average of all initial velocities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d800b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_game_velocities():\n",
    "    \"\"\"Demonstration of MHMC chain to win canon ball in one move.\"\"\"\n",
    "\n",
    "    CASTLE_HEIGHT = 12\n",
    "\n",
    "    terrain, castle_a, castle_b = configure_game()\n",
    "    pos0_a = np.array([castle_a[0], castle_a[1] + CASTLE_HEIGHT])\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    def ln_stat_dens(vel0_a):\n",
    "        \"\"\"Compute the probability density at the final positions.\"\"\"\n",
    "        trajectory = compute_trajectory(pos0_a, vel0_a)\n",
    "        pos1_b = trajectory[:, -1]\n",
    "        pos0_b = np.array([castle_b[0], castle_b[1] + CASTLE_HEIGHT])\n",
    "        sigma = 1.0\n",
    "        delta = pos1_b - pos0_b\n",
    "        return -np.dot(delta, delta) / (2 * sigma)\n",
    "\n",
    "    def prop_gen(xcur):\n",
    "        \"\"\"Propose a change in initial velocities.\"\"\"\n",
    "        # return xcur + rng.standard_normal(size=2) * 2\n",
    "        return xcur + rng.standard_cauchy(size=2) / 2\n",
    "\n",
    "    vel0_a_init = np.array([4.0, 4.0])\n",
    "    chain, ln_sds = mhmc_driver(300, vel0_a_init, ln_stat_dens, prop_gen)\n",
    "    chain = np.array(chain)\n",
    "\n",
    "    # Discard the first steps (burn-in)\n",
    "    ndiscard = 0\n",
    "    chain = chain[ndiscard:]\n",
    "    ln_sds = ln_sds[ndiscard:]\n",
    "\n",
    "    # Plot the MHMC results\n",
    "    plt.close(\"game_mhmc\")\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(7, 4), num=\"game_mhmc\")\n",
    "    vel0_a_mean = chain.mean(axis=0)\n",
    "    axs[0].plot(chain[:, 0], chain[:, 1], \"k-\", label=\"chain\")\n",
    "    axs[0].plot(vel0_a_mean[0], vel0_a_mean[1], \"C2*\", ms=12, label=\"average\")\n",
    "    axs[0].set_xlabel(\"Init vel x [m/s]\")\n",
    "    axs[0].set_ylabel(\"Init vel y [m/s]\")\n",
    "    axs[0].legend(loc=0)\n",
    "    axs[1].plot(ln_sds)\n",
    "    axs[1].set_xlabel(\"MHMC iteration\")\n",
    "    axs[1].set_ylabel(\"Log(prob dens)\")\n",
    "\n",
    "    # Plot the game outcome for the last state in the chain\n",
    "    trajectory = compute_trajectory(pos0_a, vel0_a_mean)\n",
    "    plot_game(terrain, castle_a, castle_b, trajectory, num=\"game_traj_opt\")\n",
    "\n",
    "\n",
    "demo_game_velocities()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8faf2",
   "metadata": {},
   "source": [
    "**Remarks:**\n",
    "\n",
    "- The Markov chain exhibits a significant amount of **burn-in**.\n",
    "  The first state of the initial velocity vector is so far out of distribution, that several iterations are needed to find the center of the distribution.\n",
    "  For such a short run, the burn-in phase biases the averages. \n",
    "  In the limit of very long MC chains, burn-in becomes negligible.\n",
    "  However, such long runs are usually too costly and one resorts to manual removal of the burn-in.\n",
    "  This is often a subjective choice, for which [automatic methods have recently been proposed](https://doi.org/10.1021/acs.jctc.5b00784).\n",
    "  \n",
    "- **The Cauchy distribution is an interesting proposal distribution.**\n",
    "  Due to the heavy tails of the distribution, both large steps (exploration) and small steps (refinement) are considered in one chain.\n",
    "  The same strategy is used by wild animals to optimize their efficiency when gathering food, which is known as the [Lévy Flight Foraging Hypothesis](https://en.wikipedia.org/wiki/L%C3%A9vy_flight_foraging_hypothesis).\n",
    "  Try replacing the Cauchy distribution with a normal distribution: A large standard deviation is needed to quickly overcome the burn-in, but then the steps are generally too large for an efficient sampling of the sharply peaked distribution.\n",
    "\n",
    "- Instead of using a Cauchy distribution, one can also work with **adaptive step sizes** in the proposal distribution.\n",
    "  Naive approaches to control the step size, based on the acceptance rate of previous iterations, may bias the stationary distribution.\n",
    "  Use these with care.\n",
    "  Specialized algorithms have been developed to deal specifically with this difficulty, e.g. the Affine Invariant Markov Chain implemented in [emcee](https://emcee.readthedocs.io/en/stable/) is a good solution for when step sizes are difficult to set manually.\n",
    "  \n",
    "- **This game is a simplified model for many problems in physics** with the same statistical structure:\n",
    "\n",
    "  - *\"For what distribution of physical parameters is a particular process or outcome observed?\"*\n",
    "  \n",
    "    > For such problems, the parameters can be found by coupling an MHMC algorithm to a computer-controlled experimental setup.\n",
    "    > This approach is also used to discover new materials, to crystallize proteins, to discover new pharmaceuticals, etc.\n",
    "    > \n",
    "    > Other algorithms than MHMC, such as genetic algorithms or the particle swarm method, are popular for this purpose.\n",
    "    > Just remember that they don't have a stationary distribution, unless they are cast into an MHMC framework.\n",
    "    \n",
    "  - *\"What is the distribution of model parameters that can explain a distribution of measurements?\"*\n",
    "  \n",
    "    > This type of modeling is called Bayesian inference and goes beyond the scope of this section.\n",
    "  \n",
    "- One **important class of simulations is not covered in this section**, namely simulations of physical systems characterized by a probability density.\n",
    "  These will be treated in the Statistical Physics course.\n",
    "  Needless to say, MHMC is one of the main simulation workhorses in statistical physics.\n",
    "  Besides MHMC, one can also use (stochastic or chaotic) numerical integrators to sample probability densities.\n",
    "  All these techniques are beyond the scope of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38cbb92-1a81-497d-aa38-417a6d7fedca",
   "metadata": {},
   "source": [
    "#### Extended Cannonball game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4a8ab6",
   "metadata": {},
   "source": [
    "Consider the following **extension of the above game**: treat the wind speed as an extra stochastic quantity.\n",
    "Assume that it has a normal distribution with a mean of $0\\,\\textrm{m/s}$ and a standard deviation of $5\\,\\textrm{m/s}$.\n",
    "You can consider two scenarios:\n",
    "\n",
    "  1. Castle A has no control over the wind speed, but can measure it.\n",
    "     Find out how to adjust the distribution of initial explosive velocities as a function of the wind speed.\n",
    "     Instead of sweeping through all wind speeds, treat it as an extra stochastic quantity and collect all the relevant information in one MHMC chain.\n",
    "  \n",
    "  2. Castle A has no control over the wind speed, and cannot measure it.\n",
    "     What is the probability it will still win the game?\n",
    "  \n",
    "Both questions can be solved by analyzing the same MHMC chain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "title": "Monte Carlo"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

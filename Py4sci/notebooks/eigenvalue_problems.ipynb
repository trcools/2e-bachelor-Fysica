{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0f2a102",
   "metadata": {},
   "source": [
    "# Eigenvalue Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02dc772",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import integrate, linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db30cae-69db-4239-be79-eec029efb1bd",
   "metadata": {},
   "source": [
    "## Introduction, concept and useful properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e3a05f",
   "metadata": {},
   "source": [
    "A given direction in a vector space is determined by any nonzero vector pointing in that direction. Given an $n\\times n$ matrix $\\mathbf{A}$ representing a linear transformation on an $n$-dimensional vector space, we wish to find a nonzero vector $\\mathbf{x}$ and a scalar $\\lambda$ such that\n",
    "\n",
    "$$\n",
    "\\mathbf{Ax}=\\lambda\\mathbf{x}\n",
    "$$\n",
    "\n",
    "Such a scalar $\\lambda$ is called an **eigenvalue**, and $\\mathbf{x}$ is a corresponding **eigenvector**. \n",
    "\n",
    "An eigenvector of a matrix determines a direction in which the effect of the matrix is particularly simple: The matrix expands or shrinks any vector lying in that direction by a scalar multiple, and the expansion or contraction factor is given by the corresponding eigenvalue $\\lambda$. Thus, eigenvalues and eigenvectors provide a means of understanding the complicated behavior of a general linear transformation by decomposing it into simpler actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc10e4f7-9799-4c93-9c72-f6703f2a179d",
   "metadata": {},
   "source": [
    "### Characteristic polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9bd5b",
   "metadata": {},
   "source": [
    "For a square matrix $\\mathbf{A}$, the equation $\\mathbf{Ax}=\\lambda\\mathbf{x}$ is equivalent to\n",
    "\n",
    "$$\n",
    "(\\mathbf{A}-\\lambda\\mathbf{I})\\mathbf{x}=\\mathbf{0}\n",
    "$$\n",
    "\n",
    "The eigenvalues of $\\mathbf{A}$ are the values of $\\lambda$ such that\n",
    "\n",
    "$$\n",
    "\\mathrm{det}(\\mathbf{A}-\\lambda\\mathbf{I})=0\n",
    "$$\n",
    "\n",
    "The polynomial $p(\\lambda)=\\mathrm{det}(\\mathbf{A}-\\lambda\\mathbf{I})$ is called the **characteristic polynomial** of $\\mathbf{A}$ and its roots are the eigenvalues of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b3dec",
   "metadata": {},
   "source": [
    "> **Example**\n",
    ">\n",
    "> Consider the matrix\n",
    "> \n",
    "> $$\\begin{bmatrix}\n",
    "3 & 1  \\\\\n",
    "1 & 3 \n",
    "\\end{bmatrix}$$\n",
    ">\n",
    "> The characteristic polynomial is\n",
    "> \n",
    "> $$\n",
    "\\mathrm{det}\\left(\\begin{bmatrix}\n",
    "3 & 1  \\\\\n",
    "1 & 3 \n",
    "\\end{bmatrix}-\\lambda \\begin{bmatrix}\n",
    "1 & 0  \\\\\n",
    "0 & 1 \n",
    "\\end{bmatrix}\\right)\n",
    "=\\mathrm{det}\\left(\\begin{bmatrix} 3-\\lambda& 1 \\\\\n",
    "1 &3-\\lambda\\end{bmatrix}\\right)$$\n",
    ">\n",
    "> $$ = (3-\\lambda)(3-\\lambda)-(1)(1)=\\lambda^2-6\\lambda+8=0$$\n",
    ">\n",
    "> The roots of this polynomial (and hence the eigenvalues of $\\mathbf{A}$) are 4 and 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc43200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute eigenvalues with this technique\n",
    "A = np.array([[3, 1], [1, 3]])\n",
    "\n",
    "\n",
    "def char_pol(A):\n",
    "    \"\"\"compute the characteristic polynomial of a matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A: a square matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A string that shows the characteristic polynomial with X the eigenvalues.\n",
    "    \"\"\"\n",
    "\n",
    "    # First we will check the dimensions of the array\n",
    "    if np.shape(A)[0] != np.shape(A)[1]:\n",
    "        raise TypeError(\"Matrix A must be a square matrix.\")\n",
    "\n",
    "    # Now we will use the .poly() function imported from numpy\n",
    "    # to find the characteristic polynomial.\n",
    "    return np.poly(A)\n",
    "\n",
    "\n",
    "A = char_pol(A)\n",
    "\n",
    "\n",
    "def format_poly(A):\n",
    "    \"\"\"Return the characteristic polynomial of matrix A formatted as a string.\"\"\"\n",
    "    string = \"\"\n",
    "    n = len(A) - 1\n",
    "    for el in A:\n",
    "        string += f\"{el}X^{n} + \"\n",
    "        n -= 1\n",
    "    return string.rstrip(\"+ \")\n",
    "\n",
    "\n",
    "poly = format_poly(A)\n",
    "\n",
    "# The next function will compute the roots of the characteristic polynomial of A\n",
    "# (i.e. the eigenvalues of A).\n",
    "eigenvalues = np.roots(A)\n",
    "\n",
    "print(f\"The characteristic polynomial of matrix A is given by {poly}, \")\n",
    "print(f\"and the corresponding eigenvalues are {eigenvalues}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7eac2",
   "metadata": {},
   "source": [
    "Although, in theory, this is a nice way to find the eigenvalues of a matrix $\\mathbf{A}$, calculating the roots of its characteristic polynomial is not a good numerical way to find the eigenvalues of a matrix of nontrivial size for several reasons\n",
    "- Computing the coefficients of the characteristic polynomial for a large matrix is in itself already a substantial task\n",
    "- The coefficients of the characteristic polynomial can be highly sensitive to small perturbations in $\\mathbf{A}$ which can render their computation instable\n",
    "- Rounding errors in finding the characteristic polynomial can destroy the accuracy of the roots \n",
    "- Computing the roots of a polynomial of high degree is a nontrivial and substantial task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be55a78f-6fc6-4ff4-a2f8-e55d76205ca5",
   "metadata": {},
   "source": [
    "### Properties and transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d51fe27",
   "metadata": {},
   "source": [
    "Many numerical methods for computing eigenvalues and eigenvectors are based on reducing the original matrix to a simpler form, whose eigenvalues and eigenvectors are then easily determined. Thus, we need to identify what types of transformations preserve eigenvalues, and for what types of matrices the eigenvalues are easily determined.\n",
    "\n",
    "- If $\\mathbf{A}$ is symmetric/Hermitian, all its eigenvalues are real.\n",
    "- **Shift**: if $\\mathbf{Ax}=\\lambda\\mathbf{x}$ and $\\sigma$ any scalar, then ($\\mathbf{A}-\\sigma \\mathbf{I})\\mathbf{x}=(\\lambda-\\sigma)\\mathbf{x}$; The eigenvalues are shifted by $\\sigma$, but the eigenvectors remain unchanged.\n",
    "- **Inversion**: $\\mathbf{A}^{-1}$ has the same eigenvectors as $\\mathbf{A}$, and eigenvalues $1/\\lambda$\n",
    "- **Powers**: $\\mathbf{A}^k$ has the same eigenvectors as $\\mathbf{A}$, and eigenvalues $\\lambda^k$\n",
    "- **Polynomials**: for a general polynomial $p(t)$, $p(\\mathbf{A})\\mathbf{x}=p(\\lambda)\\mathbf{x}$. Thus the eigenvalues of a polynomial in a matrix $\\mathbf{A}$ are given by the same polynomial, evaluated at the eigenvalues of $\\mathbf{A}$ and the corresponding eigenvectors remain the same as those of $\\mathbf{A}$.\n",
    "- **Similarity**: A matrix $\\mathbf{B}$ is *similar* to a matrix $\\mathbf{A}$ if there exists an invertible matrix $\\mathbf{T}$ such that\n",
    "- \n",
    "$$\n",
    "\\mathbf{B}=\\mathbf{T^{-1}}\\mathbf{A}\\mathbf{T}\n",
    "$$\n",
    "\n",
    "It follows that:\n",
    "\n",
    "$$\n",
    "\\mathbf{By}=\\lambda\\mathbf{y}\\Rightarrow\\mathbf{T^{-1}}\\mathbf{ATy}=\\lambda\\mathbf{y}\\Rightarrow \\mathbf{ATy}=\\lambda\\mathbf{Ty}\n",
    "$$\n",
    "\n",
    "In other words, $\\mathbf{B}=\\mathbf{T^{-1}}\\mathbf{A}\\mathbf{T}$ has the same eigenvalues as $\\mathbf{A}$, but systematically transforms its eigenvectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fee840",
   "metadata": {},
   "source": [
    "## Calculating eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3214a26-35d6-4b88-b0e7-ef00074a0f45",
   "metadata": {},
   "source": [
    "### Power iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad6536",
   "metadata": {},
   "source": [
    "This is a simple, but limited method that allows to estimate the dominant eigenvalue and its corresponding eigenvector.\n",
    "\n",
    "It works by multiplying an arbitrary nonzero vector repeatedly by the matrix.\n",
    "\n",
    "Assuming that $\\mathbf{A}$ has a unique eigenvalue $\\lambda_1$ of maximum modulus, with corresponding eigenvector $\\mathbf{v}_1$, power iteration converges to a multiple of $\\mathbf{v}_1$.\n",
    "\n",
    "> **Proof:**\n",
    ">\n",
    "> Assume that we can express the starting vector $\\mathbf{x}_0$ as a linear combination $\\mathbf{x}_0=\\sum_{j=1}^n\\alpha_j\\mathbf{v}_j$, with $\\mathbf{v}_j$ the eigenvectors of $\\mathbf{A}$.\n",
    ">\n",
    ">$$\\begin{aligned}\n",
    "\\mathbf{x}_k &= \\mathbf{A}\\mathbf{x}_{k-1}=\\mathbf{A}^2\\mathbf{x}_{k-2}=\\ldots=\\mathbf{A}^k\\mathbf{x}_{0} \\\\\n",
    "&=\\mathbf{A}^k\\sum_{j=1}^n\\alpha_j\\mathbf{v}_j=\\sum_{j=1}^n\\alpha_j\\mathbf{A}^k\\mathbf{v}_j=\\sum_{j=1}^n\\lambda_j^k\\alpha_j\\mathbf{v}_j \\\\\n",
    "&=\\lambda_1^k\\left(\\alpha_1\\mathbf{v}_1+\\sum_{j=2}^n(\\lambda_j/\\lambda_1)^k\\alpha_j\\mathbf{v}_j\\right)\n",
    "\\end{aligned}$$\n",
    ">\n",
    "> Here is $|\\lambda_j/\\lambda_1|$ < 1 since $\\lambda_1$ is of maximum modulus. As a result, this factor will converge to 0 when k becomes large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43571de0",
   "metadata": {},
   "source": [
    "Power iteration usually works well in practice, but might fail for the following reasons:\n",
    "- The starting vector $\\mathbf{x}_0$ may have *no* component in the dominant eigenvector $\\mathbf{v}_1$. In practice this is very unlikely and is mitigated after a few iterations due to rounding errors that introduce such a component.\n",
    "- There may be more than 1 eigenvalue with the same maximum modulus, in which case the algorithm might converge to a linear combination of the corresponding eigenvectors.\n",
    "- For a real matrix and real starting vector, the iteration can never converge to a complex vector.\n",
    "\n",
    "Geometric growth of the components at each iteration risks overflow or underflow, so in practice the approximate eigenvector is rescaled to have norm 1 at every iteration. Then, $\\mathbf{x}_k\\rightarrow\\mathbf{v}_1/\\|\\mathbf{v}_1\\|_\\infty$ and $\\|\\mathbf{y}_k\\|_\\infty\\rightarrow\\|\\lambda_1\\|$. With $\\mathbf{A}\\mathbf{x}_k =\\mathbf{y}_k$ and using the infinity norm defined as $\\|\\mathbf{a}\\|_\\infty$ = $max(|a_1|$, $|a_2|$, ..., $|a_n|$).\n",
    "\n",
    "The convergence rate of power iteration is linear (and proportional with $\\|\\lambda_2/\\lambda_1\\|$, where $\\lambda_2$ is the eigenvalue with second largest modulus.\n",
    "\n",
    "A straightforward implementation is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd331b41-dc17-4704-b5fe-3ffbd0296498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def power(A):\n",
    "    \"\"\"Normalized power iteration.\"\"\"\n",
    "    # x = np.random.random(len(A))\n",
    "    x = np.array([0, 1])\n",
    "    vectors = [x]\n",
    "    for _ in range(15):\n",
    "        y = A @ x\n",
    "        x = y / linalg.norm(y, np.inf)\n",
    "        print(f\"[{x[0]:5.3f}, {x[1]:5.3f}]: {linalg.norm(y, np.inf):.3f}\")\n",
    "        vectors.append(x)\n",
    "    return x, vectors\n",
    "\n",
    "\n",
    "# Example\n",
    "A = np.array([[1, 3], [3, 1]])\n",
    "iteration = power(A)[1]\n",
    "# the actual eigenvectors v_0 and v_1, solved by hand\n",
    "eigenvectors = np.array([1, 1]), np.array([1, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23051d58-7ad4-4bf9-91d8-68ea77f66711",
   "metadata": {},
   "source": [
    "As expected, this converges to the eigenvalue 4, and the corresponding eigenvector `[1, 1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2085ab7e-2fe1-4503-99d6-1f41fadad6ae",
   "metadata": {},
   "source": [
    "### Inverse iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c63a2",
   "metadata": {},
   "source": [
    "For some applications, we're interested in the smallest eigenvalue of a matrix. Then we can make use of the fact that the eigenvalues of $\\mathbf{A}^{-1}$ are $1/\\lambda$. This suggests to use power iteration on the inverse of $\\mathbf{A}$, but as usual the inverse of $\\mathbf{A}$ does not need to be calculated explicitly.\n",
    "\n",
    "Instead, the equivalent system of linear equations is solved at each iteration using the triangular factors resulting from e.g. LU-factorization of $\\mathbf{A}$, which need only to be calculated once. Using $\\mathbf{L}$ and $\\mathbf{U}$, we can then efficiently solve $\\mathbf{Ay}=\\mathbf{x}$ using forward and backward substitution.\n",
    "(These functions are also used for the Rayleigh quotient iteration below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce8c49-1e15-4d3d-b1d6-a0a21f6da424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions for the forward and backward substitution\n",
    "def forward_substitution(L, b):\n",
    "    n = len(L)\n",
    "    x = np.zeros(n)\n",
    "    for j in range(n):\n",
    "        if L[j][j] == 0:  # stop if matrix is singular\n",
    "            break\n",
    "        x[j] = b[j] / L[j][j]\n",
    "        for i in range(j, n):\n",
    "            b[i] = b[i] - L[i][j] * x[j]\n",
    "    return x\n",
    "\n",
    "\n",
    "def backward_substitution(U, b):\n",
    "    n = len(U)\n",
    "    x = np.zeros(n)\n",
    "    # Notice that the last value of range is exclusive,\n",
    "    # which is very counter-intuitive for countdowns).\n",
    "    for j in range(n - 1, -1, -1):\n",
    "        if U[j][j] == 0:  # stop if matrix is singular\n",
    "            break\n",
    "        x[j] = b[j] / U[j][j]\n",
    "        for i in range(0, j):\n",
    "            b[i] = b[i] - U[i][j] * x[j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648c89e-8637-4063-9db4-8e369cbba9b2",
   "metadata": {},
   "source": [
    "Inverse iteration converges to the eigenvector corresponding to the largest eigenvalue of $\\mathbf{A}^{-1}$, which is the smallest eigenvalue of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec55fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_iter(A):\n",
    "    num_iters = 50\n",
    "    tol = 1e-10\n",
    "\n",
    "    _, L, U = linalg.lu(A, permute_l=False)\n",
    "\n",
    "    # Initialize a random starting vector x and normalize it\n",
    "    x = np.random.random(len(A))\n",
    "    x /= linalg.norm(x, np.inf)  # Normalize x to avoid scaling issues\n",
    "\n",
    "    # Lists to store the sequence of approximate eigenvectors and eigenvalues\n",
    "    eigvecs = [x.copy()]\n",
    "    eigvals = [0]\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        # Solve the system A * y = x using LU decomposition, where A = L*U.\n",
    "        # This involves forward and backward substitution.\n",
    "        y = forward_substitution(L, x)  # Solves L * y = x for y\n",
    "        y = backward_substitution(U, y)  # Solves U * y = y for y\n",
    "\n",
    "        # Normalize the resulting vector to avoid numerical overflow or underflow\n",
    "        x = y / linalg.norm(y, np.inf)\n",
    "\n",
    "        # Append the current eigenvector and eigenvalue approximation\n",
    "        eigvecs.append(x.copy())\n",
    "        eigvals.append(linalg.norm(y, np.inf))\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.abs(eigvals[-1] - eigvals[-2]) < tol:\n",
    "            print(\"converged after\", len(eigvals), \"iterations\")\n",
    "            break\n",
    "\n",
    "    return np.array(eigvecs), eigvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0779acba-e0e4-45bf-9b17-a7bd6552ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[3, 1], [1, 3]])\n",
    "inverse_iter(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12d1a80-5ff0-4484-bc0b-202f5d0f9e42",
   "metadata": {},
   "source": [
    "As expected this converges to [-1, 1] which is the eigenvector corresponding to  \n",
    "the dominant eigenvalue of $A^{-1}$ is 0.5.\n",
    "This corroborates, what we already knew, i.e. the smallest eigenvalue of A is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f97c4cc",
   "metadata": {},
   "source": [
    "By shifting the matrix $\\mathbf{A}$ to $\\mathbf{A}-\\sigma\\mathbf{I}$, all eigenvalues are also shifted by $\\sigma$.\n",
    "In case of inverse iteration this approach gives some flexibility in which eigenvalue will be found. If we apply inverse iteration on the matrix $\\mathbf{A}-\\sigma\\mathbf{I}$, the largest eigenvalue of its inverse will be found. The inverse of the eigenvalue gives than the smallest eigenvalue of $\\mathbf{A}-\\sigma\\mathbf{I}$. If we now add $\\sigma$ to the eigenvalue, we find the eigenvalue of $\\mathbf{A}$ closest to $\\sigma$. Also, when the shift is already a close approximation of the eigenvalue, the convergence is very rapid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[3, 1], [1, 3]]) - np.array([[3.8, 0], [0, 3.8]])\n",
    "inverse_iter(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886510f8",
   "metadata": {},
   "source": [
    "When using shifted inverse iteration, the value obtained is the **inverse of the shifted eigenvalue**. To find the corresponding eigenvalue of $ \\mathbf{A} $, first take the reciprocal of the result, then add the shift $ \\sigma $ back. This final value is the eigenvalue of $\\mathbf{A} $ closest to $\\sigma $.\n",
    "\n",
    "In the case of the example above, the eigenvalue is 1/5 + 3.8=4, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793f9e08-c175-4f7d-aa78-0502c629d9a8",
   "metadata": {},
   "source": [
    "### Rayleigh quotient iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e90a698",
   "metadata": {},
   "source": [
    "Given an approximate eigenvector $\\mathbf{x}$ for a real matrix $\\mathbf{A}$, finding the best estimate for the corresponding eigenvalue $\\lambda$ can be considered as a linear least squares approximation problem:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}\\lambda\\cong\\mathbf{Ax}\n",
    "$$\n",
    "\n",
    "It's solution, the **Rayleigh quotient**, is given by\n",
    "\n",
    "$$\n",
    "\\lambda=\\frac{\\mathbf{x}^\\intercal\\mathbf{Ax}}{\\mathbf{x}^\\intercal\\mathbf{x}}\n",
    "$$\n",
    "\n",
    "This is a better approximation for the eigenvalue than the one obtained at each stage in the power iteration algorithm.\n",
    "\n",
    "Given an approximate eigenvector, the Rayleigh quotient provides a good estimate for the corresponding eigenvalue. Conversely, inverse iteration converges very rapidly to an eigenvector if an approximate eigenvalue is used as shift. When combining these ideas, we arrive at **Rayleigh quotient iteration**.\n",
    "\n",
    "An example implementation is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9716fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rayleigh_iter(A):\n",
    "    # x = np.random.random(len(A))\n",
    "    x = np.array([2, 0.05])\n",
    "    eigvecs = [x.copy()]\n",
    "    eigvals = [0]\n",
    "    while True:\n",
    "        shift = (x @ A @ x) / (x @ x)\n",
    "        B = A - shift * np.identity(len(A))\n",
    "        _, L, U = linalg.lu(B, permute_l=False)\n",
    "        y = forward_substitution(L, x)\n",
    "        y = backward_substitution(U, y)\n",
    "        if linalg.norm(y, np.inf) != 0:\n",
    "            x = y / linalg.norm(y, np.inf)\n",
    "            eigvecs.append(x.copy())\n",
    "            eigvals.append(shift)\n",
    "        else:\n",
    "            # the iteration will halt as the shifted matrix\n",
    "            # becomes singular (eigenvalue = 0)\n",
    "            break\n",
    "    return np.array(eigvecs), eigvals\n",
    "\n",
    "\n",
    "A = np.array([[3.0, 1.0], [1.0, 3.0]])\n",
    "rayleigh_iter(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a362b8-c1ec-48dc-91ed-ae7ebc2eb04a",
   "metadata": {},
   "source": [
    "Note that we quickly converge to the eigenvector [1, 1] with eigenvalue 4(quicker than with power iteration)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0022b07-90a7-4ad7-8120-c82373b97138",
   "metadata": {},
   "source": [
    "### Deflation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae764fb",
   "metadata": {},
   "source": [
    "The process of **deflation** removes a known eigenvalue from a matrix, so that further eigenvalues and eigenvectors can be determined. This process is similar to removing a known root $\\lambda_1$ from a polynomial $p(\\lambda)$ by dividing it out to obtain $p(\\lambda)/(\\lambda-\\lambda_1)$.\n",
    "\n",
    "This can be achieved by letting $\\mathbf{u}_1$ be any vector such that $\\mathbf{u}_1^\\intercal \\mathbf{x}_1=\\lambda_1$.\n",
    "Then the matrix $\\mathbf{A}-\\mathbf{x}_1\\mathbf{u}_1^\\intercal$ has eigenvalues $0,\\lambda_2,\\ldots,\\lambda_n$.\n",
    "\n",
    "> An **example** of a deflation procedure is shown below to find both eigenvalues of the matrix\n",
    ">\n",
    "> $$\\begin{bmatrix}\n",
    "3 & 1  \\\\\n",
    "1 & 3 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Note that the eigenvector found depends on the choice of $\\mathbf{u}_1$, when using this procedure and **the remaining eigenvectors of the deflated matrix are generally different from those of the original matrix**. This is why deflation is often limited to theoretical applications, and practical computations of multiple eigenvalues are usually performed with other methods, such as shifted inverse iteration.\n",
    "\n",
    "We're not going to look deeper into this procedure because \n",
    "- it becomes increasingly cumbersome and numerically less accurate to find eigenvalues using deflation (so that inverse iteration using the estimated eigenvalues as a shift are necessary)\n",
    "- there are better ways to find many eigenvalues of a matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0027087c-fd22-42cc-a4b9-5eef29e7eaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by finding the largest eigenvalue of A using power iteration\n",
    "# which gives us \"4\"\n",
    "A = np.array([[3, 1], [1, 3]])\n",
    "print(\"Original Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nApplying Power Iteration to A:\")\n",
    "power(A);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c986cc",
   "metadata": {},
   "source": [
    "We now perform deflation to remove the largest eigenvalue $ \\lambda_1 = 4 $ using two different choices of $\\mathbf{u}_1 $.\n",
    "\n",
    "**First Choice of $\\mathbf{u}_1$**\n",
    "\n",
    "We choose:\n",
    "\n",
    "$$\n",
    "\\mathbf{u}_1 = \\begin{bmatrix} 0 \\\\ 4 \\end{bmatrix}, \\quad \\mathbf{x}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15871ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.array([[0], [4]])\n",
    "x = np.array([[1], [1]])\n",
    "print(\"\\nFirst Choice for u:\")\n",
    "print(\"u =\", u.flatten())\n",
    "print(\"\\nVerifying u^\\intercal x = lambda_1:\")\n",
    "print(\"u^\\intercal x =\", np.dot(u.T, x))\n",
    "\n",
    "# Perform deflation\n",
    "print(\"\\nDeflating A with x * u^\\intercal:\")\n",
    "A_deflated = A - np.dot(x, u.T)\n",
    "print(A_deflated)\n",
    "print(\"\\nApplying Power Iteration to Deflated A:\")\n",
    "power(A_deflated);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f7701b",
   "metadata": {},
   "source": [
    "**Second Choice of $\\mathbf{u}_1$**\n",
    "\n",
    "Now we choose a different vector $ \\mathbf{u}_1 $:\n",
    "\n",
    "$$\\mathbf{u}_1 = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}$$\n",
    "\n",
    "Repeating the steps with this choice of $ \\mathbf{u}_1 $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3655dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[3, 1], [1, 3]])\n",
    "print(\"\\nSecond Choice for u:\")\n",
    "u = np.array([[2], [2]])\n",
    "x = np.array([[1], [1]])\n",
    "print(\"u =\", u.flatten())\n",
    "print(\"\\nVerifying u^\\intercal x = lambda_1:\")\n",
    "print(\"u^\\intercal x =\", np.dot(u.T, x))\n",
    "\n",
    "# Perform deflation\n",
    "print(\"\\nDeflating A with x * u^\\intercal:\")\n",
    "A_deflated = A - np.dot(x, u.T)\n",
    "print(A_deflated)\n",
    "print(\"\\nApplying Power Iteration to Deflated A:\")\n",
    "power(A_deflated);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6636e6fe-10d5-420a-b3ac-29d86d1e6399",
   "metadata": {},
   "source": [
    "###  QR  Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72643b39",
   "metadata": {},
   "source": [
    "In practice, the fastest and most used method to find the eigenvalues of a matrix is **QR iteration**. \n",
    "Starting from a matrix $\\mathbf{A}$, we define the following sequence:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{A}_m & = \\mathbf{Q}_m\\mathbf{R}_m \\\\\n",
    "\\mathbf{A}_{m+1} &= \\mathbf{R}_m\\mathbf{Q}_m\n",
    "\\end{aligned}$$\n",
    "\n",
    "With $\\mathbf{Q}$ an orthogonal matrix and an $\\mathbf{R}$ an upper-triangular matrix. This sequence will converge to a triangular matrix with the eigenvalues of $\\mathbf{A}$ on its diagonal, or a near-triangular form, which easily allows calculating the eigenvalues.\n",
    "\n",
    "As an example, we use QR iteration on the matrix\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "2.9766&0.3945&0.4198&1.1159\\\\\n",
    "0.3945&2.7328&-0.3097&0.1129\\\\\n",
    "0.4198&-0.3097&2.5675&0.6079\\\\\n",
    "1.1159&0.1129&0.6079&1.7231\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "which has eigenvalues 1, 2, 3 and 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da447d7-b4cc-4a7d-b43e-a3bb43990f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_iter(A):\n",
    "    \"\"\"QR iteration\"\"\"\n",
    "    for _ in range(10):\n",
    "        q, r = linalg.qr(A)\n",
    "        A = np.dot(r, q)\n",
    "        print(A)\n",
    "        print()\n",
    "    return A\n",
    "\n",
    "\n",
    "A = np.array(\n",
    "    [\n",
    "        [2.9766, 0.3945, 0.4198, 1.1159],\n",
    "        [0.3945, 2.7328, -0.3097, 0.1129],\n",
    "        [0.4198, -0.3097, 2.5675, 0.6079],\n",
    "        [1.1159, 0.1129, 0.6079, 1.7231],\n",
    "    ]\n",
    ")\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    qr_iter(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e76826",
   "metadata": {},
   "source": [
    "To speed up this procedure we can use shifts, similar to their use in the power method. The most straightforward choice as shift is the lower right element of the matrix, but depending on the specifics of the problem better shifts might exist. In the example below, note how the obtained off-diagonal entries converge faster to zero than in the case without shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c26398-8e66-42a1-95fc-9d1fb7322d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_iter_shift(A):\n",
    "    \"\"\"QR iteration with shift.\"\"\"\n",
    "    for _ in range(9):\n",
    "        shift = A[len(A) - 1][len(A) - 1]\n",
    "        q, r = linalg.qr(A - shift * np.identity(len(A)))\n",
    "        A = np.dot(r, q) + shift * np.identity(len(A))\n",
    "        print(A)\n",
    "        print()\n",
    "    return A\n",
    "\n",
    "\n",
    "A = np.array(\n",
    "    [\n",
    "        [2.9766, 0.3945, 0.4198, 1.1159],\n",
    "        [0.3945, 2.7328, -0.3097, 0.1129],\n",
    "        [0.4198, -0.3097, 2.5675, 0.6079],\n",
    "        [1.1159, 0.1129, 0.6079, 1.7231],\n",
    "    ]\n",
    ")\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    qr_iter_shift(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d49ede-248a-435a-8362-ba30c08d7614",
   "metadata": {},
   "source": [
    "## Calculating the Singular Value Decomposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef156b-961c-4a45-a9e9-f58ea4704113",
   "metadata": {},
   "source": [
    "Recall from the notebook about linear least squares that the **singular value decomposition (SVD)** of an $m \\times n$ matrix $\\mathbf{A}$ has the form\n",
    "\n",
    "$$\n",
    "\\mathbf{A}=\\mathbf{U\\Sigma V^\\intercal}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{U}$ is an $m \\times m$ orthogonal matrix,  $\\mathbf{V}$ is an $n \\times n$ orthogonal matrix, and $\\mathbf{\\Sigma}$ is an $m \\times n$ diagonal matrix, with \n",
    "\n",
    "$$\n",
    "\\sigma_{ij}=\\begin{cases}\n",
    "    0, & \\text{for $i\\neq j$}\\\\\n",
    "    \\sigma_i\\geq 0, & \\text{for $i=j$}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "The diagonal entries $\\sigma_i$ are called the **singular values** of $\\mathbf{A}$ and are usually ordered so that $\\sigma_{i-1}\\geq \\sigma_{i}, i=2,\\ldots,\\mathrm{min}\\{m,n\\}$, i.e. from largest value (upper left) to smallest value (bottom right). The columns $\\mathbf{u}_i$ of $\\mathbf{U}$ and $\\mathbf{v}_i$ of $\\mathbf{V}$ are the corresponding left and right **singular vectors**.\n",
    "\n",
    "We discussed some handy applications of the SVD in that notebook, but postponed the calculation of the decomposition matrices. Here, we revisit the concept because singular values and vectors are intimately related to eigenvalues and eigenvectors. The singular values of $\\mathbf{A}$ are the non-negative square roots of the eigenvalues of $\\mathbf{A^\\intercal A}$, and the columns of $\\mathbf{U}$ and $\\mathbf{V}$ are orthonormal eigenvectors of $\\mathbf{A A^\\intercal}$ and $\\mathbf{A^\\intercal A}$, respectively.\n",
    "\n",
    "> **Example**\n",
    ">\n",
    "> The singular value decomposition of the matrix \n",
    ">\n",
    "> $$\\mathbf{A} = \\begin{bmatrix}\n",
    "3 & 1  \\\\\n",
    "1 & 3\n",
    "\\end{bmatrix}$$\n",
    "is given by\n",
    ">\n",
    "> $$\\mathbf{U\\Sigma V^\\intercal} = \\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{2}} & - \\frac{1}{\\sqrt{2}} \\\\\n",
    "\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "4 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n",
    "-\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "> \n",
    "> This statement can be verified by explicitly calculating $\\mathbf{U}$, $\\mathbf{\\Sigma}$ and $\\mathbf{V}$. We begin with\n",
    ">\n",
    "> $$\n",
    "\\mathbf{A^\\intercal A} = \\mathbf{A A^\\intercal} = \\begin{bmatrix}\n",
    "10 & 6 \\\\\n",
    "6 & 10\n",
    "\\end{bmatrix}\n",
    "$$\n",
    ">\n",
    "> which are equal here because $\\mathrm{A}$ is a symmetric matrix. We can employ one of the methods discussed above to calculate its eigenvalues and eigenvectors. These are $\\lambda_1 = \\sigma_1^2 = 16$ with eigenvector $\\mathbf{v}_1 = [1, 1]^\\intercal$ and $\\lambda_2 = \\sigma_2^2 = 4$ with $\\mathbf{v}_2 =[-1, 1]^\\intercal$.\n",
    "> The eigenvectors are easily converted to their orthonormal form, which results in\n",
    ">\n",
    "> $$\n",
    "\\mathbf{U} = \\mathbf{V} = \\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{2}} & - \\frac{1}{\\sqrt{2}} \\\\\n",
    "\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    ">\n",
    "> Now we construct $\\mathbf{\\Sigma}$ as diag$\\left(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2}\\right)$ and transpose $\\mathbf{V}$ in order to find the proposed SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea5555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for the example\n",
    "\n",
    "A = np.array([(3, 1), (1, 3)])\n",
    "\n",
    "transA = A.transpose()\n",
    "product = A @ transA\n",
    "eigenvalues, U = np.linalg.eig(product)\n",
    "eigenvalues = np.sqrt(eigenvalues)\n",
    "Delta = np.diag(eigenvalues)\n",
    "\n",
    "SVD = U @ Delta @ U.transpose()\n",
    "\n",
    "print(\"A^\\intercalA = AA^\\intercal = \")\n",
    "print(product)\n",
    "print(\"\\neigenvalues =\")\n",
    "print(eigenvalues)\n",
    "\n",
    "print(\"\\nU = V =\")\n",
    "print(U)\n",
    "\n",
    "print(\"\\nSVD =\")\n",
    "print(SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca8cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or with SciPy's SVD\n",
    "U, s, vh = linalg.svd(A)\n",
    "SVD = U @ np.diag(s) @ vh\n",
    "print(U, \"\\n\\n\", s, \"\\n\\n\", vh, \"\\n\\n\", SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0b1535-4334-4dbc-8b5d-e2d16fd13b14",
   "metadata": {},
   "source": [
    "## Software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0b601",
   "metadata": {},
   "source": [
    "Until just a few years ago, QR iteration was the standard method for computing all of the eigenvalues (and optionally eigenvectors) of the resulting tridiagonal matrix. More recently, however, first divide-and-conquer and then relatively robust representation (RRR) methods (not shown in this course) have surpassed QR iteration in speed for computing all the eigenvectors. Implementations of both of these newer methods are available in LAPACK, but they do not yet have the decades-long record of reliability enjoyed by QR iteration. Thus, for now, the choice is between the speed of the newer methods, especially RRR, and the more proven dependability of QR iteration.\n",
    "\n",
    "In `scipy` the most general method you can use is `linalg.eig`, which uses QR iteration. There exist similar methods to calculate an SVD. However, if your matrix has special properties, there are faster options that specifically make use of this information:\n",
    "\n",
    "| Method                 | Description                                                                                           |\n",
    "|:-----------------------|:------------------------------------------------------------------------------------------------------|\n",
    "| `eig`                  | Solve an ordinary or generalized eigenvalue problem of a square matrix.                               |\n",
    "|\n",
    "| `eigvals`              | Compute eigenvalues from an ordinary or generalized eigenvalue problem.                               |\n",
    "| `eigh`                 | Solve a standard or generalized eigenvalue problem for a complex Hermitian or real symmetric matrix.  |\n",
    "| `eigvalsh`             | Solves a standard or generalized eigenvalue problem for a complex Hermitian or real symmetric matrix. |\n",
    "| `eig_banded`           | Solve real symmetric or complex Hermitian band matrix eigenvalue problem.                             |\n",
    "| `eigvals_banded`       | Solve real symmetric or complex Hermitian band matrix eigenvalue problem.                             |\n",
    "| `eigh_tridiagonal`     | Solve eigenvalue problem for a real symmetric tridiagonal matrix.                                     |\n",
    "| `eigvalsh_tridiagonal` | Solve eigenvalue problem for a real symmetric tridiagonal matrix.                                     |\n",
    "|                        |\n",
    "| `svd`                  | Compute the single decomposition matrices.                                                            |\n",
    "| `svdvals`              | Compute singular values of a matrix.                                                                  |\n",
    "\n",
    "Further documentation can be found [here](https://docs.scipy.org/doc/scipy/tutorial/linalg.html#eigenvalues-and-eigenvectors) and [here](https://docs.scipy.org/doc/scipy/tutorial/linalg.html#singular-value-decomposition).\n",
    "\n",
    "An example of the use of `linalg.eig` is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa58a2b-f2d8-4921-ab1e-c3fc17a99a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array(\n",
    "    [\n",
    "        [2.9766, 0.3945, 0.4198, 1.1159],\n",
    "        [0.3945, 2.7328, -0.3097, 0.1129],\n",
    "        [0.4198, -0.3097, 2.5675, 0.6079],\n",
    "        [1.1159, 0.1129, 0.6079, 1.7231],\n",
    "    ]\n",
    ")\n",
    "\n",
    "la, v = linalg.eig(A)\n",
    "l1, l2, l3, l4 = la\n",
    "print(l1, l2, l3, l4)  # eigenvalues\n",
    "\n",
    "print(v[:, 0])  # first eigenvector\n",
    "print(v[:, 1])  # second eigenvector\n",
    "print(v[:, 2])  # third eigenvector\n",
    "print(v[:, 3])  # fourth eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd206b-5cbc-4e80-b250-1a3a63ef9422",
   "metadata": {},
   "source": [
    "## Physics Example: spring-and-mass system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07805284",
   "metadata": {
    "tags": []
   },
   "source": [
    "> This example is partially inspired by the cc-licensed material from Michael Richmond found [here](http://spiff.rit.edu/classes/phys283/lectures/eigen_ii/eigen_ii.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba711d7",
   "metadata": {},
   "source": [
    "Consider the following system consisting of 2 masses, connected by identical springs fixed to a wall at the sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e552d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_spring(x1, x2, steps=12, height=1.0):\n",
    "    l = x2 - x1\n",
    "    nd = np.sqrt(max(0, height**2 - (l**2 / steps**2))) / 2\n",
    "\n",
    "    def rx(i):\n",
    "        return x1 + (l * (2 * i - 1)) / (2 * steps)\n",
    "\n",
    "    def ry(i):\n",
    "        return nd * (i % 2 * 2 - 1)\n",
    "\n",
    "    s = [(rx(i), ry(i)) for i in range(1, steps + 1)]\n",
    "    s = [(x1, 0), *s, (x2, 0)]\n",
    "\n",
    "    return np.array(s)\n",
    "\n",
    "\n",
    "def draw_springs(spring_configs):\n",
    "    # The use of matplotlib to draw the spring system is considered\n",
    "    # \"spielerei\" and not part of the course material.\n",
    "    springs = [\n",
    "        mk_spring(x1, x2, steps=s, height=h) for (x1, x2, s, h) in spring_configs\n",
    "    ]\n",
    "\n",
    "    # Plot settings\n",
    "    plt.close(\"springs\")\n",
    "    fig, ax = plt.subplots(num=\"springs\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_ylim(-1, 1)\n",
    "\n",
    "    # Walls\n",
    "    ax.axvline(x=0, color=\"k\", linewidth=3)\n",
    "    ax.axvline(x=23, color=\"k\", linewidth=3)\n",
    "\n",
    "    # Springs\n",
    "    for spring in springs:\n",
    "        ax.plot(*spring.T, \"k\")\n",
    "        ax.plot(*spring[0], \"ko\", markersize=7)\n",
    "        ax.plot(*spring[-1], \"ko\", markersize=7)\n",
    "\n",
    "    # Labels\n",
    "    for idx, spring in enumerate(springs[1:]):\n",
    "        ax.plot(spring[0][0], spring[0][1], \"ko\", markersize=7)\n",
    "        ax.text(spring[0][0], spring[0][1] - 0.12, f\"M{idx+1}\")\n",
    "\n",
    "\n",
    "draw_springs([(0, 7, 12, 1), (7, 16, 12, 1), (16, 23, 12, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d195a9",
   "metadata": {},
   "source": [
    "Let's call $x_1$ and $x_2$ the displacements of $M_1$ and $M_2$, respectively, from their equilibrium positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e63b3",
   "metadata": {},
   "source": [
    "The forces (which define the accelerations) acting on each mass are\n",
    "\n",
    "$$\\begin{aligned}\n",
    "  F_1=M_1\\frac{d^2x_1}{dt^2} & =-kx_1 +k(x_2-x_1) \\\\\n",
    "  F_2=M_2\\frac{d^2x_2}{dt^2} & =-k(x_2-x_1)-k x_2\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc10f8d",
   "metadata": {},
   "source": [
    "This can be written as the following matrix equation\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-\\frac{2k}{M_1}&\\frac{k}{M_1}\\\\\n",
    "\\frac{k}{M_2}&-\\frac{2k}{M_2}\n",
    "\\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}\n",
    "=\\begin{bmatrix}\\frac{d^2x_1}{dt^2}\\\\\\frac{d^2x_2}{dt^2}\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa75e97e",
   "metadata": {},
   "source": [
    "We're looking for a specific combination of $x_1$ and $x_2$ for which $\\mathbf{Ax}=\\lambda\\mathbf{x}$ with $\\lambda$ the eigenvalue and $\\mathbf{x}$= $[a$  $b]^\\intercal$ the corresponding eigenvector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ec8318",
   "metadata": {},
   "source": [
    "When comparing this to our original matrix equation, this means that \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-\\frac{2k}{M_1}&\\frac{k}{M_1}\\\\\n",
    "\\frac{k}{M_2}&-\\frac{2k}{M_2}\n",
    "\\end{bmatrix}\\begin{bmatrix}a\\\\b\\end{bmatrix}=\\lambda\n",
    "\\begin{bmatrix}a\\\\b\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f780e16",
   "metadata": {},
   "source": [
    "We can move the right-hand side to the left and end up with\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-\\frac{2k}{M_1}&\\frac{k}{M_1}\\\\\n",
    "\\frac{k}{M_2}&-\\frac{2k}{M_2}\n",
    "\\end{bmatrix}\\begin{bmatrix}a\\\\b\\end{bmatrix}-\\lambda\n",
    "\\begin{bmatrix}a\\\\b\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde0854c",
   "metadata": {},
   "source": [
    "Writing these equations out explicitly gives\n",
    "\n",
    "$$\\begin{aligned}\n",
    " \\left(\\frac{-2k}{M_1}-\\lambda\\right) a + \\frac{k}{M_1}b= & 0 \\\\\n",
    " \\frac{k}{M_2} a - \\left(\\frac{2k}{M_2}+\\lambda\\right)b= & 0\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a4c09",
   "metadata": {},
   "source": [
    "The latter can be written as\n",
    "\n",
    "$$a=\\frac{M_2}{k}\\left(\\frac{2k}{M_2}+\\lambda\\right)b=\\left(2+\\frac{M_2\\lambda}{k}\\right)b$$\n",
    "\n",
    "and be filled in in the former to give\n",
    "\n",
    "$$\\left(\\frac{-2k}{M_1}-\\lambda\\right) \\left(2+\\frac{M_2\\lambda}{k}\\right)b + \\frac{k}{M_1}b=0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8dc900",
   "metadata": {},
   "source": [
    "$$-\\frac{M_2}{k}\\lambda^2-\\left(2+\\frac{2M_2}{M_1}\\right)\\lambda-\\frac{3k}{M_1}=0$$\n",
    "\n",
    "multiply by $-\\frac{k}{M_2}$\n",
    "\n",
    "$$\\lambda^2+2k\\left(\\frac{M_1+M_2}{M_1M_2}\\right)\\lambda+\\frac{3k^2}{M_1M_2}=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b7eb3",
   "metadata": {},
   "source": [
    "Solving this for $\\lambda$ gives\n",
    "\n",
    "$$\\lambda=\\frac{-2k\\left(\\frac{M_1+M_2}{M_1M_2}\\right)\\pm\\sqrt{4k^2\\left(\\frac{M_1+M_2}{M_1M_2}\\right)^2-4\\frac{3k^2}{M_1M_2}}}{2}$$\n",
    "\n",
    "Or,\n",
    "\n",
    "$$\\lambda=\\frac{-k}{M_1M_2}\\left[(M_1+M_2)\\pm\\sqrt{(M_1-M_2)^2+M_1M_2}\\right]$$\n",
    "\n",
    "For simplicity, let's assume that $M_1=M_2$ so this reduces to $\\lambda=-k/M$ and $\\lambda=-3k/M$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6b1885",
   "metadata": {},
   "source": [
    "Using these $\\lambda$ in the following set of equations\n",
    "\n",
    "$$\\begin{aligned}\n",
    "  \\left(\\frac{-2k}{M_1}-\\lambda\\right) a + \\frac{k}{M_1}b= & 0 \\\\\n",
    "  \\frac{k}{M_2} a - \\left(\\frac{2k}{M_2}+\\lambda\\right)b= & 0\n",
    "\\end{aligned}$$\n",
    "\n",
    "gives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a68a53",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "  \\left(\\frac{-2k}{M}+\\frac{k}{M}\\right) a + \\frac{k}{M}b= & 0 \\\\\n",
    "  \\frac{k}{M} a - \\left(\\frac{2k}{M}-\\frac{k}{M}\\right)b= & 0\n",
    "\\end{aligned}$$ \n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{aligned}\n",
    "  \\left(\\frac{-2k}{M}+3\\frac{k}{M}\\right) a + \\frac{k}{M}b= & 0 \\\\\n",
    "  \\frac{k}{M} a - \\left(\\frac{2k}{M}-3\\frac{k}{M}\\right)b= & 0\n",
    "\\end{aligned}$$ \n",
    "\n",
    "which can be solved as $a=b$ and $a=-b$, so the corresponding eigenvectors are $[1$  $1]^\\intercal$ and $[1$  $-1]^\\intercal$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d553a3a",
   "metadata": {},
   "source": [
    "We could have saved ourselves all this work if we would just have asked scipy:\n",
    "\n",
    "For instance, for $k=M_1=M_2=1$, we would find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f7e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([(-2, 1), (1, -2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851fbe7-933c-4d75-99b7-792c077a061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "la, v = linalg.eig(A)\n",
    "print(la, \"\\n\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9862202a",
   "metadata": {},
   "source": [
    "Which are indeed the expected eigenvalues of -1 and -3 and eigenvectors $[1$  $1]^\\intercal$ and $[1$  $-1]^\\intercal$ (normalized to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8e260-dfb6-4825-8861-e50ff6c185ca",
   "metadata": {},
   "source": [
    ">You could find a simple animation for the three eigenmodes of the oscillator [here](https://www.acs.psu.edu/drussell/Demos/multi-dof-springs/multi-dof-springs.html). The animations and text on the page are Â©2004-2013 by Daniel A. Russell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc24d00b",
   "metadata": {},
   "source": [
    "If we now look at this combination of the original equations in our set \n",
    "\n",
    "$$\\begin{aligned}\n",
    "  F_1=M\\frac{d^2x_1}{dt^2} & =-kx_1 +k(x_2-x_1)  \\\\\n",
    "  F_2=M\\frac{d^2x_2}{dt^2} & =-k(x_2-x_1)-k x_2\n",
    "\\end{aligned}$$\n",
    "\n",
    "we find for the eigenvector $[1 1]^\\intercal$:\n",
    "\n",
    "$$M\\frac{d^2x_1}{dt^2}+M\\frac{d^2x_1}{dt^2}=-k (x_1+x_2)$$\n",
    "\n",
    "and for eigenvector $[1 -1]^\\intercal$:\n",
    "\n",
    "$$M\\frac{d^2x_2}{dt^2}-M\\frac{d^2x_1}{dt^2}=-3k (x_2-x_1)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528da29b",
   "metadata": {},
   "source": [
    "when introducing the variables $s_1=(x_1+x_2)$ and $s_2=(x_2-x_1)$ this results in the following equations\n",
    "\n",
    "$$\\begin{aligned}\n",
    "  \\frac{d^2s_1}{dt^2} & =-\\frac{k}{M}s_1 \\\\\n",
    "  \\frac{d^2s_2}{dt^2} & =-\\frac{3k}{M}s_2\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b34c7f",
   "metadata": {},
   "source": [
    "which can easily be solved as\n",
    "\n",
    "$$\\begin{aligned}\n",
    "  x_1+x_2=s_1 & =A \\cos\\left(\\sqrt{k/M}t+\\phi\\right) \\\\\n",
    "  x_2-x_1=s_2 & =A \\cos\\left(\\sqrt{3k/M}t+\\phi\\right)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ebf416",
   "metadata": {},
   "source": [
    "which shows that solving $A\\mathbf{s}=\\lambda\\mathbf{s}$ very elegantly give you the dynamical equations that describe this system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e08b95-ca96-4663-bce6-6a1bb48fe2ac",
   "metadata": {},
   "source": [
    "### Dynamical problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc688e01",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's say we would want to know the position of the first block at time $t = 5s$, given the following initial conditions.\n",
    "- the mass equals $M = 1\\,\\mathrm{kg}$\n",
    "- the constant of the springs  $k = 1\\,\\mathrm{N/m}$\n",
    "- at $t=0$, the first mass is at position $x_1 = 2\\,\\mathrm{m}$\n",
    "- at $t = 0$ the second mass is at position $x_2 = -1\\,\\mathrm{m}$\n",
    "- at $t=0$, the starting velocity is $v_1 = -1\\,\\mathrm{m/s}$\n",
    "- at $t=0$, the staring velocity is $v_2 = 1\\,\\mathrm{m/s}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a1143-3825-4d14-87bd-45155d60aae2",
   "metadata": {},
   "source": [
    "We can find the constants of integration by plugging these conditions into $s_1$ and $s_2$.\n",
    "\n",
    "$$\n",
    "s_1(0) = 2 - 1 = A_1 \\cos\\left(\\sqrt{1/1} \\cdot 0+\\phi_1\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c5fb3-a2a4-4aa9-aacc-588104552c1d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\implies \\left\\lbrace \\begin{aligned}\n",
    "    A_1 &= 1\\,\\mathrm{m} \\\\\n",
    "    \\phi_1 &= 0\n",
    "\\end{aligned} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb79d30-ae4c-4292-b6c1-1ecd2d4335dc",
   "metadata": {},
   "source": [
    "By using the same method in $s_2$ and its time derivative, we find;\n",
    "\n",
    "$$\n",
    "\\left\\lbrace \\begin{aligned}\n",
    "  A_2 &= -3.21\\,\\mathrm{m} \\\\\n",
    "  \\phi_2 &= 0.364\\,\\mathrm{rad}\n",
    "\\end{aligned}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5dfc73-040c-40a1-b5e2-de488fd56fee",
   "metadata": {},
   "source": [
    "Such that,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "  s_1(t) & = (1\\,\\mathrm{m}) \\cos\\left(\\omega_1 t \\right) \\\\\n",
    "  s_2(t) & = (-3.21\\,\\mathrm{m}) \\cos\\left(\\omega_2 t +0.364\\,\\mathrm{rad} \\right)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b797f0d-16cd-46fc-8b7c-c90820b1555e",
   "metadata": {},
   "source": [
    "We want to know the positions of the actual boxes, so we need the expressions for $x_1(t)$ and $x_2(t)$ instead of $s_1(t)$ and $s_2(t)$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1(t) &= \\frac{1}{2} \\left[ s_1(t) - s_2(t) \\right] \\\\\n",
    "&= (0.5\\,\\mathrm{m}) \\cos(\\omega_1 t) + (1.61\\,\\mathrm{m}) \\cos(\\omega_2 t + 0.364\\,\\mathrm{rad})\\\\\n",
    "\\\\\n",
    "x_2(t) &= \\frac{1}{2} \\left[ s_1(t) + s_2(t) \\right] \\\\\n",
    "&= (0.5\\,\\mathrm{m}) \\cos(\\omega_1 t) - (1.61\\,\\mathrm{m}) \\cos(\\omega_2 t + 0.364\\,\\mathrm{rad})\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To find the positions of the masses we define the functions found with the initial conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8bea73-dd43-4db4-a1b5-d0627fe30ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x1(t):\n",
    "    k = 1\n",
    "    M = 1\n",
    "    pos = 0.5 * np.cos(np.sqrt(k / M) * t) + 1.6 * np.cos(\n",
    "        np.sqrt(3 * k / M) * t + 0.364\n",
    "    )\n",
    "    return pos\n",
    "\n",
    "\n",
    "def x2(t):\n",
    "    k = 1\n",
    "    M = 1\n",
    "    pos = 0.5 * np.cos(np.sqrt(k / M) * t) - 1.6 * np.cos(\n",
    "        np.sqrt(3 * k / M) * t + 0.364\n",
    "    )\n",
    "    return pos\n",
    "\n",
    "\n",
    "print(\"Mass one has position x =\", x1(5), \"at time t = 5s.\")\n",
    "print(\"Mass two has position x =\", x2(5), \"at time t = 5s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d60b5-e450-468c-a5dd-aeb59d8ec983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_solution():\n",
    "    \"\"\"The positions of the masses over time for our initial conditions.\"\"\"\n",
    "    plt.close(\"solution\")\n",
    "    fig, ax = plt.subplots(figsize=(7, 4), num=\"solution\")\n",
    "    t = np.arange(0, 50, 0.1)\n",
    "    ax.plot(t, x1(t))\n",
    "    ax.plot(t, x2(t))\n",
    "\n",
    "    ax.set_xlim([0, 50])\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend((\"Block 1\", \"Block 2\"))\n",
    "    ax.set_xlabel(\"Time $t$ (s)\")\n",
    "    ax.set_ylabel(\"displacement from equlibrium $x$ (m)\")\n",
    "\n",
    "\n",
    "plot_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2b0f39-4892-4890-b006-ebe9f01c96a8",
   "metadata": {},
   "source": [
    "#### Alternative solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c29ab",
   "metadata": {},
   "source": [
    "Let's solve the same problem by integrating the differential equations (see notebook integrating ODE's).\n",
    "\n",
    "Adapted from <https://scipy-cookbook.readthedocs.io/items/CoupledSpringMassSystem.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b6e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorfield(w, t, p):\n",
    "    \"\"\"Defines the differential equations for the coupled spring-mass system.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w\n",
    "        vector of the state variables: w = [x1,y1,x2,y2].\n",
    "    t\n",
    "        time.\n",
    "    p\n",
    "        vector of the parameters: p = [m1,m2,k1,k2].\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = w\n",
    "    m1, m2, k1, k2 = p\n",
    "\n",
    "    # Create f = (x1',y1',x2',y2'):\n",
    "    f = [y1, (-k1 * x1 + k2 * (x2 - x1)) / m1, y2, (-k2 * (x2 - x1) - k2 * x2) / m2]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a331b832",
   "metadata": {},
   "source": [
    "The code above explicitly implements the following set of 1st order differential equations, equivalent to the 2 2nd order ODE's given by Newton's law we initially began the example with:\n",
    "\n",
    "$$\n",
    "\\left\\lbrace\\begin{aligned}\n",
    "\\frac{dx_1}{dt} &= y_1 \\\\[4pt]\n",
    "\\frac{dy_1}{dt} &= \\frac{-k_1 x_1 + k_2 (x_2 - x_1)}{m_1} \\\\[4pt]\n",
    "\\frac{dx_2}{dt} &= y_2 \\\\[4pt]\n",
    "\\frac{dy_2}{dt} &= \\frac{-k_2 (x_2 - x_1) - k_2 x_2}{m_2}\n",
    "\\end{aligned}\\right.\n",
    "$$\n",
    "\n",
    "Below, it will be solved using methods we saw in the ODE notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_ode():\n",
    "    \"\"\"Use ODEINT to solve the differential equations defined by the vector field.\"\"\"\n",
    "\n",
    "    # Parameter values\n",
    "    # Masses:\n",
    "    m1 = 1.0\n",
    "    m2 = 1.0\n",
    "    # Spring constants\n",
    "    k1 = 1\n",
    "    k2 = 1\n",
    "\n",
    "    # Initial conditions\n",
    "    # x1 and x2 are the initial displacements; y1 and y2 are the initial velocities\n",
    "    x1 = 2.0\n",
    "    y1 = -1.0\n",
    "    x2 = -1.0\n",
    "    y2 = 1.0\n",
    "\n",
    "    # ODE solver parameters\n",
    "    abserr = 1.0e-8\n",
    "    relerr = 1.0e-6\n",
    "    stoptime = 50.0\n",
    "    numpoints = 1000\n",
    "\n",
    "    # Create the time samples for the output of the ODE solver.\n",
    "    # I use a large number of points, only because I want to make\n",
    "    # a plot of the solution that looks nice.\n",
    "    t = [stoptime * float(i) / (numpoints - 1) for i in range(numpoints)]\n",
    "\n",
    "    # Pack up the parameters and initial conditions:\n",
    "    p = [m1, m2, k1, k2]\n",
    "    w0 = [x1, y1, x2, y2]\n",
    "\n",
    "    # Call the ODE solver.\n",
    "    return integrate.odeint(vectorfield, w0, t, args=(p,), atol=abserr, rtol=relerr)\n",
    "\n",
    "\n",
    "wsol = solve_ode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9391c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_alternative_solution():\n",
    "    plt.close(\"alternative\")\n",
    "    fig, ax = plt.subplots(figsize=(7, 4), num=\"alternative\")\n",
    "\n",
    "    t = np.arange(0, 50, 0.1)\n",
    "    ax.plot(t, wsol[::2, 0])\n",
    "    ax.plot(t, wsol[::2, 2])\n",
    "\n",
    "    ax.set_xlim([0, 50])\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend((\"Block 1\", \"Block 2\"))\n",
    "    ax.set_xlabel(\"Time $t$ (s)\")\n",
    "    ax.set_ylabel(\"displacement from equilibrium $x$ (m)\")\n",
    "\n",
    "\n",
    "plot_alternative_solution()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "title": "Eigenvalue probems",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.polynomial.polynomial as poly\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although many phenomena in nature can be described (or approximated) by a linear response, many others are inherently **nonlinear** in that effects are not directly proportional to their causes. \n",
    "For instance, the air resistance of a moving object is proportional to the square of the velocity.\n",
    "\n",
    "In analogy to linear equations, where a system of equations is written as $\\mathbf{Ax}=\\mathbf{b}$, we could write down a system of nonlinear equations as $\\mathbf{f}(\\mathbf{x})=\\mathbf{y}$.\n",
    "\n",
    "However, it is more customary to subtract $\\mathbf{y}$ from $\\mathbf{f}(\\mathbf{x})$ so the equation that needs to be solved is expressed as $\\mathbf{f}(\\mathbf{x})=\\mathbf{0}$.\n",
    "\n",
    "In one dimension, this means that we are looking for the intersection of a curve with the x-axis. \n",
    "In general, we seek a vector $\\mathbf{x}$ such that all component function $\\mathbf{f}(\\mathbf{x})$ are zero simultaneously.\n",
    "\n",
    "A solution value $\\mathbf{x}$ such that $\\mathbf{f}(\\mathbf{x})=\\mathbf{0}$ is called a **root** of the equation, and a **zero** of the function $\\mathbf{f}$. \n",
    "This problem is thus referred to as **root finding** or **zero finding**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear systems, each equation describes a *flat* hyperplane in $\\mathbb{R}^n$, and the solution corresponded with the points where all of them intersect.\n",
    "For nonlinear equations, this is also true, but here each equation can describe a *curved* hyperplane.\n",
    "Because curved surfaces can intersect in many more ways than flat ones, it is not possible to make general statements about the number of solutions to a nonlinear problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Examples**\n",
    "> \n",
    "> Even in 1 dimension, many different cases are possible:\n",
    ">\n",
    "> - $e^x+1=0$ has no solution.\n",
    "> - $e^{-x}-x=0$ has one solution.\n",
    "> - $x^2 - 4 \\sin(x)=0$ has two solutions.\n",
    "> - $x^3-6x^2+10x-4=0$ has three solutions.\n",
    "> - $\\sin(x)=0$ has infinitely many solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_nr_solutions():\n",
    "    plt.close(\"nr_solutions\")\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(8, 5), num=\"nr_solutions\")\n",
    "    x = np.arange(-1, 4, 0.01)\n",
    "\n",
    "    plots = [\n",
    "        np.exp(x) + 1,\n",
    "        np.exp(-x) - x,\n",
    "        x**2 - 4 * np.sin(x),\n",
    "        x**3 - 6 * x**2 + 10 * x - 4,\n",
    "        np.sin(np.arange(-1, 4 * np.pi, 0.01)),\n",
    "    ]\n",
    "    labels = [\n",
    "        \"$e^x+1$\",\n",
    "        \"$e^{-x}-x$\",\n",
    "        r\"$x^2 - 4 \\sin(x)$\",\n",
    "        \"$x^3-6x^2+10x-4$\",\n",
    "        r\"$\\sin(x)$\",\n",
    "    ]\n",
    "\n",
    "    for i in range(5):\n",
    "        row, col = divmod(i, 3)\n",
    "        ax = axs[row, col]\n",
    "\n",
    "        if i == 4:\n",
    "            x = np.arange(-1, 4 * np.pi, 0.01)\n",
    "\n",
    "        ax.axhline(0, color=\"black\")\n",
    "        ax.plot(x, plots[i], label=labels[i])\n",
    "        ax.legend()\n",
    "\n",
    "\n",
    "fig_nr_solutions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a nonlinear equation it is possible to have degenerate solutions, which are called **multiple roots**. Generally, for a smooth function $f$, if $f(x^*)=f'(x^*)=f''(x^*)=\\cdots=f^{(m-1)}(x^*)=0$ and $f^m(x^*)\\neq0$, then $x^*$ is a root of **multiplicity** $m$. \n",
    "\n",
    "If $m=1$, then the solution is not degenerate and is called a **simple root**. \n",
    "\n",
    "Geometrically, this means that the curve defined by $f$ has a horizontal tangent at the x-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Examples**\n",
    "> \n",
    "> $x^2-4x+4=(x-2)^2=0$ has a root $x=2$ of multiplicity 2\n",
    ">\n",
    "> $x^3-6x^2+12x-8=(x-2)^3=0$ has a root $x=2$ of multiplicity 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_multiplicity():\n",
    "    plt.close(\"multiplicity\")\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 4), num=\"multiplicity\")\n",
    "\n",
    "    x = np.arange(1, 3, 0.01)\n",
    "\n",
    "    axs[0].plot(x, x**2 - 4 * x + 4, label=\"$x^2-4x+4=(x-2)^2$\")\n",
    "    axs[0].axhline(0, color=\"black\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(\n",
    "        x,\n",
    "        x**3 - 6 * x**2 + 12 * x - 8,\n",
    "        label=\"$x^3-6x^2+12x-8=(x-2)^3$\",\n",
    "    )\n",
    "    axs[1].axhline(0, color=\"black\")\n",
    "    axs[1].legend()\n",
    "\n",
    "\n",
    "fig_multiplicity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the sensitivity of the root finding problem $f(x)=0$, i.e. if $x^*$ is a root of $f$, how much does $x^*$ change for small changes to the parameters of $f$?\n",
    "\n",
    "In one dimension, the condition number for the root-finding problem of $f$ near $x^*$ is $\\frac{1}{\\|f'(x^*)\\|}$.\n",
    "\n",
    "In other words, for functions for which $f'(x)$ is small near the root, the error in the root finding problem can be substantial.\n",
    "\n",
    "At a multiple root $x^*$, $f'(x^*)=0$, so the condition number of a multiple root is infinite. \n",
    "Intuitively this is clear because a small change in the parameters of $f$ can cause the multiple root to disappear or split up in more than one root.\n",
    "\n",
    "> **Example**\n",
    ">\n",
    "> As an example, consider the root-finding problem $f(x)=x^2=0$, which has twofold degenerate solution $x^*=0$.\n",
    ">\n",
    "> For a small change $\\epsilon >0$ in $f$, we can find\n",
    ">\n",
    "> $x^2-\\epsilon=0$, which has two roots at $\\pm \\sqrt{\\epsilon}$\n",
    ">\n",
    "> or\n",
    "> \n",
    "> $x^2+\\epsilon=0$, which has no roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parabola(epsilon, ax):\n",
    "    # Generate x data\n",
    "    x = np.linspace(-3, 3, 100)\n",
    "\n",
    "    # Calculate the roots of the equation y = x^2 + epsilon = 0\n",
    "    if epsilon < 0:\n",
    "        roots = [np.sqrt(-epsilon), -np.sqrt(-epsilon)]\n",
    "    elif epsilon == 0:\n",
    "        roots = [0]\n",
    "    else:\n",
    "        roots = []\n",
    "\n",
    "    # Plot the parabola and its roots\n",
    "    ax.axhline(y=0, color=\"black\")\n",
    "    ax.plot(x, x**2 + epsilon, \"b\", linewidth=2.0, label=r\"$y = x^2 + \\epsilon$\")\n",
    "    ax.plot(roots, [0 for _ in roots], \"o\", color=\"black\", label=\"Roots\")\n",
    "\n",
    "    # Labeling\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-1.5, 9)\n",
    "    ax.set_title(\n",
    "        rf\"Sensitivity of $x^2 + \\epsilon = 0$ with $\\epsilon = {epsilon:.1f}$\"\n",
    "    )\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "# Set up the figure and subplots\n",
    "plt.close(\"parabolas1\")\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), num=\"parabolas1\")\n",
    "\n",
    "# Plot for epsilon = -0.2\n",
    "plot_parabola(-0.2, ax1)\n",
    "\n",
    "# Plot for epsilon = 0.2\n",
    "plot_parabola(0.2, ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiple dimensions, the condition number is generalized using the Jacobian $\\mathbf{J}$ to $\\|\\mathbf{J}^{-1}_{\\mathbf{f}(\\mathbf{x})}\\|$.\n",
    "\n",
    "> **Example** \n",
    ">\n",
    "> Consider the two-dimensional system\n",
    ">\n",
    "> $$\\mathbf{f}(\\mathbf{x})=\\begin{bmatrix}x_1^2-x_2+\\gamma\\\\-x_1+x^2_2+\\gamma\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\end{bmatrix}$$\n",
    ">\n",
    "> Each of these equations defines a parabola, and any point where they intersect is a solution to the system. \n",
    "> Depending on the value of $\\gamma$, this system can have either zero, one, two or four solutions.\n",
    ">\n",
    "> For the specific case of $\\gamma=0.25$, both parabola touch each other, i.e. they have one degenerate solution.\n",
    "> \n",
    "> For this value, the Jacobian matrix reads\n",
    ">\n",
    "> $$\\mathbf{J}_{\\mathbf{f}(\\mathbf{x})}=\\begin{bmatrix}2x_1&-1\\\\-1&2x_2\\end{bmatrix}$$\n",
    ">\n",
    "> which is singular at the unique solution $\\mathbf{x}^*=[0.5,0.5]^\\intercal$. \n",
    ">\n",
    "> For a larger value of $\\gamma$ the parabola no longer intersect, and for a smaller value of $\\gamma$, they intersect at 2 points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parabolas(gamma, ax):\n",
    "    # Define the x ranges for each curve\n",
    "    x1, x2 = np.linspace(-2, 2.75, 100), np.linspace(-2, 2.75, 100)\n",
    "\n",
    "    # Define the system of equations\n",
    "    def f(x):\n",
    "        return [x[0] ** 2 - x[1] + gamma, -x[0] + x[1] ** 2 + gamma]\n",
    "\n",
    "    # Find intersection points\n",
    "    roots = []\n",
    "    start_conditions = np.array([[0, 0], [1, 1], [-1, 0], [0, -1]])\n",
    "    for start_condition in start_conditions:\n",
    "        result = optimize.root(f, start_condition)\n",
    "        if result.success:\n",
    "            roots.append(result.x)\n",
    "\n",
    "    # Plot the curves\n",
    "    ax.plot(x1, x1**2 + gamma, \"b\", linewidth=2.0, label=r\"$x_{1}^{2}-x_{2}+\\gamma$\")\n",
    "    ax.plot(\n",
    "        x2**2 + gamma, x2, \"r\", linewidth=2.0, label=r\"$-x_{1}+x_{2}^{2}+\\gamma$\"\n",
    "    )\n",
    "\n",
    "    # Plot intersection points\n",
    "    ax.plot(\n",
    "        [root[0] for root in roots], [root[1] for root in roots], \"o\", color=\"black\"\n",
    "    )\n",
    "\n",
    "    # Labeling and limits\n",
    "    ax.set_xlabel(\"$x_{1}$\")\n",
    "    ax.set_ylabel(\"$x_{2}$\")\n",
    "    ax.set_title(rf\"$\\gamma = {gamma:.2f}$\")\n",
    "    ax.set_xlim(-2, 4)\n",
    "    ax.set_ylim(-3.5, 5.5)\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "# Set up figure with subplots for each case\n",
    "plt.close(\"parabolas2\")\n",
    "fig, axs = plt.subplots(2, 2, figsize=(8, 6), num=\"parabolas2\")\n",
    "gammas = [-1, 0.25, -0.5, 1]  # Example values for gamma with 0, 1, 2, and 4 roots\n",
    "\n",
    "# Plot each case on a separate subplot\n",
    "for ax, gamma in zip(axs.flat, gammas, strict=True):\n",
    "    plot_parabolas(gamma, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Rates and Stopping Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **convergence rate** is the effectiveness with which a certain algorithm reaches its solution.\n",
    "\n",
    "To solve a nonlinear equation, one often has the choice between several iterative methods, with different converge rates. \n",
    "The total cost of solving the system does not only depend on the amount of iterations necessary to reach the solution with the desired accuracy, but also the computational complexity of a single iteration.\n",
    "\n",
    "The convergence rate can be defined as follows: \n",
    "\n",
    "Let $\\mathbf{e}_k=\\mathbf{x}_k-x^*$ be the error at iteration $k$, where $\\mathbf{x}_k$ is the approximate solution at iteration $k$ and $x^*$ the (usually unknown) true solution.\n",
    "\n",
    "An iterative method is said to converge with rate $r$ if\n",
    "\n",
    "$$\\lim_{k\\rightarrow\\infty}\\frac{\\|\\mathbf{e}_{k+1}\\|}{\\|\\mathbf{e}_k\\|^r}=C$$\n",
    "for some finite constant $C>0$.\n",
    "\n",
    "Interesting cases are:\n",
    "\n",
    "- $r=1$ and $C<1$: *linear* convergence\n",
    "- $r>1$ : *superlinear* convergence\n",
    "- $r=2$ : *quadratic* convergence\n",
    "- $r=3$ : *cubic* convergence\n",
    "\n",
    "In an iterative method with linear convergence, the solution gains an additional $-r \\log_{10}(C)$ number of correct digits as compared to the previous iteration.\n",
    "For superlinearly convergent methods, the solution has about $r$ times as many correct digits as compared to the previous iteration.\n",
    "\n",
    "> **Example** \n",
    ">\n",
    "> To make this more concrete we will look at a couple of examples.\n",
    ">\n",
    "> consider the following sequence.\n",
    "> \n",
    "> $$\\lbrace 1;0.5;0.25;0.125;...\\rbrace$$\n",
    ">\n",
    "> We see that this sequence will converge to 0 so that we can define the errors as $\\mathbf{e}_k=\\mathbf{x}_k-x^*=\\frac{1}{2^k}-0$. \n",
    "> This sequence has a linear convergence rate with $C=0.5$. this is easily verified because we recognize the sequence of errors as $e_k=1/2^k$\n",
    "> \n",
    "> $$\\lim_{k\\rightarrow\\infty}\\frac{\\frac{1}{2^{k+1}}}{\\frac{1}{2^{k}}}=0.5$$\n",
    ">\n",
    "> Now consider a sequence of errors. This is the sequence of errors from the demonstration of Newton's method below (the errors are those of iterations 12-16, because then the values get close enough to the real value to be meaningful).\n",
    ">\n",
    "> $$\\lbrace 0.122;0.0128;0.00016;2.66\\cdot10^{-8};6.66\\cdot10^{-16}\\rbrace$$\n",
    ">\n",
    "> As we can see we get double the amount of precision each iteration, which means that we have a convergence rate of 2, i.e. quadratic convergence.\n",
    ">\n",
    "> With this convergence rate of $r=2$ we can calculate the constant $C$. This constant seems to get closer and closer to $C=0.6$\n",
    "> In the last iteration we hit machine precision so we will not count that as a valid estimation for the value of $C$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convergence of a certain algorithm tells us that we zoom in on the correct solution at a certain rate, but it doesn't tell us the current accuracy of our solution at any given iteration.\n",
    "\n",
    "Therefore, we don't know whether we reached a solution that is sufficiently close to the real solution to decide that we can stop the algorithm. \n",
    "\n",
    "More often than not, it's not trivial to define a suitable **stopping criterion**.\n",
    "A reasonable way is to look at the relative change in the solutions for successive iterations $\\|\\mathbf{x}_{k+1}-\\mathbf{x}_{k}\\|/\\|\\mathbf{x}_{k}\\|<\\varepsilon$, and check that this quantity becomes smaller than a predefined **error tolerance** $\\varepsilon$. \n",
    "\n",
    "A sensible value for $\\varepsilon$ *might* be (but this really depends on your specific problem) the double precision accuracy of $10^{-16}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving nonlinear equations in one dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on how to find the solution to nonlinear equations in one dimension:\n",
    "For a continuous function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$, we seek a point $x^* \\in \\mathbb{R}$ such that $f(x^*)=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bisection method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there might not exist a machine number $x^*$ for which $f(x^*)$ is exactly zero using finite-precision arithmetic. An alternative is to look for a (short) interval $[a,b]$ in which $f$ changes sign. \n",
    "Such a **bracket** ensures that the function must take a zero value somewhere within this interval.\n",
    "\n",
    "The **bisection method** begins with an initial bracket and then iteratively reduces its length until the desired accuracy is reached. \n",
    "\n",
    "At every iteration, the function is evaluated at the **midpoint** of the interval, such that half of the interval can be discarded, based on the sign of the function value at the midpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bisection method makes no use of the magnitudes of the function values, and as a result it is certain to converge, but very slowly. At each iteration, the bound on the possible error is reduced by half, meaning that it converges linearly with $r=1$ and $C=0.5$. \n",
    "\n",
    "Given a starting interval $[a,b]$, the length of the interval after $k$ iterations is $(b-a)/2^k$, so that achieving an error tolerance of $\\varepsilon$ requires $n$ iterations, where\n",
    "\n",
    "$$\n",
    "n = \\log_2\\left(\\frac{b-a}{\\varepsilon}\\right)\n",
    "\\qquad\n",
    "\\iff\n",
    "\\qquad\n",
    "\\varepsilon = \\left(\\frac{b-a}{2^{n}}\\right)\n",
    "$$\n",
    "\n",
    "regardless of the particular function $f$ involved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_bm(x):\n",
    "    \"\"\"Example function to demonstrate the bisection method.\"\"\"\n",
    "    return x * x - 4 * np.sin(x)\n",
    "\n",
    "\n",
    "def bisection_method(f, a, b, tol):\n",
    "    \"\"\"Bisection method implementation to find the root\n",
    "    of a function within an interval.\"\"\"\n",
    "    brackets = []\n",
    "    while (b - a) >= tol:\n",
    "        m = a + (b - a) / 2\n",
    "        if np.sign(f(a)) == np.sign(f(m)):\n",
    "            a = m\n",
    "        else:\n",
    "            b = m\n",
    "        brackets.append([a, b, m])\n",
    "        print(f\"{len(brackets):3d}  {a:17.15f}  {b:17.15f}\")\n",
    "    return np.array(brackets)\n",
    "\n",
    "\n",
    "def plot_bisection_with_function(f, a=1, b=6, tol=1e-8):\n",
    "    # Perform bisection method and capture intervals\n",
    "    results = bisection_method(f, a, b, tol)\n",
    "    num_iterations = len(results)\n",
    "\n",
    "    # Set up figure and subplots with shared x-axis\n",
    "    plt.close(\"bisection\")\n",
    "    fig, (ax1, ax2) = plt.subplots(\n",
    "        2,\n",
    "        1,\n",
    "        figsize=(8, 6),\n",
    "        sharex=True,\n",
    "        gridspec_kw={\"height_ratios\": [2, 1]},\n",
    "        num=\"bisection\",\n",
    "    )\n",
    "\n",
    "    # Top Plot: Function plot with zero crossing\n",
    "    x = np.linspace(a, b, 400)\n",
    "    y = f(x)\n",
    "    ax1.plot(x, y, label=r\"$f(x) = x^2 - 4\\sin(x)$\", color=\"blue\")\n",
    "    ax1.axhline(0, color=\"black\", linewidth=0.5)\n",
    "    ax1.set_ylabel(\"f(x)\")\n",
    "    ax1.set_title(\"Bisection Method Convergence\")\n",
    "    ax1.legend()\n",
    "\n",
    "    # Highlight zero crossing (root) on the function plot, if known\n",
    "    root_approx = results[-1, 2]  # Final midpoint as the root approximation\n",
    "    ax1.plot(root_approx, f(root_approx), \"ro\", label=\"Approximate root\")\n",
    "    ax1.legend()\n",
    "\n",
    "    # Bottom Plot: Bisection funnel\n",
    "    for i, (left, right, mid) in enumerate(results):\n",
    "        offset = -i  # Vertical offset for each interval\n",
    "        ax2.hlines(\n",
    "            offset, left, right, color=\"blue\", linestyle=\"--\", linewidth=1\n",
    "        )  # Interval line\n",
    "        ax2.plot([left, right], [offset, offset], \"bo\")  # Interval endpoints\n",
    "        ax2.plot(mid, offset, \"ro\")  # Midpoint at each step\n",
    "\n",
    "    # Bottom plot settings\n",
    "    ax2.set_xlabel(\"x\")\n",
    "    ax2.set_ylabel(\"Iteration (Offset)\")\n",
    "    ax2.set_ylim(-num_iterations, 1)\n",
    "    ax2.invert_yaxis()  # Invert y-axis for funnel effect\n",
    "\n",
    "\n",
    "# Call the function to display the plot\n",
    "plot_bisection_with_function(func_bm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bisection method is also implemented in SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize.root_scalar(\n",
    "    func_bm, method=\"bisect\", bracket=[1, 6], xtol=1e-16, maxiter=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed-point iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider an alternative problem.\n",
    "Given a function $g:\\mathbb{r}\\rightarrow\\mathbb{R}$, a value $x$ such that $x=g(x)$ is called a **fixed point** of the function $g$, since $x$ remains unchanged when $g$ is applied to it.\n",
    "\n",
    "Geometrically, finding such a fixed point corresponds to finding an intersection between $g$ and the diagonal line $y=x$.\n",
    "\n",
    "This problem is important because many iterative algorithms for solving nonlinear equations (see below) are based on iterations of the form\n",
    "\n",
    "$$\n",
    "x_{k+1}=g(x_k)\n",
    "$$\n",
    "\n",
    "where $g$ is a function chosen so that its fixed points are solutions for $f(x)=0$. \n",
    "Such a scheme is called **fixed-point iteration** or **functional iteration**, since the function $g$ is applied repeatedly to an initial starting value $x_0$.\n",
    "\n",
    "For a given function $f(x)=0$, there are many equivalent fixed-point problems $x=g(x)$ with different choices for $g$. However, they are not all equally useful, as they may differ in their convergence rate and even whether or not they converge at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(x):\n",
    "    \"\"\"Divergent function for fixed-point iteration.\"\"\"\n",
    "    return x**2 - 2\n",
    "\n",
    "\n",
    "def func2(x):\n",
    "    \"\"\"Convergent function for fixed-point iteration.\"\"\"\n",
    "    return np.sqrt(x + 2)\n",
    "\n",
    "\n",
    "def func3(x):\n",
    "    \"\"\"Convergent function for fixed-point iteration.\"\"\"\n",
    "    return 1 + 2 / x\n",
    "\n",
    "\n",
    "def func4(x):\n",
    "    \"\"\"Function with potential divergence due to singularity.\"\"\"\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        result = (x**2 + 2) / (2 * x - 1)\n",
    "        if np.isscalar(result):\n",
    "            return np.nan if np.isinf(result) else result\n",
    "        result[np.isinf(result)] = np.nan\n",
    "    return result\n",
    "\n",
    "\n",
    "def fixed_point_iteration(f, x0, max_iter=10):\n",
    "    \"\"\"Perform fixed-point iteration, returning intermediate points.\"\"\"\n",
    "    results = [x0]\n",
    "    for _ in range(max_iter):\n",
    "        x_new = f(x0)\n",
    "        if np.isnan(x_new):  # Stop if we encounter a singularity\n",
    "            break\n",
    "        results.extend([x0, x_new])  # Alternate: projection and function evaluation\n",
    "        x0 = x_new\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_fixed_point_iterations():\n",
    "    \"\"\"Plot fixed-point iterations with four functions in subplots.\"\"\"\n",
    "    functions = [func1, func2, func3, func4]\n",
    "    titles = [\n",
    "        r\"$f(x) = x^2 - 2$ (may diverge)\",\n",
    "        r\"$f(x) = \\sqrt{x + 2}$\",\n",
    "        r\"$f(x) = 1 + \\frac{2}{x}$\",\n",
    "        r\"$f(x) = \\frac{x^2 + 2}{2x - 1}$\",\n",
    "    ]\n",
    "    initial_guesses = [2.1, 1, 1, 1]  # Starting points selected for demonstration\n",
    "\n",
    "    # Generate subplots\n",
    "    x_vals = np.linspace(0.5, 5, 400)\n",
    "    plt.close(\"fixed_point\")\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 8), num=\"fixed_point\")\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, (f, title, x0) in enumerate(\n",
    "        zip(functions, titles, initial_guesses, strict=True)\n",
    "    ):\n",
    "        ax = axs[i]\n",
    "        y_vals = f(x_vals)\n",
    "\n",
    "        # Run fixed-point iteration\n",
    "        results = fixed_point_iteration(f, x0, max_iter=10)\n",
    "\n",
    "        # Plot function and diagonal y=x line\n",
    "        ax.plot(x_vals, y_vals, label=\"f(x)\", color=\"blue\")\n",
    "        ax.plot(x_vals, x_vals, label=\"y=x\", color=\"gray\", linestyle=\"--\")\n",
    "\n",
    "        # Plot iteration trajectory with arrows for each step\n",
    "        for j in range(0, len(results) - 2, 1):\n",
    "            x_start, y_start = results[j], f(results[j])\n",
    "            x_proj = results[j + 1]\n",
    "            # Horizontal arrow for projection step\n",
    "            ax.annotate(\n",
    "                \"\",\n",
    "                xy=(x_proj, y_start),\n",
    "                xytext=(x_start, y_start),\n",
    "                arrowprops=dict(arrowstyle=\"->\", color=\"black\", lw=1.5),\n",
    "            )\n",
    "\n",
    "            # Vertical arrow for function step\n",
    "            ax.annotate(\n",
    "                \"\",\n",
    "                xy=(x_proj, f(x_proj)),\n",
    "                xytext=(x_proj, y_start),\n",
    "                arrowprops=dict(arrowstyle=\"->\", color=\"black\", lw=1.5),\n",
    "            )\n",
    "\n",
    "            # Mark the function step point\n",
    "            ax.plot(x_proj, f(x_proj), \"go\")  # Mark the next function step point\n",
    "\n",
    "        # Set labels and titles\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlim(0.5, 5)\n",
    "        ax.set_ylim(0.5, 5)\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"f(x)\")\n",
    "        ax.legend()\n",
    "\n",
    "\n",
    "# Execute the plot function\n",
    "plot_fixed_point_iterations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can obtain the same result using SciPy.\n",
    "Note that 20 iterations is not sufficient to reach the default relative error tolerance of $10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize.fixed_point(func3, x0=1, method=\"iteration\", maxiter=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to characterize the behavior of an iterative scheme $x_{k+1}=g(x_k)$ for a fixed-point problem $x=g(x)$ is to look at the derivative of $g$ in the solution $x^*$. \n",
    "It is a rule that if $x^*=g(x)$ and $\\|g'(x^*)\\|<1$, then the iterative scheme is **locally convergent**. \n",
    "If however $\\|g'(x^*)\\|>1$, then the scheme diverges for every initial value different from $x^*$.\n",
    "\n",
    "> **Proof**\n",
    ">\n",
    "> If $x^*$ is a fixed point, then the error at the $k$-th iteration is\n",
    "> \n",
    "> $$e_{k+1}=x_{k+1}-x^*=g(x_k)-g(x^*)$$\n",
    ">\n",
    "> There exist a point $\\theta_k$ between $x_k$ and $x^*$ for which\n",
    "> \n",
    "> $$g(x_k)-g(x^*)=g'(\\theta_k)(x_k-x^*)$$\n",
    "> \n",
    "> so\n",
    "> \n",
    "> $$e_{k+1}=g'(\\theta_k)e_k$$\n",
    "> \n",
    "> We do not know the value of $\\theta_k$, but if $\\|g'(x^*)\\|<1$, then by starting the iteration sufficiently close to $x^*$, there exists a constant $C$ for which $\\|g'(\\theta_k)\\|\\leq C<1$, for $k=0,1,\\ldots$.\n",
    ">\n",
    "> Thus we have\n",
    "> \n",
    "> $$\\|e_{k+1}\\|\\leq C \\|e_{k}\\| \\leq \\ldots\\leq C^k\\|e_{e_0}\\|$$\n",
    ">\n",
    "> As $C<1$ implies $C^k \\rightarrow 0$, also $\\|e_{k}\\|\\rightarrow 0$ and the sequence converges.\n",
    "\n",
    "The convergence rate of the iterative scheme is linear with $C=\\|g'(x^*)\\|$. The smaller this constant, the faster the convergence. Ideally, we have $\\|g'(x^*)\\|=0$, in which case the Taylor expansion gives\n",
    "\n",
    "$$g(x_k)-g(x^*)=g''(\\xi_k)(x_k-x^*)^{2}/2$$\n",
    "\n",
    "with $\\xi_k$ between $x_k$ and $x^*$. This yields\n",
    "\n",
    "$$\\lim_{k\\rightarrow \\infty}\\frac{\\|e_{k+1}\\|}{\\|e_k\\|^{2}}=\\frac{g''(x^*)}{2}$$\n",
    "\n",
    "In this case, the *rate of convergence becomes quadratic*.\n",
    "In the next sections we'll see methods to systematically choose $g$ to reach this quadratic convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bisection method does not make use of the function values (except for their sign), so it is reasonable to assume that better convergence can be achieved by also making use of their magnitude.\n",
    "\n",
    "We start from the truncated Taylor series\n",
    "\n",
    "$$\n",
    "f(x+h)\\approx f(x)+h f'(x),\n",
    "$$\n",
    "\n",
    "which is a linear function of $h$ that approximates $f$ near a given $x$. \n",
    "Its zero is easily determined to be $h=-f(x)/f'(x)$, assuming that $f'(x)\\neq 0$.\n",
    "Because the zeros of both functions are not identical, this procedure is repeated in an iterative scheme, called **Newton's method**\n",
    "\n",
    "This method can be seen as a systematic way of transforming a nonlinear equation $f(x)=0$ into a fixed-point problem $x=g(x)$, where\n",
    "\n",
    "$$\n",
    "g(x)=x-f(x)/f'(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study the convergence of this scheme we determine the derivative\n",
    "\n",
    "$$\n",
    "g'(x)=f(x)f''(x)/(f'(x))^2\n",
    "$$\n",
    "\n",
    "- For simple roots $(f(x^*)=0$ and $f'(x^*)\\neq0)$, $g'(x^*)=0.$ Thus the asymptotic convergence rate of Newton's method is quadratic.\n",
    "- For a multiple root with multiplicity $m$, it is only linearly convergent, with constant $C=1-(1/m)$.\n",
    "\n",
    "> **Proof**\n",
    ">\n",
    "> Generally, you can write a function with a root of multiplicity $M$ at $x=x^*$ as $f(x)=(x-x^*)^M$\n",
    ">\n",
    "> As shown earlier, the constant $C$ of linear convergence is given by \n",
    ">\n",
    "> $$\\|g'(x^*)\\|=  \\|f(x)f''(x)/(f'(x))^2\\|$$\n",
    ">\n",
    "> filling in \n",
    "> - $f(x)=(x-x^*)^M$\n",
    "> - $f'(x)=M(x-x^*)^{(M-1)}$\n",
    "> - $f''(x)=M(M-1)(x-x^*)^{(M-2)}$\n",
    ">\n",
    "> yields $C= 1-1/M$\n",
    "\n",
    "\n",
    "Take note that these convergences are only local and it may not converge at all unless started sufficiently close to the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_cube(x):\n",
    "    \"\"\"Example function for newton and secant methods.\"\"\"\n",
    "    return x**3 - 1  # a function with only one real root at x = 1\n",
    "\n",
    "\n",
    "def func_cube_prime(x):\n",
    "    \"\"\"Derivative of func_cube.\"\"\"\n",
    "    return 3 * x**2\n",
    "\n",
    "\n",
    "def newton_method(f, fp, x, niter):\n",
    "    \"\"\"Illutrative implementation of the Newton method.\n",
    "\n",
    "    Parametes\n",
    "    ---------\n",
    "    f\n",
    "        Function to be rooted.\n",
    "    fp\n",
    "        The derivative of the function to be rooted.\n",
    "    x0\n",
    "        The initial guess of the solution.\n",
    "    niter\n",
    "        The number of iterations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    root\n",
    "        The approximation of the root.\n",
    "    \"\"\"\n",
    "    for _ in range(niter):\n",
    "        x = x - f(x) / fp(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# run the implementation for various N values\n",
    "def newton_method_various_iterations(iterations=20):\n",
    "    for niter in range(iterations):\n",
    "        x = newton_method(func_cube, func_cube_prime, 25, niter)\n",
    "        print(f\"{niter:2d} iterations: x = {x:18.15f}\")\n",
    "\n",
    "\n",
    "newton_method_various_iterations()\n",
    "\n",
    "\n",
    "def plot_and_animate_nm():\n",
    "    x0 = 25\n",
    "    x = np.arange(-10, 26, 0.05)\n",
    "    y = func_cube(x)\n",
    "\n",
    "    fig, ax = plt.subplots(num=\"animate_nm\", clear=True)\n",
    "\n",
    "    ax.axhline(0, color=\"black\")\n",
    "    ax.plot(x, y, color=\"blue\")\n",
    "\n",
    "    ax.set_xlim(-10, 26)\n",
    "    # plt.ylim(-2, 20);\n",
    "\n",
    "    (dots,) = plt.plot([], [], \"o\", markersize=10, color=\"r\")\n",
    "\n",
    "    def animate(i):\n",
    "        x = newton_method(func_cube, func_cube_prime, x0, i)\n",
    "        y = func_cube(x)\n",
    "        dots.set_data([x], [y])\n",
    "        return (dots,)\n",
    "\n",
    "    return FuncAnimation(fig, animate, frames=20, interval=1000, repeat=False)\n",
    "\n",
    "\n",
    "plt.close(\"animate_nm\")\n",
    "nm_anim = plot_and_animate_nm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton method is also implemented in SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize.root_scalar(\n",
    "    func_cube,\n",
    "    method=\"newton\",\n",
    "    x0=25,\n",
    "    fprime=func_cube_prime,\n",
    "    xtol=1e-16,\n",
    "    maxiter=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secant method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One drawback of Newton's method is that both the function and its derivative needs to be explicitly and evaluated at every iteration. In the **Secant method**, the derivative is replaced by a finite difference approximation on successive iterates:\n",
    "\n",
    "$$\n",
    "f'(x_k)=\\frac{f(x_k)-f(x_{k-1})}{x_k-x_{k-1}}\n",
    "$$\n",
    "\n",
    "The secant method can be interpreted geometrically as approximating the function $f$ by the secant line through the previous two estimates, and taking the zero of this function as the best approximate solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with Newton's method, the secant method has the advantage of requiring only one new function evaluation per iteration, but has the disadvantage of requiring two starting guesses and converging somewhat more slowly (subquadratically but still faster than linear with $r\\approx 1.618$).\n",
    "\n",
    "The lower cost per iteration often more than offsets the larger number of iterations required, such that the total cost of finding a root is often less for the secant method than for Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secant_method(f, x0, x1, niter):\n",
    "    \"\"\"Illustrative implementation of the secant method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f\n",
    "        The function to be rooted.\n",
    "    x0, x1\n",
    "        Two different initial guesses.\n",
    "    niter\n",
    "        The number of iterations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    root\n",
    "        The approximate root\n",
    "    \"\"\"\n",
    "    fx0 = f(x0)\n",
    "    for _ in range(niter):\n",
    "        temp = x1\n",
    "        fx1 = f(x1)\n",
    "        x1 = x1 - fx1 * (x1 - x0) / (fx1 - fx0)\n",
    "        x0 = temp\n",
    "        fx0 = fx1\n",
    "    return (x0 + x1) / 2\n",
    "\n",
    "\n",
    "def secant_method_various_iterations(iterations=20):\n",
    "    for niter in range(iterations):\n",
    "        x = secant_method(func_cube, 25, 24, niter)\n",
    "        print(f\"{niter:2d} iterations: x = {x:18.15f}\")\n",
    "\n",
    "\n",
    "secant_method_various_iterations()\n",
    "\n",
    "\n",
    "def plot_and_animate_sm(f):\n",
    "    x0 = 25\n",
    "    x1 = 24\n",
    "    x = np.arange(-10, 26, 0.05)\n",
    "    y = func_cube(x)\n",
    "\n",
    "    fig, ax = plt.subplots(num=\"animate_sm\", clear=True)\n",
    "    ax.axhline(0, color=\"black\")\n",
    "    ax.plot(x, y, color=\"blue\")\n",
    "    ax.set_xlim(-10, 26)\n",
    "\n",
    "    (dots,) = plt.plot([], [], \"o\", markersize=12, color=\"r\")\n",
    "\n",
    "    def animate(i):\n",
    "        x = secant_method(func_cube, x0, x1, i)\n",
    "        y = func_cube(x)\n",
    "        dots.set_data([x], [y])\n",
    "        return (dots,)\n",
    "\n",
    "    return FuncAnimation(fig, animate, frames=20, interval=1000, repeat=False)\n",
    "\n",
    "\n",
    "plt.close(\"animate_sm\")\n",
    "sm_anim = plot_and_animate_sm(func_cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The secant method is also implemented in SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize.root_scalar(\n",
    "    func_cube, method=\"secant\", x0=25, x1=24, xtol=1e-16, maxiter=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The secant method fits a straight line to two values of the function for each iteration.\n",
    "Its convergence rate can be improved (but not made to exceed $r=2$) by fitting a higher order polynomial instead of a straight line. \n",
    "\n",
    "This has however the drawbacks that the zeros of the fitted polynomial might be difficult to compute, or might not exist at all.\n",
    "\n",
    "Instead, we can use **inverse interpolation** where, instead of fitting a polynomial to values $f(x_k)$ as function of the values $x_k$, we do the opposite:\n",
    "we fit a polynomial $p$ to the values $x_k$ as a function of the values $f(x_k)$. \n",
    "The next approximate solution is than simply $p(0)$.\n",
    "\n",
    "The most used implementation of this idea is **inverse quadratic interpolation** where a parabola is fitted through the values obtained at the last 3 iterations. \n",
    "Similar to the secant method this only requires one additional function evaluation per iteration, but requires a little more memory and overhead in fitting the parabola. \n",
    "This algorithm has a converge rate of $r\\approx 1.839$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root finding in SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best and safest method used to find the roots of a one-dimensional function is the **Brent method**\n",
    "``optimize.brentq``\n",
    "This is a so-called **safeguarded method**, which combines the safety of (slow) bracket method like the bisection method and the high converge rates of inverse quadratic interpolation.\n",
    "\n",
    "This method works by defining a suitable starting bracket (at the end of which the function has a different sign), and trying a fast-convergence method. \n",
    "If this method does not converge and the next approximate solution falls outside the defined bracket, the method falls back on the bisection method for one iteration to reduce the size of the bracket and try the fast method (with a higher chance for success) again until the solution is found.\n",
    "\n",
    "A more detailed explanation can be found on:\n",
    "<https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brentq.html#scipy.optimize.brentq>\n",
    "\n",
    "As we'll see below, in multiple dimensions we'll need the `optimize.root` function, which also works in 1 dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_par(x):\n",
    "    \"\"\"A simple quadratic test function with two obvious roots.\"\"\"\n",
    "    return x**2 - 1\n",
    "\n",
    "\n",
    "# find its root in the bracket -2 and 0\n",
    "print(f\"Root in bracket [-2, 0]: {optimize.brentq(func_par, -2, 0)} \\n\")\n",
    "\n",
    "# find its root in the bracket 0 and 2\n",
    "print(f\"Root in bracket [0, 2]: {optimize.brentq(func_par, 0, 2)} \\n\")\n",
    "\n",
    "# now search for its root using the optimize.root function, with 2 as initial guess\n",
    "optimize.root(func_par, x0=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The force $F$ acting on an object in free fall is given by the sum of the gravitational force $F_g=-mg$ (with $m$ the mass and $g$ the gravitational constant), and the air resistance $F_r=C\\rho A v^2/2$ (with $C$ the drag constant of the object, $\\rho$ the density of the fluid the object falls through, $A$ the projected surface and $v$ the velocity).\n",
    "\n",
    "For an object starting at position 0 at time 0, this leads to the following equation of motion:\n",
    "\n",
    "$$\n",
    "F=m\\frac{d^2x}{dt^2}=-mg+\\frac{C\\rho A}{2}\\left(\\frac{dx}{dt}\\right)^2\n",
    "$$\n",
    "\n",
    "The analytical solution to this differential equation is given by the nonlinear equation\n",
    "\n",
    "$$\n",
    "x(t) = -\\frac{\\log(\\cosh(\\sqrt{(AgmC\\rho/2)}t))}{(AC\\rho/2)}\n",
    "$$\n",
    "\n",
    "Let's now consider 2 skydivers: first a quite big person who jumps in a horizontal position\n",
    "The second skydiver is a 50 kg adolescent who enthusiastically dives down head-first to minimize her air resistance, but who starts 1 second later.\n",
    "\n",
    "The graph below shows that the first skydiver almost immediately reaches his terminal velocity, whereas the second skydiver needs a bit more time to accelerate, but due to her more streamlined position reaches a higher velocity and eventually overtakes the first.\n",
    "\n",
    "**the question we want to answer is when the second one overtakes the first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skydivers():\n",
    "    # define the freefall equation\n",
    "    def freefall(x, A, m, C):\n",
    "        g = 9.8  # m/s²\n",
    "        r = 1.21  # kg/m³\n",
    "        return (\n",
    "            -1.0\n",
    "            * np.log(np.cosh(np.sqrt(A * g * m * C * r / 2) * x))\n",
    "            / (A * C * r / 2)\n",
    "        )\n",
    "\n",
    "    # Parameters for both skydivers:\n",
    "    #      Skydiver 1: A = 0.2 m^2,  m =  50 kg, C = 0.7\n",
    "    #      Skydiver 2: A = 0.8 m^2,  m = 110 kg, C = 1.0\n",
    "\n",
    "    # skydiver 1\n",
    "    def skydiver1(x):\n",
    "        A1, m1, C1 = 0.2, 50.0, 0.70\n",
    "        return freefall((x - 1), A1, m1, C1)\n",
    "\n",
    "    # skydiver 2\n",
    "    def skydiver2(x):\n",
    "        A2, m2, C2 = 0.8, 110.0, 1.0\n",
    "        return freefall(x, A2, m2, C2)\n",
    "\n",
    "    # define x values\n",
    "    x = np.arange(0, 5.5, 0.01)\n",
    "\n",
    "    # plot trajectories for both skydivers\n",
    "    plt.close(\"skydivers\")\n",
    "    fig, ax = plt.subplots(num=\"skydivers\")\n",
    "\n",
    "    ax.plot(x[x > 1], skydiver1(x[x > 1]), color=\"blue\", label=\"50kg skydiver\")\n",
    "    ax.plot(x, skydiver2(x), color=\"red\", label=\"110kg skydiver\")\n",
    "    ax.set_xlabel(\"time (s)\")\n",
    "    ax.set_ylabel(\"height (m)\")\n",
    "    ax.legend()\n",
    "\n",
    "    # The question we want to answer for this problem is:\n",
    "    # \"At what time does the first skydiver overtake the second one?\"\n",
    "\n",
    "    def fall(x):\n",
    "        return skydiver1(x) - skydiver2(x)\n",
    "\n",
    "    x_root = optimize.brentq(fall, 1.01, 5.0)\n",
    "    y_root = skydiver1(x_root)\n",
    "\n",
    "    # Plot the intersection point\n",
    "    ax.plot(x_root, y_root, \"o\", color=\"black\")\n",
    "\n",
    "\n",
    "skydivers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roots of polynomial functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All methods we saw until now zoom in on a single root of the function under study.\n",
    "Sometimes we're interested in all the roots of e.g. a polynomial function. \n",
    "\n",
    "For a polynomial $p(x)$ of degree $n$, we want to find all $n$ zeros (which might be complex).\n",
    "\n",
    "To this end we can resort to several methods:\n",
    "\n",
    "- Use one of the methods shown above to find one root $x_1$ and then deflate the polynomial $p(x)$ to $p(x)/(x-x_1)$ which has a degree that is one lower and repeat the process. \n",
    "Note that it's a good idea to zoom in on each of the obtained roots using the approximate values used this way to avoid any numerical errors introduced in the deflating process.\n",
    "- Use a dedicated (complex) routine specifically designed for this purpose. \n",
    "These work by isolating the roots of a polynomial in the complex plane, and then refining in a way similar to the bisection method to zoom in on each of the roots. \n",
    "Their complexity is beyond the scope of this course.\n",
    "- Form the **companion matrix** of the given polynomial and use an eigenvalue routine to find its eigenvalues, which are also the roots of the polynomial.\n",
    "\n",
    "The latter method is the one that is used by NumPy in the [`numpy.polynomial.polynomial.polyroots`](https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.polyroots.html) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Eample:**\n",
    "> \n",
    "> Find the roots of the polynomial $x^3 - 6 x^2 + 11 x - 6$, which equals $(x-1)(x-2)(x-3)$.\n",
    "> Note that the `polyroots` function asks an array of polynomial coefficients as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly.polyroots([-6, 11, -6, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systems of nonlinear equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systems of nonlinear equations are more difficult to solve than single nonlinear equations for a number of reasons:\n",
    "\n",
    "- A much wider range of behavior is possible, so we don't get as far with theoretical analysis of the existence and number of solutions.\n",
    "- There is no simple way to bracket a desired solution.\n",
    "- Computational overhead increases rapidly with the dimension of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the methods we saw to solve a 1-dimensional nonlinear problem do not generalize to more than 1 dimension. One method that does is Newton's method:\n",
    "\n",
    "For a differentiable vector function $\\mathbf{f}$, the truncated Taylor series reads:\n",
    "\n",
    "$$\\mathbf{f}(\\mathbf{x}+\\mathbf{s})\\approx\\mathbf{f}(\\mathbf{x})+\\mathbf{J}_{f(\\mathbf{x})}\\,\\mathbf{s}$$\n",
    "\n",
    "where ${\\mathbf{J}_{\\mathbf{f}(\\mathbf{x})}}$ is the Jacobian matrix of $\\mathbf{f}$ with elements\n",
    "\n",
    "$$\\bigl[\\mathbf{J}_{\\mathbf{f}(\\mathbf{x})}\\bigr]_{ij}=\\frac{\\partial f_i(\\mathbf{x})}{\\partial x_j}$$\n",
    "\n",
    "If $\\mathbf{s}$ satisfies the linear system $\\mathbf{J}_{f(\\mathbf{x})}\\,\\mathbf{s}=-\\mathbf{f}(\\mathbf{x})$, then $\\mathbf{x+s}$ is taken as an approximate zero of $\\mathbf{f}$.\n",
    "\n",
    "Essentially, Newton's method replaces a system of nonlinear equations with a system of linear equations, but as the solutions of both systems are not identical, the process must be repeated until the desired accuracy is reached.\n",
    "\n",
    "If the Jacobian of the function is not available, there exist more advanced methods which estimate the Jacobian based on function evaluations, similar to how the secant method works in 1 dimension. \n",
    "\n",
    "The computational cost of Newton's method in $n$ dimensions is substantial: \n",
    "- Evaluating the Jacobian matrix (or approximating it) requires $n^2$ function evaluations.\n",
    "- Solving the system $\\mathbf{J}_{\\mathbf{f}(\\mathbf{x})}\\,\\mathbf{s}=-\\mathbf{f}(\\mathbf{x})$, for instance using LU-factorization, costs $\\mathcal{O}(n^3)$ operations.\n",
    "\n",
    "Without going into too much detail in how these methods work, we'll have a look how the solution to such problems can be found using `scipy` using the `optimize.root` function.\n",
    "Its documentation can be found here\n",
    "<https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.root.html#scipy.optimize.root>\n",
    "\n",
    "Its use is illustrated with an example:\n",
    "\n",
    "> **Example**\n",
    ">\n",
    "> Solve the nonlinear system \n",
    ">\n",
    "> $$\\mathbf{f}(\\mathbf{x})=\\begin{bmatrix}x_1+2x_2-2\\\\x_1^2+4x_2^2-4\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_2d(x):\n",
    "    return [x[0] + 2 * x[1] - 2, x[0] ** 2 + 4 * x[1] ** 2 - 4]\n",
    "\n",
    "\n",
    "# find the roots of this equation with the point [2, 2] as initial guess\n",
    "optimize.root(func_2d, [2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We find the solution $\\mathbf{x^*}=[0,1]^\\intercal$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "title": "Nonlinear Equations",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
